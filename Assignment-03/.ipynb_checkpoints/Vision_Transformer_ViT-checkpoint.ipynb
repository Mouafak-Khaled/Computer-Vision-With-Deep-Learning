{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beginning-apartment",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by Comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-louis",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complete-missouri",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "forbidden-yellow",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-portal",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "needed-charleston",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-remove",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "liquid-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        \n",
    "        self.W_O = nn.Linear(self.proj_dims,self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        attn_out = None\n",
    "        \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is implemented as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        scale_factor = np.sqrt(1 / self.head_dims)\n",
    "        attn_out = torch.matmul(q, torch.transpose(k, -1, -2)) / scale_factor       \n",
    "        \n",
    "        ## Compute attention Weights. Recall that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "        \n",
    "        attn_out = nn.Softmax(-1)(attn_out)\n",
    "        \n",
    "\n",
    "        ## Compute attention output values. Bear in mind that this operation is applied as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        ##       (or any other equivalent torch operations)\n",
    "        \n",
    "        attn_out = torch.transpose(torch.matmul(attn_out, v), 1, 2)\n",
    "        attn_out = attn_out.contiguous().view(b, n, -1)\n",
    "  \n",
    "        \n",
    "        ## Compute output feature map. This operation is just passing the concatenated attention \n",
    "        ## output that we have just obtained through a final projection layer W_O.\n",
    "        ## Both the input and the output should be of size [B, N, D]\n",
    "        \n",
    "        attn_out = self.W_O(attn_out)        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-desert",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dressed-valve",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-bermuda",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "finished-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.gelu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "immediate-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "# torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)\n",
    "# SelfAttention (self, input_dims, head_dims=128, num_heads=2,  bias=False)\n",
    "# MLP(self, input_dims, hidden_dims, output_dims, bias=True)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        self.normlayer = nn.LayerNorm(hidden_dims)\n",
    "        self.selfAttention = SelfAttention(input_dims=hidden_dims, head_dims=(hidden_dims // num_heads), num_heads=num_heads, bias=bias)\n",
    "        self.normlayer2 = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp = MLP(hidden_dims,  hidden_dims, hidden_dims, bias=bias)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "        x = x + self.selfAttention(self.normlayer(x))\n",
    "        x = x + self.mlp(self.normlayer2(x))\n",
    "        return x\n",
    "        \n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-tomato",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "closed-cabin",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,128]\n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-mainland",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "protected-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-banner",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "unable-daisy",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size())  # you should see [64,10]\n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-firewall",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-parker",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "smart-button",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, np.float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-structure",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "coordinated-strategy",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-crown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "broadband-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-religious",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "military-buying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "absolute-disorder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return correct, np.float(correct) / (b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.846 | Acc: 8.000% (2/25)\n",
      "Loss: 3.649 | Acc: 8.000% (4/50)\n",
      "Loss: 3.458 | Acc: 9.333% (7/75)\n",
      "Loss: 3.445 | Acc: 8.000% (8/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 8.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.680 | Acc: 16.000% (4/25)\n",
      "Loss: 2.702 | Acc: 20.000% (10/50)\n",
      "Loss: 2.592 | Acc: 22.667% (17/75)\n",
      "Loss: 2.721 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 3.103 | Acc: 12.000% (3/25)\n",
      "Loss: 3.078 | Acc: 12.000% (6/50)\n",
      "Loss: 2.835 | Acc: 14.667% (11/75)\n",
      "Loss: 2.891 | Acc: 12.000% (12/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 12.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.850 | Acc: 16.000% (4/25)\n",
      "Loss: 2.857 | Acc: 14.000% (7/50)\n",
      "Loss: 2.563 | Acc: 18.667% (14/75)\n",
      "Loss: 2.615 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.303 | Acc: 24.000% (6/25)\n",
      "Loss: 2.335 | Acc: 24.000% (12/50)\n",
      "Loss: 2.371 | Acc: 17.333% (13/75)\n",
      "Loss: 2.365 | Acc: 20.000% (20/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 20.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.009 | Acc: 12.000% (3/25)\n",
      "Loss: 2.680 | Acc: 18.000% (9/50)\n",
      "Loss: 2.484 | Acc: 22.667% (17/75)\n",
      "Loss: 2.528 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.839 | Acc: 32.000% (8/25)\n",
      "Loss: 1.955 | Acc: 24.000% (12/50)\n",
      "Loss: 1.986 | Acc: 25.333% (19/75)\n",
      "Loss: 2.019 | Acc: 24.000% (24/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 24.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.742 | Acc: 0.000% (0/25)\n",
      "Loss: 2.796 | Acc: 4.000% (2/50)\n",
      "Loss: 2.558 | Acc: 9.333% (7/75)\n",
      "Loss: 2.593 | Acc: 10.000% (10/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.815 | Acc: 32.000% (8/25)\n",
      "Loss: 1.879 | Acc: 24.000% (12/50)\n",
      "Loss: 1.879 | Acc: 25.333% (19/75)\n",
      "Loss: 1.865 | Acc: 22.000% (22/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 22.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.561 | Acc: 16.000% (4/25)\n",
      "Loss: 2.639 | Acc: 12.000% (6/50)\n",
      "Loss: 2.471 | Acc: 16.000% (12/75)\n",
      "Loss: 2.497 | Acc: 20.000% (20/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 20.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.343 | Acc: 64.000% (16/25)\n",
      "Loss: 1.457 | Acc: 58.000% (29/50)\n",
      "Loss: 1.583 | Acc: 48.000% (36/75)\n",
      "Loss: 1.667 | Acc: 44.000% (44/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 44.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.408 | Acc: 16.000% (4/25)\n",
      "Loss: 2.581 | Acc: 14.000% (7/50)\n",
      "Loss: 2.473 | Acc: 13.333% (10/75)\n",
      "Loss: 2.518 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.333 | Acc: 64.000% (16/25)\n",
      "Loss: 1.380 | Acc: 58.000% (29/50)\n",
      "Loss: 1.513 | Acc: 48.000% (36/75)\n",
      "Loss: 1.583 | Acc: 43.000% (43/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 43.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.415 | Acc: 4.000% (1/25)\n",
      "Loss: 2.511 | Acc: 4.000% (2/50)\n",
      "Loss: 2.394 | Acc: 5.333% (4/75)\n",
      "Loss: 2.482 | Acc: 7.000% (7/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 7.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.517 | Acc: 52.000% (13/25)\n",
      "Loss: 1.460 | Acc: 52.000% (26/50)\n",
      "Loss: 1.496 | Acc: 45.333% (34/75)\n",
      "Loss: 1.463 | Acc: 47.000% (47/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 47.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.167 | Acc: 24.000% (6/25)\n",
      "Loss: 2.307 | Acc: 20.000% (10/50)\n",
      "Loss: 2.282 | Acc: 18.667% (14/75)\n",
      "Loss: 2.382 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 1.201 | Acc: 68.000% (17/25)\n",
      "Loss: 1.252 | Acc: 64.000% (32/50)\n",
      "Loss: 1.295 | Acc: 61.333% (46/75)\n",
      "Loss: 1.366 | Acc: 60.000% (60/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 60.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.256 | Acc: 20.000% (5/25)\n",
      "Loss: 2.334 | Acc: 20.000% (10/50)\n",
      "Loss: 2.312 | Acc: 18.667% (14/75)\n",
      "Loss: 2.421 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 1.180 | Acc: 72.000% (18/25)\n",
      "Loss: 1.139 | Acc: 76.000% (38/50)\n",
      "Loss: 1.184 | Acc: 74.667% (56/75)\n",
      "Loss: 1.204 | Acc: 75.000% (75/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 75.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.370 | Acc: 12.000% (3/25)\n",
      "Loss: 2.409 | Acc: 14.000% (7/50)\n",
      "Loss: 2.413 | Acc: 14.667% (11/75)\n",
      "Loss: 2.441 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 1.027 | Acc: 88.000% (22/25)\n",
      "Loss: 0.999 | Acc: 88.000% (44/50)\n",
      "Loss: 1.067 | Acc: 82.667% (62/75)\n",
      "Loss: 1.056 | Acc: 82.000% (82/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 82.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.218 | Acc: 12.000% (3/25)\n",
      "Loss: 2.341 | Acc: 14.000% (7/50)\n",
      "Loss: 2.337 | Acc: 13.333% (10/75)\n",
      "Loss: 2.382 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.917 | Acc: 76.000% (19/25)\n",
      "Loss: 0.945 | Acc: 82.000% (41/50)\n",
      "Loss: 0.976 | Acc: 81.333% (61/75)\n",
      "Loss: 0.984 | Acc: 84.000% (84/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 84.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.300 | Acc: 8.000% (2/25)\n",
      "Loss: 2.407 | Acc: 6.000% (3/50)\n",
      "Loss: 2.350 | Acc: 9.333% (7/75)\n",
      "Loss: 2.396 | Acc: 12.000% (12/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 12.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 1.014 | Acc: 84.000% (21/25)\n",
      "Loss: 0.910 | Acc: 86.000% (43/50)\n",
      "Loss: 0.910 | Acc: 86.667% (65/75)\n",
      "Loss: 0.926 | Acc: 86.000% (86/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 86.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.374 | Acc: 4.000% (1/25)\n",
      "Loss: 2.444 | Acc: 6.000% (3/50)\n",
      "Loss: 2.366 | Acc: 8.000% (6/75)\n",
      "Loss: 2.408 | Acc: 10.000% (10/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.868 | Acc: 100.000% (25/25)\n",
      "Loss: 0.816 | Acc: 98.000% (49/50)\n",
      "Loss: 0.839 | Acc: 92.000% (69/75)\n",
      "Loss: 0.839 | Acc: 93.000% (93/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 93.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.488 | Acc: 8.000% (2/25)\n",
      "Loss: 2.454 | Acc: 8.000% (4/50)\n",
      "Loss: 2.388 | Acc: 9.333% (7/75)\n",
      "Loss: 2.427 | Acc: 12.000% (12/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 12.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.713 | Acc: 100.000% (25/25)\n",
      "Loss: 0.772 | Acc: 98.000% (49/50)\n",
      "Loss: 0.783 | Acc: 97.333% (73/75)\n",
      "Loss: 0.791 | Acc: 97.000% (97/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 97.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.391 | Acc: 8.000% (2/25)\n",
      "Loss: 2.401 | Acc: 8.000% (4/50)\n",
      "Loss: 2.339 | Acc: 9.333% (7/75)\n",
      "Loss: 2.375 | Acc: 12.000% (12/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 12.0\n",
      "\n",
      "Final train set accuracy is 97.0\n",
      "Final val set accuracy is 12.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims=hidden_dims, \n",
    "            input_dims=input_dims, \n",
    "            output_dims=output_dims, \n",
    "            num_trans_layers = num_trans_layers, \n",
    "            num_heads=num_heads, \n",
    "            image_k=image_k, \n",
    "            patch_k=patch_k, bias=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=0.25, amsgrad=True)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-auditor",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "liberal-dispatch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-48c091e2d034>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return correct, np.float(correct) / (b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.590 | Acc: 6.250% (4/64)\n",
      "Loss: 4.711 | Acc: 9.375% (12/128)\n",
      "Loss: 4.337 | Acc: 9.375% (18/192)\n",
      "Loss: 4.037 | Acc: 9.766% (25/256)\n",
      "Loss: 3.828 | Acc: 9.375% (30/320)\n",
      "Loss: 3.722 | Acc: 9.375% (36/384)\n",
      "Loss: 3.594 | Acc: 10.045% (45/448)\n",
      "Loss: 3.478 | Acc: 10.938% (56/512)\n",
      "Loss: 3.420 | Acc: 10.938% (63/576)\n",
      "Loss: 3.374 | Acc: 10.625% (68/640)\n",
      "Loss: 3.282 | Acc: 11.364% (80/704)\n",
      "Loss: 3.224 | Acc: 11.979% (92/768)\n",
      "Loss: 3.157 | Acc: 12.981% (108/832)\n",
      "Loss: 3.105 | Acc: 13.393% (120/896)\n",
      "Loss: 3.057 | Acc: 13.854% (133/960)\n",
      "Loss: 3.006 | Acc: 14.160% (145/1024)\n",
      "Loss: 2.977 | Acc: 13.971% (152/1088)\n",
      "Loss: 2.951 | Acc: 14.062% (162/1152)\n",
      "Loss: 2.927 | Acc: 14.391% (175/1216)\n",
      "Loss: 2.899 | Acc: 14.375% (184/1280)\n",
      "Loss: 2.873 | Acc: 14.509% (195/1344)\n",
      "Loss: 2.848 | Acc: 14.986% (211/1408)\n",
      "Loss: 2.826 | Acc: 15.014% (221/1472)\n",
      "Loss: 2.810 | Acc: 14.844% (228/1536)\n",
      "Loss: 2.795 | Acc: 14.875% (238/1600)\n",
      "Loss: 2.775 | Acc: 14.784% (246/1664)\n",
      "Loss: 2.756 | Acc: 15.046% (260/1728)\n",
      "Loss: 2.738 | Acc: 15.123% (271/1792)\n",
      "Loss: 2.724 | Acc: 15.140% (281/1856)\n",
      "Loss: 2.711 | Acc: 15.156% (291/1920)\n",
      "Loss: 2.697 | Acc: 15.020% (298/1984)\n",
      "Loss: 2.686 | Acc: 14.941% (306/2048)\n",
      "Loss: 2.673 | Acc: 15.057% (318/2112)\n",
      "Loss: 2.662 | Acc: 14.936% (325/2176)\n",
      "Loss: 2.650 | Acc: 15.000% (336/2240)\n",
      "Loss: 2.641 | Acc: 14.800% (341/2304)\n",
      "Loss: 2.628 | Acc: 14.865% (352/2368)\n",
      "Loss: 2.618 | Acc: 14.967% (364/2432)\n",
      "Loss: 2.604 | Acc: 15.224% (380/2496)\n",
      "Loss: 2.595 | Acc: 15.234% (390/2560)\n",
      "Loss: 2.585 | Acc: 15.396% (404/2624)\n",
      "Loss: 2.574 | Acc: 15.476% (416/2688)\n",
      "Loss: 2.562 | Acc: 15.734% (433/2752)\n",
      "Loss: 2.553 | Acc: 15.945% (449/2816)\n",
      "Loss: 2.544 | Acc: 16.076% (463/2880)\n",
      "Loss: 2.533 | Acc: 16.202% (477/2944)\n",
      "Loss: 2.523 | Acc: 16.556% (498/3008)\n",
      "Loss: 2.511 | Acc: 16.732% (514/3072)\n",
      "Loss: 2.503 | Acc: 16.964% (532/3136)\n",
      "Loss: 2.496 | Acc: 17.000% (544/3200)\n",
      "Loss: 2.489 | Acc: 17.034% (556/3264)\n",
      "Loss: 2.483 | Acc: 17.067% (568/3328)\n",
      "Loss: 2.478 | Acc: 17.070% (579/3392)\n",
      "Loss: 2.469 | Acc: 17.361% (600/3456)\n",
      "Loss: 2.462 | Acc: 17.472% (615/3520)\n",
      "Loss: 2.456 | Acc: 17.606% (631/3584)\n",
      "Loss: 2.450 | Acc: 17.626% (643/3648)\n",
      "Loss: 2.444 | Acc: 17.834% (662/3712)\n",
      "Loss: 2.440 | Acc: 17.876% (675/3776)\n",
      "Loss: 2.434 | Acc: 17.917% (688/3840)\n",
      "Loss: 2.427 | Acc: 17.982% (702/3904)\n",
      "Loss: 2.423 | Acc: 18.044% (716/3968)\n",
      "Loss: 2.416 | Acc: 18.428% (743/4032)\n",
      "Loss: 2.411 | Acc: 18.433% (755/4096)\n",
      "Loss: 2.406 | Acc: 18.558% (772/4160)\n",
      "Loss: 2.402 | Acc: 18.608% (786/4224)\n",
      "Loss: 2.398 | Acc: 18.657% (800/4288)\n",
      "Loss: 2.392 | Acc: 18.796% (818/4352)\n",
      "Loss: 2.388 | Acc: 18.818% (831/4416)\n",
      "Loss: 2.384 | Acc: 18.750% (840/4480)\n",
      "Loss: 2.380 | Acc: 18.772% (853/4544)\n",
      "Loss: 2.377 | Acc: 18.859% (869/4608)\n",
      "Loss: 2.373 | Acc: 18.878% (882/4672)\n",
      "Loss: 2.370 | Acc: 18.919% (896/4736)\n",
      "Loss: 2.367 | Acc: 18.958% (910/4800)\n",
      "Loss: 2.362 | Acc: 18.956% (922/4864)\n",
      "Loss: 2.359 | Acc: 19.034% (938/4928)\n",
      "Loss: 2.357 | Acc: 18.990% (948/4992)\n",
      "Loss: 2.355 | Acc: 19.007% (961/5056)\n",
      "Loss: 2.351 | Acc: 19.004% (973/5120)\n",
      "Loss: 2.348 | Acc: 19.078% (989/5184)\n",
      "Loss: 2.345 | Acc: 19.074% (1001/5248)\n",
      "Loss: 2.342 | Acc: 19.051% (1012/5312)\n",
      "Loss: 2.338 | Acc: 19.122% (1028/5376)\n",
      "Loss: 2.335 | Acc: 19.099% (1039/5440)\n",
      "Loss: 2.331 | Acc: 19.150% (1054/5504)\n",
      "Loss: 2.328 | Acc: 19.217% (1070/5568)\n",
      "Loss: 2.326 | Acc: 19.283% (1086/5632)\n",
      "Loss: 2.324 | Acc: 19.364% (1103/5696)\n",
      "Loss: 2.320 | Acc: 19.392% (1117/5760)\n",
      "Loss: 2.319 | Acc: 19.402% (1130/5824)\n",
      "Loss: 2.316 | Acc: 19.497% (1148/5888)\n",
      "Loss: 2.313 | Acc: 19.556% (1164/5952)\n",
      "Loss: 2.310 | Acc: 19.631% (1181/6016)\n",
      "Loss: 2.307 | Acc: 19.720% (1199/6080)\n",
      "Loss: 2.304 | Acc: 19.873% (1221/6144)\n",
      "Loss: 2.303 | Acc: 19.910% (1236/6208)\n",
      "Loss: 2.301 | Acc: 19.978% (1253/6272)\n",
      "Loss: 2.300 | Acc: 19.997% (1267/6336)\n",
      "Loss: 2.297 | Acc: 20.000% (1280/6400)\n",
      "Loss: 2.297 | Acc: 19.941% (1289/6464)\n",
      "Loss: 2.295 | Acc: 20.021% (1307/6528)\n",
      "Loss: 2.293 | Acc: 19.933% (1314/6592)\n",
      "Loss: 2.291 | Acc: 20.012% (1332/6656)\n",
      "Loss: 2.289 | Acc: 20.045% (1347/6720)\n",
      "Loss: 2.286 | Acc: 20.003% (1357/6784)\n",
      "Loss: 2.285 | Acc: 19.947% (1366/6848)\n",
      "Loss: 2.283 | Acc: 19.965% (1380/6912)\n",
      "Loss: 2.281 | Acc: 19.983% (1394/6976)\n",
      "Loss: 2.280 | Acc: 19.943% (1404/7040)\n",
      "Loss: 2.279 | Acc: 19.876% (1412/7104)\n",
      "Loss: 2.277 | Acc: 19.992% (1433/7168)\n",
      "Loss: 2.274 | Acc: 20.119% (1455/7232)\n",
      "Loss: 2.273 | Acc: 20.162% (1471/7296)\n",
      "Loss: 2.271 | Acc: 20.149% (1483/7360)\n",
      "Loss: 2.270 | Acc: 20.110% (1493/7424)\n",
      "Loss: 2.268 | Acc: 20.112% (1506/7488)\n",
      "Loss: 2.267 | Acc: 20.180% (1524/7552)\n",
      "Loss: 2.265 | Acc: 20.234% (1541/7616)\n",
      "Loss: 2.264 | Acc: 20.286% (1558/7680)\n",
      "Loss: 2.262 | Acc: 20.261% (1569/7744)\n",
      "Loss: 2.261 | Acc: 20.300% (1585/7808)\n",
      "Loss: 2.259 | Acc: 20.389% (1605/7872)\n",
      "Loss: 2.257 | Acc: 20.464% (1624/7936)\n",
      "Loss: 2.256 | Acc: 20.488% (1639/8000)\n",
      "Loss: 2.255 | Acc: 20.536% (1656/8064)\n",
      "Loss: 2.254 | Acc: 20.522% (1668/8128)\n",
      "Loss: 2.253 | Acc: 20.544% (1683/8192)\n",
      "Loss: 2.251 | Acc: 20.615% (1702/8256)\n",
      "Loss: 2.250 | Acc: 20.625% (1716/8320)\n",
      "Loss: 2.250 | Acc: 20.563% (1724/8384)\n",
      "Loss: 2.249 | Acc: 20.526% (1734/8448)\n",
      "Loss: 2.247 | Acc: 20.606% (1754/8512)\n",
      "Loss: 2.246 | Acc: 20.662% (1772/8576)\n",
      "Loss: 2.245 | Acc: 20.694% (1788/8640)\n",
      "Loss: 2.243 | Acc: 20.680% (1800/8704)\n",
      "Loss: 2.242 | Acc: 20.700% (1815/8768)\n",
      "Loss: 2.242 | Acc: 20.641% (1823/8832)\n",
      "Loss: 2.241 | Acc: 20.661% (1838/8896)\n",
      "Loss: 2.240 | Acc: 20.636% (1849/8960)\n",
      "Loss: 2.239 | Acc: 20.567% (1856/9024)\n",
      "Loss: 2.238 | Acc: 20.632% (1875/9088)\n",
      "Loss: 2.237 | Acc: 20.597% (1885/9152)\n",
      "Loss: 2.237 | Acc: 20.562% (1895/9216)\n",
      "Loss: 2.236 | Acc: 20.550% (1907/9280)\n",
      "Loss: 2.235 | Acc: 20.484% (1914/9344)\n",
      "Loss: 2.234 | Acc: 20.525% (1931/9408)\n",
      "Loss: 2.233 | Acc: 20.534% (1945/9472)\n",
      "Loss: 2.233 | Acc: 20.554% (1960/9536)\n",
      "Loss: 2.232 | Acc: 20.531% (1971/9600)\n",
      "Loss: 2.231 | Acc: 20.540% (1985/9664)\n",
      "Loss: 2.230 | Acc: 20.539% (1998/9728)\n",
      "Loss: 2.230 | Acc: 20.527% (2010/9792)\n",
      "Loss: 2.229 | Acc: 20.566% (2027/9856)\n",
      "Loss: 2.229 | Acc: 20.585% (2042/9920)\n",
      "Loss: 2.228 | Acc: 20.593% (2056/9984)\n",
      "Loss: 2.227 | Acc: 20.631% (2073/10048)\n",
      "Loss: 2.227 | Acc: 20.639% (2087/10112)\n",
      "Loss: 2.226 | Acc: 20.666% (2103/10176)\n",
      "Loss: 2.226 | Acc: 20.684% (2118/10240)\n",
      "Loss: 2.225 | Acc: 20.642% (2127/10304)\n",
      "Loss: 2.224 | Acc: 20.660% (2142/10368)\n",
      "Loss: 2.224 | Acc: 20.686% (2158/10432)\n",
      "Loss: 2.224 | Acc: 20.694% (2172/10496)\n",
      "Loss: 2.223 | Acc: 20.748% (2191/10560)\n",
      "Loss: 2.222 | Acc: 20.783% (2208/10624)\n",
      "Loss: 2.222 | Acc: 20.762% (2219/10688)\n",
      "Loss: 2.221 | Acc: 20.815% (2238/10752)\n",
      "Loss: 2.221 | Acc: 20.830% (2253/10816)\n",
      "Loss: 2.220 | Acc: 20.864% (2270/10880)\n",
      "Loss: 2.219 | Acc: 20.943% (2292/10944)\n",
      "Loss: 2.219 | Acc: 20.921% (2303/11008)\n",
      "Loss: 2.219 | Acc: 20.918% (2316/11072)\n",
      "Loss: 2.218 | Acc: 20.905% (2328/11136)\n",
      "Loss: 2.218 | Acc: 20.902% (2341/11200)\n",
      "Loss: 2.217 | Acc: 20.907% (2355/11264)\n",
      "Loss: 2.217 | Acc: 20.930% (2371/11328)\n",
      "Loss: 2.216 | Acc: 20.918% (2383/11392)\n",
      "Loss: 2.216 | Acc: 20.880% (2392/11456)\n",
      "Loss: 2.216 | Acc: 20.885% (2406/11520)\n",
      "Loss: 2.215 | Acc: 20.891% (2420/11584)\n",
      "Loss: 2.215 | Acc: 20.836% (2427/11648)\n",
      "Loss: 2.214 | Acc: 20.782% (2434/11712)\n",
      "Loss: 2.214 | Acc: 20.780% (2447/11776)\n",
      "Loss: 2.214 | Acc: 20.743% (2456/11840)\n",
      "Loss: 2.214 | Acc: 20.766% (2472/11904)\n",
      "Loss: 2.213 | Acc: 20.780% (2487/11968)\n",
      "Loss: 2.213 | Acc: 20.753% (2497/12032)\n",
      "Loss: 2.213 | Acc: 20.751% (2510/12096)\n",
      "Loss: 2.213 | Acc: 20.748% (2523/12160)\n",
      "Loss: 2.213 | Acc: 20.730% (2534/12224)\n",
      "Loss: 2.212 | Acc: 20.736% (2548/12288)\n",
      "Loss: 2.212 | Acc: 20.725% (2560/12352)\n",
      "Loss: 2.212 | Acc: 20.739% (2575/12416)\n",
      "Loss: 2.211 | Acc: 20.737% (2588/12480)\n",
      "Loss: 2.211 | Acc: 20.807% (2610/12544)\n",
      "Loss: 2.211 | Acc: 20.749% (2616/12608)\n",
      "Loss: 2.211 | Acc: 20.754% (2630/12672)\n",
      "Loss: 2.211 | Acc: 20.760% (2644/12736)\n",
      "Loss: 2.210 | Acc: 20.789% (2661/12800)\n",
      "Loss: 2.210 | Acc: 20.810% (2677/12864)\n",
      "Loss: 2.210 | Acc: 20.800% (2689/12928)\n",
      "Loss: 2.210 | Acc: 20.751% (2696/12992)\n",
      "Loss: 2.210 | Acc: 20.741% (2708/13056)\n",
      "Loss: 2.210 | Acc: 20.732% (2720/13120)\n",
      "Loss: 2.210 | Acc: 20.722% (2732/13184)\n",
      "Loss: 2.209 | Acc: 20.750% (2749/13248)\n",
      "Loss: 2.209 | Acc: 20.726% (2759/13312)\n",
      "Loss: 2.209 | Acc: 20.701% (2769/13376)\n",
      "Loss: 2.209 | Acc: 20.655% (2776/13440)\n",
      "Loss: 2.209 | Acc: 20.631% (2786/13504)\n",
      "Loss: 2.209 | Acc: 20.622% (2798/13568)\n",
      "Loss: 2.209 | Acc: 20.621% (2811/13632)\n",
      "Loss: 2.209 | Acc: 20.634% (2826/13696)\n",
      "Loss: 2.209 | Acc: 20.640% (2840/13760)\n",
      "Loss: 2.209 | Acc: 20.667% (2857/13824)\n",
      "Loss: 2.209 | Acc: 20.658% (2869/13888)\n",
      "Loss: 2.209 | Acc: 20.657% (2882/13952)\n",
      "Loss: 2.209 | Acc: 20.726% (2905/14016)\n",
      "Loss: 2.208 | Acc: 20.788% (2927/14080)\n",
      "Loss: 2.208 | Acc: 20.822% (2945/14144)\n",
      "Loss: 2.208 | Acc: 20.869% (2965/14208)\n",
      "Loss: 2.208 | Acc: 20.915% (2985/14272)\n",
      "Loss: 2.208 | Acc: 20.912% (2998/14336)\n",
      "Loss: 2.208 | Acc: 20.910% (3011/14400)\n",
      "Loss: 2.208 | Acc: 20.879% (3020/14464)\n",
      "Loss: 2.207 | Acc: 20.898% (3036/14528)\n",
      "Loss: 2.207 | Acc: 20.929% (3054/14592)\n",
      "Loss: 2.207 | Acc: 20.933% (3068/14656)\n",
      "Loss: 2.207 | Acc: 20.938% (3082/14720)\n",
      "Loss: 2.207 | Acc: 20.908% (3091/14784)\n",
      "Loss: 2.207 | Acc: 20.919% (3106/14848)\n",
      "Loss: 2.207 | Acc: 20.909% (3118/14912)\n",
      "Loss: 2.207 | Acc: 20.933% (3135/14976)\n",
      "Loss: 2.207 | Acc: 20.904% (3144/15040)\n",
      "Loss: 2.207 | Acc: 20.882% (3154/15104)\n",
      "Loss: 2.207 | Acc: 20.899% (3170/15168)\n",
      "Loss: 2.207 | Acc: 20.910% (3185/15232)\n",
      "Loss: 2.207 | Acc: 20.927% (3201/15296)\n",
      "Loss: 2.207 | Acc: 20.924% (3214/15360)\n",
      "Loss: 2.207 | Acc: 20.883% (3221/15424)\n",
      "Loss: 2.207 | Acc: 20.848% (3229/15488)\n",
      "Loss: 2.207 | Acc: 20.846% (3242/15552)\n",
      "Loss: 2.207 | Acc: 20.812% (3250/15616)\n",
      "Loss: 2.207 | Acc: 20.823% (3265/15680)\n",
      "Loss: 2.207 | Acc: 20.821% (3278/15744)\n",
      "Loss: 2.207 | Acc: 20.844% (3295/15808)\n",
      "Loss: 2.207 | Acc: 20.848% (3309/15872)\n",
      "Loss: 2.207 | Acc: 20.871% (3326/15936)\n",
      "Loss: 2.207 | Acc: 20.869% (3339/16000)\n",
      "Loss: 2.207 | Acc: 20.867% (3352/16064)\n",
      "Loss: 2.207 | Acc: 20.858% (3364/16128)\n",
      "Loss: 2.207 | Acc: 20.825% (3372/16192)\n",
      "Loss: 2.207 | Acc: 20.811% (3383/16256)\n",
      "Loss: 2.207 | Acc: 20.784% (3392/16320)\n",
      "Loss: 2.207 | Acc: 20.740% (3398/16384)\n",
      "Loss: 2.207 | Acc: 20.714% (3407/16448)\n",
      "Loss: 2.207 | Acc: 20.670% (3413/16512)\n",
      "Loss: 2.208 | Acc: 20.620% (3418/16576)\n",
      "Loss: 2.208 | Acc: 20.631% (3433/16640)\n",
      "Loss: 2.208 | Acc: 20.618% (3444/16704)\n",
      "Loss: 2.208 | Acc: 20.641% (3461/16768)\n",
      "Loss: 2.207 | Acc: 20.681% (3481/16832)\n",
      "Loss: 2.208 | Acc: 20.662% (3491/16896)\n",
      "Loss: 2.208 | Acc: 20.637% (3500/16960)\n",
      "Loss: 2.208 | Acc: 20.641% (3514/17024)\n",
      "Loss: 2.208 | Acc: 20.646% (3528/17088)\n",
      "Loss: 2.208 | Acc: 20.633% (3539/17152)\n",
      "Loss: 2.208 | Acc: 20.620% (3550/17216)\n",
      "Loss: 2.208 | Acc: 20.625% (3564/17280)\n",
      "Loss: 2.208 | Acc: 20.607% (3574/17344)\n",
      "Loss: 2.208 | Acc: 20.605% (3587/17408)\n",
      "Loss: 2.208 | Acc: 20.582% (3596/17472)\n",
      "Loss: 2.208 | Acc: 20.581% (3609/17536)\n",
      "Loss: 2.208 | Acc: 20.574% (3621/17600)\n",
      "Loss: 2.208 | Acc: 20.584% (3636/17664)\n",
      "Loss: 2.208 | Acc: 20.578% (3648/17728)\n",
      "Loss: 2.208 | Acc: 20.577% (3661/17792)\n",
      "Loss: 2.208 | Acc: 20.576% (3674/17856)\n",
      "Loss: 2.208 | Acc: 20.552% (3683/17920)\n",
      "Loss: 2.209 | Acc: 20.540% (3694/17984)\n",
      "Loss: 2.209 | Acc: 20.529% (3705/18048)\n",
      "Loss: 2.209 | Acc: 20.517% (3716/18112)\n",
      "Loss: 2.209 | Acc: 20.478% (3722/18176)\n",
      "Loss: 2.209 | Acc: 20.439% (3728/18240)\n",
      "Loss: 2.209 | Acc: 20.433% (3740/18304)\n",
      "Loss: 2.209 | Acc: 20.416% (3750/18368)\n",
      "Loss: 2.209 | Acc: 20.388% (3758/18432)\n",
      "Loss: 2.209 | Acc: 20.350% (3764/18496)\n",
      "Loss: 2.210 | Acc: 20.312% (3770/18560)\n",
      "Loss: 2.210 | Acc: 20.318% (3784/18624)\n",
      "Loss: 2.210 | Acc: 20.329% (3799/18688)\n",
      "Loss: 2.210 | Acc: 20.339% (3814/18752)\n",
      "Loss: 2.210 | Acc: 20.323% (3824/18816)\n",
      "Loss: 2.210 | Acc: 20.302% (3833/18880)\n",
      "Loss: 2.210 | Acc: 20.302% (3846/18944)\n",
      "Loss: 2.210 | Acc: 20.312% (3861/19008)\n",
      "Loss: 2.210 | Acc: 20.302% (3872/19072)\n",
      "Loss: 2.210 | Acc: 20.276% (3880/19136)\n",
      "Loss: 2.210 | Acc: 20.266% (3891/19200)\n",
      "Loss: 2.211 | Acc: 20.235% (3898/19264)\n",
      "Loss: 2.211 | Acc: 20.235% (3911/19328)\n",
      "Loss: 2.211 | Acc: 20.209% (3919/19392)\n",
      "Loss: 2.211 | Acc: 20.225% (3935/19456)\n",
      "Loss: 2.211 | Acc: 20.215% (3946/19520)\n",
      "Loss: 2.211 | Acc: 20.200% (3956/19584)\n",
      "Loss: 2.211 | Acc: 20.170% (3963/19648)\n",
      "Loss: 2.212 | Acc: 20.125% (3967/19712)\n",
      "Loss: 2.212 | Acc: 20.130% (3981/19776)\n",
      "Loss: 2.212 | Acc: 20.111% (3990/19840)\n",
      "Loss: 2.212 | Acc: 20.101% (4001/19904)\n",
      "Loss: 2.212 | Acc: 20.087% (4011/19968)\n",
      "Loss: 2.212 | Acc: 20.093% (4025/20032)\n",
      "Loss: 2.212 | Acc: 20.084% (4036/20096)\n",
      "Loss: 2.212 | Acc: 20.084% (4049/20160)\n",
      "Loss: 2.213 | Acc: 20.050% (4055/20224)\n",
      "Loss: 2.213 | Acc: 20.041% (4066/20288)\n",
      "Loss: 2.213 | Acc: 20.032% (4077/20352)\n",
      "Loss: 2.213 | Acc: 20.028% (4089/20416)\n",
      "Loss: 2.213 | Acc: 20.044% (4105/20480)\n",
      "Loss: 2.213 | Acc: 20.030% (4115/20544)\n",
      "Loss: 2.213 | Acc: 20.021% (4126/20608)\n",
      "Loss: 2.213 | Acc: 20.051% (4145/20672)\n",
      "Loss: 2.214 | Acc: 20.076% (4163/20736)\n",
      "Loss: 2.214 | Acc: 20.082% (4177/20800)\n",
      "Loss: 2.214 | Acc: 20.073% (4188/20864)\n",
      "Loss: 2.214 | Acc: 20.069% (4200/20928)\n",
      "Loss: 2.214 | Acc: 20.046% (4208/20992)\n",
      "Loss: 2.214 | Acc: 20.028% (4217/21056)\n",
      "Loss: 2.214 | Acc: 20.024% (4229/21120)\n",
      "Loss: 2.215 | Acc: 19.987% (4234/21184)\n",
      "Loss: 2.215 | Acc: 19.992% (4248/21248)\n",
      "Loss: 2.215 | Acc: 19.970% (4256/21312)\n",
      "Loss: 2.215 | Acc: 19.948% (4264/21376)\n",
      "Loss: 2.215 | Acc: 19.925% (4272/21440)\n",
      "Loss: 2.215 | Acc: 19.931% (4286/21504)\n",
      "Loss: 2.216 | Acc: 19.909% (4294/21568)\n",
      "Loss: 2.216 | Acc: 19.920% (4309/21632)\n",
      "Loss: 2.216 | Acc: 19.893% (4316/21696)\n",
      "Loss: 2.216 | Acc: 19.862% (4322/21760)\n",
      "Loss: 2.216 | Acc: 19.863% (4335/21824)\n",
      "Loss: 2.216 | Acc: 19.837% (4342/21888)\n",
      "Loss: 2.217 | Acc: 19.798% (4346/21952)\n",
      "Loss: 2.217 | Acc: 19.813% (4362/22016)\n",
      "Loss: 2.217 | Acc: 19.823% (4377/22080)\n",
      "Loss: 2.217 | Acc: 19.816% (4388/22144)\n",
      "Loss: 2.217 | Acc: 19.795% (4396/22208)\n",
      "Loss: 2.217 | Acc: 19.756% (4400/22272)\n",
      "Loss: 2.217 | Acc: 19.753% (4412/22336)\n",
      "Loss: 2.218 | Acc: 19.768% (4428/22400)\n",
      "Loss: 2.218 | Acc: 19.752% (4437/22464)\n",
      "Loss: 2.218 | Acc: 19.727% (4444/22528)\n",
      "Loss: 2.218 | Acc: 19.706% (4452/22592)\n",
      "Loss: 2.218 | Acc: 19.686% (4460/22656)\n",
      "Loss: 2.218 | Acc: 19.674% (4470/22720)\n",
      "Loss: 2.219 | Acc: 19.659% (4479/22784)\n",
      "Loss: 2.219 | Acc: 19.625% (4484/22848)\n",
      "Loss: 2.219 | Acc: 19.632% (4498/22912)\n",
      "Loss: 2.219 | Acc: 19.616% (4507/22976)\n",
      "Loss: 2.219 | Acc: 19.588% (4513/23040)\n",
      "Loss: 2.219 | Acc: 19.581% (4524/23104)\n",
      "Loss: 2.220 | Acc: 19.574% (4535/23168)\n",
      "Loss: 2.220 | Acc: 19.594% (4552/23232)\n",
      "Loss: 2.220 | Acc: 19.578% (4561/23296)\n",
      "Loss: 2.220 | Acc: 19.576% (4573/23360)\n",
      "Loss: 2.220 | Acc: 19.553% (4580/23424)\n",
      "Loss: 2.220 | Acc: 19.521% (4585/23488)\n",
      "Loss: 2.221 | Acc: 19.523% (4598/23552)\n",
      "Loss: 2.221 | Acc: 19.504% (4606/23616)\n",
      "Loss: 2.221 | Acc: 19.506% (4619/23680)\n",
      "Loss: 2.221 | Acc: 19.495% (4629/23744)\n",
      "Loss: 2.221 | Acc: 19.485% (4639/23808)\n",
      "Loss: 2.221 | Acc: 19.458% (4645/23872)\n",
      "Loss: 2.222 | Acc: 19.439% (4653/23936)\n",
      "Loss: 2.222 | Acc: 19.450% (4668/24000)\n",
      "Loss: 2.222 | Acc: 19.440% (4678/24064)\n",
      "Loss: 2.222 | Acc: 19.417% (4685/24128)\n",
      "Loss: 2.222 | Acc: 19.403% (4694/24192)\n",
      "Loss: 2.222 | Acc: 19.373% (4699/24256)\n",
      "Loss: 2.223 | Acc: 19.375% (4712/24320)\n",
      "Loss: 2.223 | Acc: 19.353% (4719/24384)\n",
      "Loss: 2.223 | Acc: 19.314% (4722/24448)\n",
      "Loss: 2.223 | Acc: 19.284% (4727/24512)\n",
      "Loss: 2.223 | Acc: 19.263% (4734/24576)\n",
      "Loss: 2.223 | Acc: 19.241% (4741/24640)\n",
      "Loss: 2.224 | Acc: 19.207% (4745/24704)\n",
      "Loss: 2.224 | Acc: 19.182% (4751/24768)\n",
      "Loss: 2.224 | Acc: 19.145% (4754/24832)\n",
      "Loss: 2.224 | Acc: 19.128% (4762/24896)\n",
      "Loss: 2.224 | Acc: 19.115% (4771/24960)\n",
      "Loss: 2.224 | Acc: 19.102% (4780/25024)\n",
      "Loss: 2.225 | Acc: 19.089% (4789/25088)\n",
      "Loss: 2.225 | Acc: 19.092% (4802/25152)\n",
      "Loss: 2.225 | Acc: 19.095% (4815/25216)\n",
      "Loss: 2.225 | Acc: 19.074% (4822/25280)\n",
      "Loss: 2.225 | Acc: 19.066% (4832/25344)\n",
      "Loss: 2.225 | Acc: 19.037% (4837/25408)\n",
      "Loss: 2.226 | Acc: 19.013% (4843/25472)\n",
      "Loss: 2.226 | Acc: 19.012% (4855/25536)\n",
      "Loss: 2.226 | Acc: 18.977% (4858/25600)\n",
      "Loss: 2.226 | Acc: 18.953% (4864/25664)\n",
      "Loss: 2.226 | Acc: 18.937% (4872/25728)\n",
      "Loss: 2.226 | Acc: 18.917% (4879/25792)\n",
      "Loss: 2.227 | Acc: 18.901% (4887/25856)\n",
      "Loss: 2.227 | Acc: 18.870% (4891/25920)\n",
      "Loss: 2.227 | Acc: 18.858% (4900/25984)\n",
      "Loss: 2.227 | Acc: 18.838% (4907/26048)\n",
      "Loss: 2.227 | Acc: 18.834% (4918/26112)\n",
      "Loss: 2.227 | Acc: 18.823% (4927/26176)\n",
      "Loss: 2.227 | Acc: 18.788% (4930/26240)\n",
      "Loss: 2.228 | Acc: 18.758% (4934/26304)\n",
      "Loss: 2.228 | Acc: 18.754% (4945/26368)\n",
      "Loss: 2.228 | Acc: 18.742% (4954/26432)\n",
      "Loss: 2.228 | Acc: 18.724% (4961/26496)\n",
      "Loss: 2.228 | Acc: 18.709% (4969/26560)\n",
      "Loss: 2.228 | Acc: 18.682% (4974/26624)\n",
      "Loss: 2.229 | Acc: 18.660% (4980/26688)\n",
      "Loss: 2.229 | Acc: 18.638% (4986/26752)\n",
      "Loss: 2.229 | Acc: 18.638% (4998/26816)\n",
      "Loss: 2.229 | Acc: 18.616% (5004/26880)\n",
      "Loss: 2.229 | Acc: 18.605% (5013/26944)\n",
      "Loss: 2.229 | Acc: 18.598% (5023/27008)\n",
      "Loss: 2.230 | Acc: 18.599% (5035/27072)\n",
      "Loss: 2.230 | Acc: 18.584% (5043/27136)\n",
      "Loss: 2.230 | Acc: 18.585% (5055/27200)\n",
      "Loss: 2.230 | Acc: 18.567% (5062/27264)\n",
      "Loss: 2.230 | Acc: 18.545% (5068/27328)\n",
      "Loss: 2.230 | Acc: 18.527% (5075/27392)\n",
      "Loss: 2.230 | Acc: 18.517% (5084/27456)\n",
      "Loss: 2.231 | Acc: 18.510% (5094/27520)\n",
      "Loss: 2.231 | Acc: 18.493% (5101/27584)\n",
      "Loss: 2.231 | Acc: 18.497% (5114/27648)\n",
      "Loss: 2.231 | Acc: 18.490% (5124/27712)\n",
      "Loss: 2.231 | Acc: 18.476% (5132/27776)\n",
      "Loss: 2.231 | Acc: 18.452% (5137/27840)\n",
      "Loss: 2.231 | Acc: 18.431% (5143/27904)\n",
      "Loss: 2.232 | Acc: 18.403% (5147/27968)\n",
      "Loss: 2.232 | Acc: 18.379% (5152/28032)\n",
      "Loss: 2.232 | Acc: 18.355% (5157/28096)\n",
      "Loss: 2.232 | Acc: 18.331% (5162/28160)\n",
      "Loss: 2.232 | Acc: 18.314% (5169/28224)\n",
      "Loss: 2.232 | Acc: 18.298% (5176/28288)\n",
      "Loss: 2.233 | Acc: 18.288% (5185/28352)\n",
      "Loss: 2.233 | Acc: 18.268% (5191/28416)\n",
      "Loss: 2.233 | Acc: 18.237% (5194/28480)\n",
      "Loss: 2.233 | Acc: 18.232% (5204/28544)\n",
      "Loss: 2.233 | Acc: 18.236% (5217/28608)\n",
      "Loss: 2.233 | Acc: 18.227% (5226/28672)\n",
      "Loss: 2.233 | Acc: 18.225% (5237/28736)\n",
      "Loss: 2.234 | Acc: 18.215% (5246/28800)\n",
      "Loss: 2.234 | Acc: 18.206% (5255/28864)\n",
      "Loss: 2.234 | Acc: 18.197% (5264/28928)\n",
      "Loss: 2.234 | Acc: 18.191% (5274/28992)\n",
      "Loss: 2.234 | Acc: 18.186% (5284/29056)\n",
      "Loss: 2.234 | Acc: 18.170% (5291/29120)\n",
      "Loss: 2.234 | Acc: 18.181% (5306/29184)\n",
      "Loss: 2.235 | Acc: 18.182% (5318/29248)\n",
      "Loss: 2.235 | Acc: 18.180% (5329/29312)\n",
      "Loss: 2.235 | Acc: 18.164% (5336/29376)\n",
      "Loss: 2.235 | Acc: 18.152% (5344/29440)\n",
      "Loss: 2.235 | Acc: 18.143% (5353/29504)\n",
      "Loss: 2.235 | Acc: 18.128% (5360/29568)\n",
      "Loss: 2.235 | Acc: 18.126% (5371/29632)\n",
      "Loss: 2.236 | Acc: 18.124% (5382/29696)\n",
      "Loss: 2.236 | Acc: 18.125% (5394/29760)\n",
      "Loss: 2.236 | Acc: 18.120% (5404/29824)\n",
      "Loss: 2.236 | Acc: 18.101% (5410/29888)\n",
      "Loss: 2.236 | Acc: 18.092% (5419/29952)\n",
      "Loss: 2.236 | Acc: 18.084% (5428/30016)\n",
      "Loss: 2.236 | Acc: 18.098% (5444/30080)\n",
      "Loss: 2.237 | Acc: 18.090% (5453/30144)\n",
      "Loss: 2.237 | Acc: 18.088% (5464/30208)\n",
      "Loss: 2.237 | Acc: 18.079% (5473/30272)\n",
      "Loss: 2.237 | Acc: 18.078% (5484/30336)\n",
      "Loss: 2.237 | Acc: 18.069% (5493/30400)\n",
      "Loss: 2.237 | Acc: 18.057% (5501/30464)\n",
      "Loss: 2.237 | Acc: 18.062% (5514/30528)\n",
      "Loss: 2.237 | Acc: 18.060% (5525/30592)\n",
      "Loss: 2.238 | Acc: 18.055% (5535/30656)\n",
      "Loss: 2.238 | Acc: 18.037% (5541/30720)\n",
      "Loss: 2.238 | Acc: 18.026% (5549/30784)\n",
      "Loss: 2.238 | Acc: 18.037% (5564/30848)\n",
      "Loss: 2.238 | Acc: 18.022% (5571/30912)\n",
      "Loss: 2.238 | Acc: 18.001% (5576/30976)\n",
      "Loss: 2.238 | Acc: 17.999% (5587/31040)\n",
      "Loss: 2.238 | Acc: 17.985% (5594/31104)\n",
      "Loss: 2.239 | Acc: 17.974% (5602/31168)\n",
      "Loss: 2.239 | Acc: 17.956% (5608/31232)\n",
      "Loss: 2.239 | Acc: 17.938% (5614/31296)\n",
      "Loss: 2.239 | Acc: 17.908% (5616/31360)\n",
      "Loss: 2.239 | Acc: 17.900% (5625/31424)\n",
      "Loss: 2.239 | Acc: 17.893% (5634/31488)\n",
      "Loss: 2.239 | Acc: 17.875% (5640/31552)\n",
      "Loss: 2.239 | Acc: 17.874% (5651/31616)\n",
      "Loss: 2.240 | Acc: 17.857% (5657/31680)\n",
      "Loss: 2.240 | Acc: 17.852% (5667/31744)\n",
      "Loss: 2.240 | Acc: 17.832% (5672/31808)\n",
      "Loss: 2.240 | Acc: 17.818% (5679/31872)\n",
      "Loss: 2.240 | Acc: 17.792% (5682/31936)\n",
      "Loss: 2.240 | Acc: 17.778% (5689/32000)\n",
      "Loss: 2.240 | Acc: 17.758% (5694/32064)\n",
      "Loss: 2.240 | Acc: 17.735% (5698/32128)\n",
      "Loss: 2.241 | Acc: 17.716% (5703/32192)\n",
      "Loss: 2.241 | Acc: 17.687% (5705/32256)\n",
      "Loss: 2.241 | Acc: 17.679% (5714/32320)\n",
      "Loss: 2.241 | Acc: 17.666% (5721/32384)\n",
      "Loss: 2.241 | Acc: 17.662% (5731/32448)\n",
      "Loss: 2.241 | Acc: 17.643% (5736/32512)\n",
      "Loss: 2.241 | Acc: 17.626% (5742/32576)\n",
      "Loss: 2.241 | Acc: 17.601% (5745/32640)\n",
      "Loss: 2.242 | Acc: 17.591% (5753/32704)\n",
      "Loss: 2.242 | Acc: 17.575% (5759/32768)\n",
      "Loss: 2.242 | Acc: 17.565% (5767/32832)\n",
      "Loss: 2.242 | Acc: 17.555% (5775/32896)\n",
      "Loss: 2.242 | Acc: 17.561% (5788/32960)\n",
      "Loss: 2.242 | Acc: 17.557% (5798/33024)\n",
      "Loss: 2.242 | Acc: 17.562% (5811/33088)\n",
      "Loss: 2.242 | Acc: 17.546% (5817/33152)\n",
      "Loss: 2.242 | Acc: 17.534% (5824/33216)\n",
      "Loss: 2.243 | Acc: 17.518% (5830/33280)\n",
      "Loss: 2.243 | Acc: 17.499% (5835/33344)\n",
      "Loss: 2.243 | Acc: 17.490% (5843/33408)\n",
      "Loss: 2.243 | Acc: 17.486% (5853/33472)\n",
      "Loss: 2.243 | Acc: 17.477% (5861/33536)\n",
      "Loss: 2.243 | Acc: 17.458% (5866/33600)\n",
      "Loss: 2.243 | Acc: 17.437% (5870/33664)\n",
      "Loss: 2.243 | Acc: 17.419% (5875/33728)\n",
      "Loss: 2.243 | Acc: 17.406% (5882/33792)\n",
      "Loss: 2.244 | Acc: 17.397% (5890/33856)\n",
      "Loss: 2.244 | Acc: 17.385% (5897/33920)\n",
      "Loss: 2.244 | Acc: 17.367% (5902/33984)\n",
      "Loss: 2.244 | Acc: 17.378% (5917/34048)\n",
      "Loss: 2.244 | Acc: 17.360% (5922/34112)\n",
      "Loss: 2.244 | Acc: 17.340% (5926/34176)\n",
      "Loss: 2.244 | Acc: 17.322% (5931/34240)\n",
      "Loss: 2.244 | Acc: 17.310% (5938/34304)\n",
      "Loss: 2.244 | Acc: 17.307% (5948/34368)\n",
      "Loss: 2.245 | Acc: 17.292% (5954/34432)\n",
      "Loss: 2.245 | Acc: 17.277% (5960/34496)\n",
      "Loss: 2.245 | Acc: 17.269% (5968/34560)\n",
      "Loss: 2.245 | Acc: 17.242% (5970/34624)\n",
      "Loss: 2.245 | Acc: 17.234% (5978/34688)\n",
      "Loss: 2.245 | Acc: 17.219% (5984/34752)\n",
      "Loss: 2.245 | Acc: 17.196% (5987/34816)\n",
      "Loss: 2.245 | Acc: 17.182% (5993/34880)\n",
      "Loss: 2.245 | Acc: 17.170% (6000/34944)\n",
      "Loss: 2.246 | Acc: 17.153% (6005/35008)\n",
      "Loss: 2.246 | Acc: 17.142% (6012/35072)\n",
      "Loss: 2.246 | Acc: 17.122% (6016/35136)\n",
      "Loss: 2.246 | Acc: 17.111% (6023/35200)\n",
      "Loss: 2.246 | Acc: 17.100% (6030/35264)\n",
      "Loss: 2.246 | Acc: 17.077% (6033/35328)\n",
      "Loss: 2.246 | Acc: 17.060% (6038/35392)\n",
      "Loss: 2.246 | Acc: 17.046% (6044/35456)\n",
      "Loss: 2.246 | Acc: 17.024% (6047/35520)\n",
      "Loss: 2.246 | Acc: 17.010% (6053/35584)\n",
      "Loss: 2.247 | Acc: 17.000% (6060/35648)\n",
      "Loss: 2.247 | Acc: 16.994% (6069/35712)\n",
      "Loss: 2.247 | Acc: 16.975% (6073/35776)\n",
      "Loss: 2.247 | Acc: 16.964% (6080/35840)\n",
      "Loss: 2.247 | Acc: 16.945% (6084/35904)\n",
      "Loss: 2.247 | Acc: 16.932% (6090/35968)\n",
      "Loss: 2.247 | Acc: 16.927% (6099/36032)\n",
      "Loss: 2.247 | Acc: 16.905% (6102/36096)\n",
      "Loss: 2.247 | Acc: 16.900% (6111/36160)\n",
      "Loss: 2.247 | Acc: 16.892% (6119/36224)\n",
      "Loss: 2.248 | Acc: 16.873% (6123/36288)\n",
      "Loss: 2.248 | Acc: 16.857% (6128/36352)\n",
      "Loss: 2.248 | Acc: 16.833% (6130/36416)\n",
      "Loss: 2.248 | Acc: 16.826% (6138/36480)\n",
      "Loss: 2.248 | Acc: 16.813% (6144/36544)\n",
      "Loss: 2.248 | Acc: 16.808% (6153/36608)\n",
      "Loss: 2.248 | Acc: 16.806% (6163/36672)\n",
      "Loss: 2.248 | Acc: 16.787% (6167/36736)\n",
      "Loss: 2.248 | Acc: 16.785% (6177/36800)\n",
      "Loss: 2.248 | Acc: 16.772% (6183/36864)\n",
      "Loss: 2.248 | Acc: 16.760% (6189/36928)\n",
      "Loss: 2.249 | Acc: 16.747% (6195/36992)\n",
      "Loss: 2.249 | Acc: 16.734% (6201/37056)\n",
      "Loss: 2.249 | Acc: 16.735% (6212/37120)\n",
      "Loss: 2.249 | Acc: 16.722% (6218/37184)\n",
      "Loss: 2.249 | Acc: 16.696% (6219/37248)\n",
      "Loss: 2.249 | Acc: 16.676% (6222/37312)\n",
      "Loss: 2.249 | Acc: 16.666% (6229/37376)\n",
      "Loss: 2.249 | Acc: 16.653% (6235/37440)\n",
      "Loss: 2.249 | Acc: 16.644% (6242/37504)\n",
      "Loss: 2.249 | Acc: 16.639% (6251/37568)\n",
      "Loss: 2.249 | Acc: 16.627% (6257/37632)\n",
      "Loss: 2.250 | Acc: 16.609% (6261/37696)\n",
      "Loss: 2.250 | Acc: 16.613% (6273/37760)\n",
      "Loss: 2.250 | Acc: 16.595% (6277/37824)\n",
      "Loss: 2.250 | Acc: 16.588% (6285/37888)\n",
      "Loss: 2.250 | Acc: 16.581% (6293/37952)\n",
      "Loss: 2.250 | Acc: 16.580% (6303/38016)\n",
      "Loss: 2.250 | Acc: 16.576% (6312/38080)\n",
      "Loss: 2.250 | Acc: 16.577% (6323/38144)\n",
      "Loss: 2.250 | Acc: 16.565% (6329/38208)\n",
      "Loss: 2.250 | Acc: 16.558% (6337/38272)\n",
      "Loss: 2.250 | Acc: 16.538% (6340/38336)\n",
      "Loss: 2.251 | Acc: 16.529% (6347/38400)\n",
      "Loss: 2.251 | Acc: 16.512% (6351/38464)\n",
      "Loss: 2.251 | Acc: 16.507% (6360/38528)\n",
      "Loss: 2.251 | Acc: 16.493% (6365/38592)\n",
      "Loss: 2.251 | Acc: 16.479% (6370/38656)\n",
      "Loss: 2.251 | Acc: 16.467% (6376/38720)\n",
      "Loss: 2.251 | Acc: 16.455% (6382/38784)\n",
      "Loss: 2.251 | Acc: 16.444% (6388/38848)\n",
      "Loss: 2.251 | Acc: 16.432% (6394/38912)\n",
      "Loss: 2.251 | Acc: 16.425% (6402/38976)\n",
      "Loss: 2.251 | Acc: 16.414% (6408/39040)\n",
      "Loss: 2.251 | Acc: 16.402% (6414/39104)\n",
      "Loss: 2.252 | Acc: 16.383% (6417/39168)\n",
      "Loss: 2.252 | Acc: 16.367% (6421/39232)\n",
      "Loss: 2.252 | Acc: 16.348% (6424/39296)\n",
      "Loss: 2.252 | Acc: 16.341% (6432/39360)\n",
      "Loss: 2.252 | Acc: 16.338% (6441/39424)\n",
      "Loss: 2.252 | Acc: 16.326% (6447/39488)\n",
      "Loss: 2.252 | Acc: 16.315% (6453/39552)\n",
      "Loss: 2.252 | Acc: 16.301% (6458/39616)\n",
      "Loss: 2.252 | Acc: 16.295% (6466/39680)\n",
      "Loss: 2.252 | Acc: 16.289% (6474/39744)\n",
      "Loss: 2.252 | Acc: 16.281% (6481/39808)\n",
      "Loss: 2.252 | Acc: 16.270% (6487/39872)\n",
      "Loss: 2.253 | Acc: 16.259% (6493/39936)\n",
      "Loss: 2.253 | Acc: 16.245% (6498/40000)\n",
      "Loss: 2.253 | Acc: 16.237% (6505/40064)\n",
      "Loss: 2.253 | Acc: 16.218% (6508/40128)\n",
      "Loss: 2.253 | Acc: 16.207% (6514/40192)\n",
      "Loss: 2.253 | Acc: 16.194% (6519/40256)\n",
      "Loss: 2.253 | Acc: 16.186% (6526/40320)\n",
      "Loss: 2.253 | Acc: 16.185% (6536/40384)\n",
      "Loss: 2.253 | Acc: 16.179% (6544/40448)\n",
      "Loss: 2.253 | Acc: 16.163% (6548/40512)\n",
      "Loss: 2.253 | Acc: 16.160% (6557/40576)\n",
      "Loss: 2.253 | Acc: 16.147% (6562/40640)\n",
      "Loss: 2.253 | Acc: 16.129% (6565/40704)\n",
      "Loss: 2.254 | Acc: 16.125% (6574/40768)\n",
      "Loss: 2.254 | Acc: 16.120% (6582/40832)\n",
      "Loss: 2.254 | Acc: 16.112% (6589/40896)\n",
      "Loss: 2.254 | Acc: 16.106% (6597/40960)\n",
      "Loss: 2.254 | Acc: 16.091% (6601/41024)\n",
      "Loss: 2.254 | Acc: 16.080% (6607/41088)\n",
      "Loss: 2.254 | Acc: 16.075% (6615/41152)\n",
      "Loss: 2.254 | Acc: 16.062% (6620/41216)\n",
      "Loss: 2.254 | Acc: 16.054% (6627/41280)\n",
      "Loss: 2.254 | Acc: 16.046% (6634/41344)\n",
      "Loss: 2.254 | Acc: 16.031% (6638/41408)\n",
      "Loss: 2.254 | Acc: 16.018% (6643/41472)\n",
      "Loss: 2.254 | Acc: 16.013% (6651/41536)\n",
      "Loss: 2.255 | Acc: 16.000% (6656/41600)\n",
      "Loss: 2.255 | Acc: 15.985% (6660/41664)\n",
      "Loss: 2.255 | Acc: 15.975% (6666/41728)\n",
      "Loss: 2.255 | Acc: 15.962% (6671/41792)\n",
      "Loss: 2.255 | Acc: 15.943% (6673/41856)\n",
      "Loss: 2.255 | Acc: 15.930% (6678/41920)\n",
      "Loss: 2.255 | Acc: 15.920% (6684/41984)\n",
      "Loss: 2.255 | Acc: 15.908% (6689/42048)\n",
      "Loss: 2.255 | Acc: 15.903% (6697/42112)\n",
      "Loss: 2.255 | Acc: 15.902% (6707/42176)\n",
      "Loss: 2.255 | Acc: 15.895% (6714/42240)\n",
      "Loss: 2.255 | Acc: 15.880% (6718/42304)\n",
      "Loss: 2.255 | Acc: 15.875% (6726/42368)\n",
      "Loss: 2.255 | Acc: 15.861% (6730/42432)\n",
      "Loss: 2.256 | Acc: 15.858% (6739/42496)\n",
      "Loss: 2.256 | Acc: 15.851% (6746/42560)\n",
      "Loss: 2.256 | Acc: 15.846% (6754/42624)\n",
      "Loss: 2.256 | Acc: 15.831% (6758/42688)\n",
      "Loss: 2.256 | Acc: 15.819% (6763/42752)\n",
      "Loss: 2.256 | Acc: 15.810% (6769/42816)\n",
      "Loss: 2.256 | Acc: 15.795% (6773/42880)\n",
      "Loss: 2.256 | Acc: 15.781% (6777/42944)\n",
      "Loss: 2.256 | Acc: 15.776% (6785/43008)\n",
      "Loss: 2.256 | Acc: 15.762% (6789/43072)\n",
      "Loss: 2.256 | Acc: 15.741% (6790/43136)\n",
      "Loss: 2.256 | Acc: 15.729% (6795/43200)\n",
      "Loss: 2.256 | Acc: 15.715% (6799/43264)\n",
      "Loss: 2.256 | Acc: 15.708% (6806/43328)\n",
      "Loss: 2.257 | Acc: 15.701% (6813/43392)\n",
      "Loss: 2.257 | Acc: 15.703% (6824/43456)\n",
      "Loss: 2.257 | Acc: 15.699% (6832/43520)\n",
      "Loss: 2.257 | Acc: 15.689% (6838/43584)\n",
      "Loss: 2.257 | Acc: 15.687% (6847/43648)\n",
      "Loss: 2.257 | Acc: 15.684% (6856/43712)\n",
      "Loss: 2.257 | Acc: 15.678% (6863/43776)\n",
      "Loss: 2.257 | Acc: 15.671% (6870/43840)\n",
      "Loss: 2.257 | Acc: 15.659% (6875/43904)\n",
      "Loss: 2.257 | Acc: 15.645% (6879/43968)\n",
      "Loss: 2.257 | Acc: 15.630% (6882/44032)\n",
      "Loss: 2.257 | Acc: 15.620% (6888/44096)\n",
      "Loss: 2.257 | Acc: 15.611% (6894/44160)\n",
      "Loss: 2.257 | Acc: 15.600% (6899/44224)\n",
      "Loss: 2.257 | Acc: 15.591% (6905/44288)\n",
      "Loss: 2.258 | Acc: 15.587% (6913/44352)\n",
      "Loss: 2.258 | Acc: 15.569% (6915/44416)\n",
      "Loss: 2.258 | Acc: 15.551% (6917/44480)\n",
      "Loss: 2.258 | Acc: 15.544% (6924/44544)\n",
      "Loss: 2.258 | Acc: 15.529% (6927/44608)\n",
      "Loss: 2.258 | Acc: 15.529% (6937/44672)\n",
      "Loss: 2.258 | Acc: 15.527% (6946/44736)\n",
      "Loss: 2.258 | Acc: 15.513% (6950/44800)\n",
      "Loss: 2.258 | Acc: 15.502% (6955/44864)\n",
      "Loss: 2.258 | Acc: 15.494% (6961/44928)\n",
      "Loss: 2.258 | Acc: 15.487% (6968/44992)\n",
      "Loss: 2.258 | Acc: 15.481% (6975/45056)\n",
      "Loss: 2.258 | Acc: 15.470% (6980/45120)\n",
      "Loss: 2.258 | Acc: 15.472% (6991/45184)\n",
      "Loss: 2.258 | Acc: 15.468% (6999/45248)\n",
      "Loss: 2.258 | Acc: 15.464% (7007/45312)\n",
      "Loss: 2.259 | Acc: 15.451% (7011/45376)\n",
      "Loss: 2.259 | Acc: 15.442% (7017/45440)\n",
      "Loss: 2.259 | Acc: 15.427% (7020/45504)\n",
      "Loss: 2.259 | Acc: 15.419% (7026/45568)\n",
      "Loss: 2.259 | Acc: 15.410% (7032/45632)\n",
      "Loss: 2.259 | Acc: 15.404% (7039/45696)\n",
      "Loss: 2.259 | Acc: 15.404% (7049/45760)\n",
      "Loss: 2.259 | Acc: 15.394% (7054/45824)\n",
      "Loss: 2.259 | Acc: 15.392% (7063/45888)\n",
      "Loss: 2.259 | Acc: 15.392% (7073/45952)\n",
      "Loss: 2.259 | Acc: 15.390% (7082/46016)\n",
      "Loss: 2.259 | Acc: 15.375% (7085/46080)\n",
      "Loss: 2.259 | Acc: 15.367% (7091/46144)\n",
      "Loss: 2.259 | Acc: 15.357% (7096/46208)\n",
      "Loss: 2.259 | Acc: 15.346% (7101/46272)\n",
      "Loss: 2.259 | Acc: 15.336% (7106/46336)\n",
      "Loss: 2.260 | Acc: 15.328% (7112/46400)\n",
      "Loss: 2.260 | Acc: 15.319% (7118/46464)\n",
      "Loss: 2.260 | Acc: 15.318% (7127/46528)\n",
      "Loss: 2.260 | Acc: 15.314% (7135/46592)\n",
      "Loss: 2.260 | Acc: 15.310% (7143/46656)\n",
      "Loss: 2.260 | Acc: 15.304% (7150/46720)\n",
      "Loss: 2.260 | Acc: 15.309% (7162/46784)\n",
      "Loss: 2.260 | Acc: 15.305% (7170/46848)\n",
      "Loss: 2.260 | Acc: 15.303% (7179/46912)\n",
      "Loss: 2.260 | Acc: 15.293% (7184/46976)\n",
      "Loss: 2.260 | Acc: 15.289% (7192/47040)\n",
      "Loss: 2.260 | Acc: 15.279% (7197/47104)\n",
      "Loss: 2.260 | Acc: 15.269% (7202/47168)\n",
      "Loss: 2.260 | Acc: 15.261% (7208/47232)\n",
      "Loss: 2.260 | Acc: 15.251% (7213/47296)\n",
      "Loss: 2.260 | Acc: 15.232% (7214/47360)\n",
      "Loss: 2.260 | Acc: 15.235% (7225/47424)\n",
      "Loss: 2.261 | Acc: 15.221% (7228/47488)\n",
      "Loss: 2.261 | Acc: 15.215% (7235/47552)\n",
      "Loss: 2.261 | Acc: 15.203% (7239/47616)\n",
      "Loss: 2.261 | Acc: 15.199% (7247/47680)\n",
      "Loss: 2.261 | Acc: 15.191% (7253/47744)\n",
      "Loss: 2.261 | Acc: 15.186% (7260/47808)\n",
      "Loss: 2.261 | Acc: 15.184% (7269/47872)\n",
      "Loss: 2.261 | Acc: 15.176% (7275/47936)\n",
      "Loss: 2.261 | Acc: 15.162% (7278/48000)\n",
      "Loss: 2.261 | Acc: 15.163% (7288/48064)\n",
      "Loss: 2.261 | Acc: 15.151% (7292/48128)\n",
      "Loss: 2.261 | Acc: 15.135% (7294/48192)\n",
      "Loss: 2.261 | Acc: 15.140% (7306/48256)\n",
      "Loss: 2.261 | Acc: 15.128% (7310/48320)\n",
      "Loss: 2.261 | Acc: 15.123% (7317/48384)\n",
      "Loss: 2.261 | Acc: 15.113% (7322/48448)\n",
      "Loss: 2.261 | Acc: 15.101% (7326/48512)\n",
      "Loss: 2.261 | Acc: 15.084% (7327/48576)\n",
      "Loss: 2.262 | Acc: 15.082% (7336/48640)\n",
      "Loss: 2.262 | Acc: 15.069% (7339/48704)\n",
      "Loss: 2.262 | Acc: 15.063% (7346/48768)\n",
      "Loss: 2.262 | Acc: 15.050% (7349/48832)\n",
      "Loss: 2.262 | Acc: 15.038% (7353/48896)\n",
      "Loss: 2.262 | Acc: 15.033% (7360/48960)\n",
      "Loss: 2.262 | Acc: 15.027% (7363/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 15.026530612244898\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.305 | Acc: 10.938% (7/64)\n",
      "Loss: 2.303 | Acc: 13.281% (17/128)\n",
      "Loss: 2.303 | Acc: 13.021% (25/192)\n",
      "Loss: 2.303 | Acc: 13.281% (34/256)\n",
      "Loss: 2.303 | Acc: 12.500% (40/320)\n",
      "Loss: 2.303 | Acc: 11.979% (46/384)\n",
      "Loss: 2.303 | Acc: 11.384% (51/448)\n",
      "Loss: 2.303 | Acc: 11.133% (57/512)\n",
      "Loss: 2.303 | Acc: 10.417% (60/576)\n",
      "Loss: 2.303 | Acc: 10.781% (69/640)\n",
      "Loss: 2.303 | Acc: 10.511% (74/704)\n",
      "Loss: 2.303 | Acc: 10.807% (83/768)\n",
      "Loss: 2.303 | Acc: 10.577% (88/832)\n",
      "Loss: 2.303 | Acc: 10.268% (92/896)\n",
      "Loss: 2.303 | Acc: 10.521% (101/960)\n",
      "Loss: 2.303 | Acc: 10.742% (110/1024)\n",
      "Loss: 2.303 | Acc: 10.386% (113/1088)\n",
      "Loss: 2.303 | Acc: 10.590% (122/1152)\n",
      "Loss: 2.303 | Acc: 10.609% (129/1216)\n",
      "Loss: 2.303 | Acc: 11.016% (141/1280)\n",
      "Loss: 2.303 | Acc: 10.938% (147/1344)\n",
      "Loss: 2.303 | Acc: 10.938% (154/1408)\n",
      "Loss: 2.303 | Acc: 10.870% (160/1472)\n",
      "Loss: 2.303 | Acc: 11.003% (169/1536)\n",
      "Loss: 2.303 | Acc: 10.750% (172/1600)\n",
      "Loss: 2.303 | Acc: 10.697% (178/1664)\n",
      "Loss: 2.303 | Acc: 10.764% (186/1728)\n",
      "Loss: 2.303 | Acc: 10.714% (192/1792)\n",
      "Loss: 2.303 | Acc: 10.722% (199/1856)\n",
      "Loss: 2.303 | Acc: 10.938% (210/1920)\n",
      "Loss: 2.303 | Acc: 10.837% (215/1984)\n",
      "Loss: 2.303 | Acc: 10.840% (222/2048)\n",
      "Loss: 2.303 | Acc: 10.701% (226/2112)\n",
      "Loss: 2.303 | Acc: 10.708% (233/2176)\n",
      "Loss: 2.303 | Acc: 10.670% (239/2240)\n",
      "Loss: 2.303 | Acc: 10.677% (246/2304)\n",
      "Loss: 2.303 | Acc: 10.557% (250/2368)\n",
      "Loss: 2.303 | Acc: 10.691% (260/2432)\n",
      "Loss: 2.303 | Acc: 10.737% (268/2496)\n",
      "Loss: 2.303 | Acc: 10.781% (276/2560)\n",
      "Loss: 2.303 | Acc: 10.823% (284/2624)\n",
      "Loss: 2.303 | Acc: 10.677% (287/2688)\n",
      "Loss: 2.303 | Acc: 10.719% (295/2752)\n",
      "Loss: 2.303 | Acc: 10.547% (297/2816)\n",
      "Loss: 2.303 | Acc: 10.451% (301/2880)\n",
      "Loss: 2.303 | Acc: 10.530% (310/2944)\n",
      "Loss: 2.303 | Acc: 10.505% (316/3008)\n",
      "Loss: 2.303 | Acc: 10.482% (322/3072)\n",
      "Loss: 2.303 | Acc: 10.523% (330/3136)\n",
      "Loss: 2.303 | Acc: 10.500% (336/3200)\n",
      "Loss: 2.303 | Acc: 10.447% (341/3264)\n",
      "Loss: 2.303 | Acc: 10.487% (349/3328)\n",
      "Loss: 2.303 | Acc: 10.495% (356/3392)\n",
      "Loss: 2.303 | Acc: 10.388% (359/3456)\n",
      "Loss: 2.303 | Acc: 10.312% (363/3520)\n",
      "Loss: 2.303 | Acc: 10.268% (368/3584)\n",
      "Loss: 2.303 | Acc: 10.252% (374/3648)\n",
      "Loss: 2.303 | Acc: 10.237% (380/3712)\n",
      "Loss: 2.303 | Acc: 10.169% (384/3776)\n",
      "Loss: 2.303 | Acc: 10.182% (391/3840)\n",
      "Loss: 2.303 | Acc: 10.220% (399/3904)\n",
      "Loss: 2.303 | Acc: 10.207% (405/3968)\n",
      "Loss: 2.303 | Acc: 10.268% (414/4032)\n",
      "Loss: 2.303 | Acc: 10.254% (420/4096)\n",
      "Loss: 2.303 | Acc: 10.288% (428/4160)\n",
      "Loss: 2.303 | Acc: 10.275% (434/4224)\n",
      "Loss: 2.303 | Acc: 10.308% (442/4288)\n",
      "Loss: 2.303 | Acc: 10.317% (449/4352)\n",
      "Loss: 2.303 | Acc: 10.326% (456/4416)\n",
      "Loss: 2.303 | Acc: 10.268% (460/4480)\n",
      "Loss: 2.303 | Acc: 10.343% (470/4544)\n",
      "Loss: 2.303 | Acc: 10.308% (475/4608)\n",
      "Loss: 2.303 | Acc: 10.295% (481/4672)\n",
      "Loss: 2.303 | Acc: 10.262% (486/4736)\n",
      "Loss: 2.303 | Acc: 10.188% (489/4800)\n",
      "Loss: 2.303 | Acc: 10.136% (493/4864)\n",
      "Loss: 2.303 | Acc: 10.146% (500/4928)\n",
      "Loss: 2.303 | Acc: 10.096% (504/4992)\n",
      "Loss: 2.303 | Acc: 10.028% (507/5056)\n",
      "Loss: 2.303 | Acc: 10.020% (513/5120)\n",
      "Loss: 2.303 | Acc: 9.954% (516/5184)\n",
      "Loss: 2.303 | Acc: 9.928% (521/5248)\n",
      "Loss: 2.303 | Acc: 9.902% (526/5312)\n",
      "Loss: 2.303 | Acc: 9.877% (531/5376)\n",
      "Loss: 2.303 | Acc: 9.835% (535/5440)\n",
      "Loss: 2.303 | Acc: 9.811% (540/5504)\n",
      "Loss: 2.303 | Acc: 9.752% (543/5568)\n",
      "Loss: 2.303 | Acc: 9.783% (551/5632)\n",
      "Loss: 2.303 | Acc: 9.761% (556/5696)\n",
      "Loss: 2.303 | Acc: 9.722% (560/5760)\n",
      "Loss: 2.303 | Acc: 9.736% (567/5824)\n",
      "Loss: 2.303 | Acc: 9.681% (570/5888)\n",
      "Loss: 2.303 | Acc: 9.661% (575/5952)\n",
      "Loss: 2.303 | Acc: 9.658% (581/6016)\n",
      "Loss: 2.303 | Acc: 9.688% (589/6080)\n",
      "Loss: 2.303 | Acc: 9.668% (594/6144)\n",
      "Loss: 2.303 | Acc: 9.617% (597/6208)\n",
      "Loss: 2.303 | Acc: 9.630% (604/6272)\n",
      "Loss: 2.303 | Acc: 9.628% (610/6336)\n",
      "Loss: 2.303 | Acc: 9.656% (618/6400)\n",
      "Loss: 2.303 | Acc: 9.731% (629/6464)\n",
      "Loss: 2.303 | Acc: 9.773% (638/6528)\n",
      "Loss: 2.303 | Acc: 9.739% (642/6592)\n",
      "Loss: 2.303 | Acc: 9.721% (647/6656)\n",
      "Loss: 2.303 | Acc: 9.717% (653/6720)\n",
      "Loss: 2.303 | Acc: 9.817% (666/6784)\n",
      "Loss: 2.303 | Acc: 9.828% (673/6848)\n",
      "Loss: 2.303 | Acc: 9.751% (674/6912)\n",
      "Loss: 2.303 | Acc: 9.776% (682/6976)\n",
      "Loss: 2.303 | Acc: 9.773% (688/7040)\n",
      "Loss: 2.303 | Acc: 9.783% (695/7104)\n",
      "Loss: 2.303 | Acc: 9.738% (698/7168)\n",
      "Loss: 2.303 | Acc: 9.748% (705/7232)\n",
      "Loss: 2.303 | Acc: 9.718% (709/7296)\n",
      "Loss: 2.303 | Acc: 9.715% (715/7360)\n",
      "Loss: 2.303 | Acc: 9.725% (722/7424)\n",
      "Loss: 2.303 | Acc: 9.736% (729/7488)\n",
      "Loss: 2.303 | Acc: 9.746% (736/7552)\n",
      "Loss: 2.303 | Acc: 9.769% (744/7616)\n",
      "Loss: 2.303 | Acc: 9.779% (751/7680)\n",
      "Loss: 2.303 | Acc: 9.775% (757/7744)\n",
      "Loss: 2.303 | Acc: 9.772% (763/7808)\n",
      "Loss: 2.303 | Acc: 9.756% (768/7872)\n",
      "Loss: 2.303 | Acc: 9.715% (771/7936)\n",
      "Loss: 2.303 | Acc: 9.700% (776/8000)\n",
      "Loss: 2.303 | Acc: 9.697% (782/8064)\n",
      "Loss: 2.303 | Acc: 9.707% (789/8128)\n",
      "Loss: 2.303 | Acc: 9.790% (802/8192)\n",
      "Loss: 2.303 | Acc: 9.835% (812/8256)\n",
      "Loss: 2.303 | Acc: 9.868% (821/8320)\n",
      "Loss: 2.303 | Acc: 9.876% (828/8384)\n",
      "Loss: 2.303 | Acc: 9.896% (836/8448)\n",
      "Loss: 2.303 | Acc: 9.868% (840/8512)\n",
      "Loss: 2.303 | Acc: 9.911% (850/8576)\n",
      "Loss: 2.303 | Acc: 9.907% (856/8640)\n",
      "Loss: 2.303 | Acc: 9.938% (865/8704)\n",
      "Loss: 2.303 | Acc: 9.945% (872/8768)\n",
      "Loss: 2.303 | Acc: 9.907% (875/8832)\n",
      "Loss: 2.303 | Acc: 9.960% (886/8896)\n",
      "Loss: 2.303 | Acc: 9.955% (892/8960)\n",
      "Loss: 2.303 | Acc: 9.996% (902/9024)\n",
      "Loss: 2.303 | Acc: 9.980% (907/9088)\n",
      "Loss: 2.303 | Acc: 9.976% (913/9152)\n",
      "Loss: 2.303 | Acc: 9.983% (920/9216)\n",
      "Loss: 2.303 | Acc: 10.000% (928/9280)\n",
      "Loss: 2.303 | Acc: 9.996% (934/9344)\n",
      "Loss: 2.303 | Acc: 10.002% (941/9408)\n",
      "Loss: 2.303 | Acc: 9.987% (946/9472)\n",
      "Loss: 2.303 | Acc: 9.994% (953/9536)\n",
      "Loss: 2.303 | Acc: 9.958% (956/9600)\n",
      "Loss: 2.303 | Acc: 9.965% (963/9664)\n",
      "Loss: 2.303 | Acc: 9.971% (970/9728)\n",
      "Loss: 2.303 | Acc: 9.967% (976/9792)\n",
      "Loss: 2.303 | Acc: 9.963% (982/9856)\n",
      "Loss: 2.303 | Acc: 9.980% (990/9920)\n",
      "Loss: 2.303 | Acc: 9.986% (997/9984)\n",
      "Loss: 2.303 | Acc: 10.000% (1000/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 2.303 | Acc: 9.375% (6/64)\n",
      "Loss: 2.303 | Acc: 10.156% (13/128)\n",
      "Loss: 2.304 | Acc: 8.854% (17/192)\n",
      "Loss: 2.304 | Acc: 9.766% (25/256)\n",
      "Loss: 2.304 | Acc: 9.062% (29/320)\n",
      "Loss: 2.304 | Acc: 8.333% (32/384)\n",
      "Loss: 2.304 | Acc: 8.036% (36/448)\n",
      "Loss: 2.304 | Acc: 8.789% (45/512)\n",
      "Loss: 2.303 | Acc: 8.681% (50/576)\n",
      "Loss: 2.303 | Acc: 8.438% (54/640)\n",
      "Loss: 2.303 | Acc: 8.381% (59/704)\n",
      "Loss: 2.303 | Acc: 8.333% (64/768)\n",
      "Loss: 2.303 | Acc: 8.293% (69/832)\n",
      "Loss: 2.303 | Acc: 8.705% (78/896)\n",
      "Loss: 2.303 | Acc: 8.646% (83/960)\n",
      "Loss: 2.303 | Acc: 9.082% (93/1024)\n",
      "Loss: 2.303 | Acc: 9.099% (99/1088)\n",
      "Loss: 2.303 | Acc: 9.115% (105/1152)\n",
      "Loss: 2.303 | Acc: 9.046% (110/1216)\n",
      "Loss: 2.303 | Acc: 9.141% (117/1280)\n",
      "Loss: 2.303 | Acc: 9.301% (125/1344)\n",
      "Loss: 2.303 | Acc: 9.446% (133/1408)\n",
      "Loss: 2.303 | Acc: 9.443% (139/1472)\n",
      "Loss: 2.303 | Acc: 9.440% (145/1536)\n",
      "Loss: 2.303 | Acc: 9.375% (150/1600)\n",
      "Loss: 2.303 | Acc: 9.315% (155/1664)\n",
      "Loss: 2.303 | Acc: 9.259% (160/1728)\n",
      "Loss: 2.303 | Acc: 9.431% (169/1792)\n",
      "Loss: 2.303 | Acc: 9.267% (172/1856)\n",
      "Loss: 2.303 | Acc: 9.375% (180/1920)\n",
      "Loss: 2.303 | Acc: 9.526% (189/1984)\n",
      "Loss: 2.303 | Acc: 9.521% (195/2048)\n",
      "Loss: 2.303 | Acc: 9.517% (201/2112)\n",
      "Loss: 2.303 | Acc: 9.605% (209/2176)\n",
      "Loss: 2.303 | Acc: 9.509% (213/2240)\n",
      "Loss: 2.303 | Acc: 9.592% (221/2304)\n",
      "Loss: 2.303 | Acc: 9.628% (228/2368)\n",
      "Loss: 2.303 | Acc: 9.663% (235/2432)\n",
      "Loss: 2.303 | Acc: 9.615% (240/2496)\n",
      "Loss: 2.303 | Acc: 9.648% (247/2560)\n",
      "Loss: 2.303 | Acc: 9.832% (258/2624)\n",
      "Loss: 2.303 | Acc: 9.747% (262/2688)\n",
      "Loss: 2.303 | Acc: 9.775% (269/2752)\n",
      "Loss: 2.303 | Acc: 9.695% (273/2816)\n",
      "Loss: 2.303 | Acc: 9.722% (280/2880)\n",
      "Loss: 2.303 | Acc: 9.749% (287/2944)\n",
      "Loss: 2.303 | Acc: 9.741% (293/3008)\n",
      "Loss: 2.303 | Acc: 9.798% (301/3072)\n",
      "Loss: 2.303 | Acc: 9.853% (309/3136)\n",
      "Loss: 2.303 | Acc: 9.906% (317/3200)\n",
      "Loss: 2.303 | Acc: 9.804% (320/3264)\n",
      "Loss: 2.303 | Acc: 9.856% (328/3328)\n",
      "Loss: 2.303 | Acc: 9.847% (334/3392)\n",
      "Loss: 2.303 | Acc: 9.751% (337/3456)\n",
      "Loss: 2.303 | Acc: 9.744% (343/3520)\n",
      "Loss: 2.303 | Acc: 9.794% (351/3584)\n",
      "Loss: 2.303 | Acc: 9.704% (354/3648)\n",
      "Loss: 2.303 | Acc: 9.725% (361/3712)\n",
      "Loss: 2.303 | Acc: 9.666% (365/3776)\n",
      "Loss: 2.303 | Acc: 9.583% (368/3840)\n",
      "Loss: 2.303 | Acc: 9.580% (374/3904)\n",
      "Loss: 2.303 | Acc: 9.627% (382/3968)\n",
      "Loss: 2.303 | Acc: 9.573% (386/4032)\n",
      "Loss: 2.303 | Acc: 9.570% (392/4096)\n",
      "Loss: 2.303 | Acc: 9.615% (400/4160)\n",
      "Loss: 2.303 | Acc: 9.612% (406/4224)\n",
      "Loss: 2.303 | Acc: 9.585% (411/4288)\n",
      "Loss: 2.303 | Acc: 9.559% (416/4352)\n",
      "Loss: 2.303 | Acc: 9.579% (423/4416)\n",
      "Loss: 2.303 | Acc: 9.487% (425/4480)\n",
      "Loss: 2.303 | Acc: 9.463% (430/4544)\n",
      "Loss: 2.303 | Acc: 9.440% (435/4608)\n",
      "Loss: 2.303 | Acc: 9.418% (440/4672)\n",
      "Loss: 2.303 | Acc: 9.481% (449/4736)\n",
      "Loss: 2.303 | Acc: 9.521% (457/4800)\n",
      "Loss: 2.303 | Acc: 9.498% (462/4864)\n",
      "Loss: 2.303 | Acc: 9.497% (468/4928)\n",
      "Loss: 2.303 | Acc: 9.475% (473/4992)\n",
      "Loss: 2.303 | Acc: 9.513% (481/5056)\n",
      "Loss: 2.303 | Acc: 9.531% (488/5120)\n",
      "Loss: 2.303 | Acc: 9.491% (492/5184)\n",
      "Loss: 2.303 | Acc: 9.451% (496/5248)\n",
      "Loss: 2.303 | Acc: 9.450% (502/5312)\n",
      "Loss: 2.303 | Acc: 9.449% (508/5376)\n",
      "Loss: 2.303 | Acc: 9.449% (514/5440)\n",
      "Loss: 2.303 | Acc: 9.430% (519/5504)\n",
      "Loss: 2.303 | Acc: 9.447% (526/5568)\n",
      "Loss: 2.303 | Acc: 9.464% (533/5632)\n",
      "Loss: 2.303 | Acc: 9.533% (543/5696)\n",
      "Loss: 2.303 | Acc: 9.618% (554/5760)\n",
      "Loss: 2.303 | Acc: 9.650% (562/5824)\n",
      "Loss: 2.303 | Acc: 9.630% (567/5888)\n",
      "Loss: 2.303 | Acc: 9.661% (575/5952)\n",
      "Loss: 2.303 | Acc: 9.724% (585/6016)\n",
      "Loss: 2.303 | Acc: 9.704% (590/6080)\n",
      "Loss: 2.303 | Acc: 9.701% (596/6144)\n",
      "Loss: 2.303 | Acc: 9.745% (605/6208)\n",
      "Loss: 2.303 | Acc: 9.694% (608/6272)\n",
      "Loss: 2.303 | Acc: 9.675% (613/6336)\n",
      "Loss: 2.303 | Acc: 9.672% (619/6400)\n",
      "Loss: 2.303 | Acc: 9.623% (622/6464)\n",
      "Loss: 2.303 | Acc: 9.727% (635/6528)\n",
      "Loss: 2.303 | Acc: 9.694% (639/6592)\n",
      "Loss: 2.303 | Acc: 9.691% (645/6656)\n",
      "Loss: 2.303 | Acc: 9.717% (653/6720)\n",
      "Loss: 2.303 | Acc: 9.655% (655/6784)\n",
      "Loss: 2.303 | Acc: 9.667% (662/6848)\n",
      "Loss: 2.303 | Acc: 9.664% (668/6912)\n",
      "Loss: 2.303 | Acc: 9.662% (674/6976)\n",
      "Loss: 2.303 | Acc: 9.688% (682/7040)\n",
      "Loss: 2.303 | Acc: 9.657% (686/7104)\n",
      "Loss: 2.303 | Acc: 9.626% (690/7168)\n",
      "Loss: 2.303 | Acc: 9.652% (698/7232)\n",
      "Loss: 2.303 | Acc: 9.663% (705/7296)\n",
      "Loss: 2.303 | Acc: 9.688% (713/7360)\n",
      "Loss: 2.303 | Acc: 9.685% (719/7424)\n",
      "Loss: 2.303 | Acc: 9.655% (723/7488)\n",
      "Loss: 2.303 | Acc: 9.653% (729/7552)\n",
      "Loss: 2.303 | Acc: 9.624% (733/7616)\n",
      "Loss: 2.303 | Acc: 9.609% (738/7680)\n",
      "Loss: 2.303 | Acc: 9.646% (747/7744)\n",
      "Loss: 2.303 | Acc: 9.618% (751/7808)\n",
      "Loss: 2.303 | Acc: 9.680% (762/7872)\n",
      "Loss: 2.303 | Acc: 9.740% (773/7936)\n",
      "Loss: 2.303 | Acc: 9.700% (776/8000)\n",
      "Loss: 2.303 | Acc: 9.685% (781/8064)\n",
      "Loss: 2.303 | Acc: 9.683% (787/8128)\n",
      "Loss: 2.303 | Acc: 9.631% (789/8192)\n",
      "Loss: 2.303 | Acc: 9.617% (794/8256)\n",
      "Loss: 2.303 | Acc: 9.615% (800/8320)\n",
      "Loss: 2.303 | Acc: 9.602% (805/8384)\n",
      "Loss: 2.303 | Acc: 9.588% (810/8448)\n",
      "Loss: 2.303 | Acc: 9.633% (820/8512)\n",
      "Loss: 2.303 | Acc: 9.632% (826/8576)\n",
      "Loss: 2.303 | Acc: 9.641% (833/8640)\n",
      "Loss: 2.303 | Acc: 9.651% (840/8704)\n",
      "Loss: 2.303 | Acc: 9.649% (846/8768)\n",
      "Loss: 2.303 | Acc: 9.635% (851/8832)\n",
      "Loss: 2.303 | Acc: 9.712% (864/8896)\n",
      "Loss: 2.303 | Acc: 9.777% (876/8960)\n",
      "Loss: 2.303 | Acc: 9.807% (885/9024)\n",
      "Loss: 2.303 | Acc: 9.848% (895/9088)\n",
      "Loss: 2.303 | Acc: 9.878% (904/9152)\n",
      "Loss: 2.303 | Acc: 9.874% (910/9216)\n",
      "Loss: 2.303 | Acc: 9.925% (921/9280)\n",
      "Loss: 2.303 | Acc: 9.889% (924/9344)\n",
      "Loss: 2.303 | Acc: 9.885% (930/9408)\n",
      "Loss: 2.303 | Acc: 9.892% (937/9472)\n",
      "Loss: 2.303 | Acc: 9.868% (941/9536)\n",
      "Loss: 2.303 | Acc: 9.896% (950/9600)\n",
      "Loss: 2.303 | Acc: 9.913% (958/9664)\n",
      "Loss: 2.303 | Acc: 9.920% (965/9728)\n",
      "Loss: 2.303 | Acc: 9.916% (971/9792)\n",
      "Loss: 2.303 | Acc: 9.892% (975/9856)\n",
      "Loss: 2.303 | Acc: 9.879% (980/9920)\n",
      "Loss: 2.303 | Acc: 9.866% (985/9984)\n",
      "Loss: 2.303 | Acc: 9.853% (990/10048)\n",
      "Loss: 2.303 | Acc: 9.879% (999/10112)\n",
      "Loss: 2.303 | Acc: 9.837% (1001/10176)\n",
      "Loss: 2.303 | Acc: 9.873% (1011/10240)\n",
      "Loss: 2.303 | Acc: 9.880% (1018/10304)\n",
      "Loss: 2.303 | Acc: 9.838% (1020/10368)\n",
      "Loss: 2.303 | Acc: 9.845% (1027/10432)\n",
      "Loss: 2.303 | Acc: 9.832% (1032/10496)\n",
      "Loss: 2.303 | Acc: 9.820% (1037/10560)\n",
      "Loss: 2.303 | Acc: 9.827% (1044/10624)\n",
      "Loss: 2.303 | Acc: 9.843% (1052/10688)\n",
      "Loss: 2.303 | Acc: 9.821% (1056/10752)\n",
      "Loss: 2.303 | Acc: 9.819% (1062/10816)\n",
      "Loss: 2.303 | Acc: 9.816% (1068/10880)\n",
      "Loss: 2.303 | Acc: 9.823% (1075/10944)\n",
      "Loss: 2.303 | Acc: 9.802% (1079/11008)\n",
      "Loss: 2.303 | Acc: 9.790% (1084/11072)\n",
      "Loss: 2.303 | Acc: 9.770% (1088/11136)\n",
      "Loss: 2.303 | Acc: 9.759% (1093/11200)\n",
      "Loss: 2.303 | Acc: 9.775% (1101/11264)\n",
      "Loss: 2.303 | Acc: 9.808% (1111/11328)\n",
      "Loss: 2.303 | Acc: 9.831% (1120/11392)\n",
      "Loss: 2.303 | Acc: 9.811% (1124/11456)\n",
      "Loss: 2.303 | Acc: 9.792% (1128/11520)\n",
      "Loss: 2.303 | Acc: 9.763% (1131/11584)\n",
      "Loss: 2.303 | Acc: 9.770% (1138/11648)\n",
      "Loss: 2.303 | Acc: 9.793% (1147/11712)\n",
      "Loss: 2.303 | Acc: 9.766% (1150/11776)\n",
      "Loss: 2.303 | Acc: 9.747% (1154/11840)\n",
      "Loss: 2.303 | Acc: 9.753% (1161/11904)\n",
      "Loss: 2.303 | Acc: 9.726% (1164/11968)\n",
      "Loss: 2.303 | Acc: 9.749% (1173/12032)\n",
      "Loss: 2.303 | Acc: 9.730% (1177/12096)\n",
      "Loss: 2.303 | Acc: 9.712% (1181/12160)\n",
      "Loss: 2.303 | Acc: 9.776% (1195/12224)\n",
      "Loss: 2.303 | Acc: 9.774% (1201/12288)\n",
      "Loss: 2.303 | Acc: 9.756% (1205/12352)\n",
      "Loss: 2.303 | Acc: 9.762% (1212/12416)\n",
      "Loss: 2.303 | Acc: 9.768% (1219/12480)\n",
      "Loss: 2.303 | Acc: 9.774% (1226/12544)\n",
      "Loss: 2.303 | Acc: 9.772% (1232/12608)\n",
      "Loss: 2.303 | Acc: 9.746% (1235/12672)\n",
      "Loss: 2.303 | Acc: 9.744% (1241/12736)\n",
      "Loss: 2.303 | Acc: 9.750% (1248/12800)\n",
      "Loss: 2.303 | Acc: 9.756% (1255/12864)\n",
      "Loss: 2.303 | Acc: 9.723% (1257/12928)\n",
      "Loss: 2.303 | Acc: 9.706% (1261/12992)\n",
      "Loss: 2.303 | Acc: 9.704% (1267/13056)\n",
      "Loss: 2.303 | Acc: 9.710% (1274/13120)\n",
      "Loss: 2.303 | Acc: 9.694% (1278/13184)\n",
      "Loss: 2.303 | Acc: 9.722% (1288/13248)\n",
      "Loss: 2.303 | Acc: 9.706% (1292/13312)\n",
      "Loss: 2.303 | Acc: 9.704% (1298/13376)\n",
      "Loss: 2.303 | Acc: 9.717% (1306/13440)\n",
      "Loss: 2.303 | Acc: 9.745% (1316/13504)\n",
      "Loss: 2.303 | Acc: 9.744% (1322/13568)\n",
      "Loss: 2.303 | Acc: 9.742% (1328/13632)\n",
      "Loss: 2.303 | Acc: 9.747% (1335/13696)\n",
      "Loss: 2.303 | Acc: 9.753% (1342/13760)\n",
      "Loss: 2.303 | Acc: 9.737% (1346/13824)\n",
      "Loss: 2.303 | Acc: 9.764% (1356/13888)\n",
      "Loss: 2.303 | Acc: 9.776% (1364/13952)\n",
      "Loss: 2.303 | Acc: 9.782% (1371/14016)\n",
      "Loss: 2.303 | Acc: 9.773% (1376/14080)\n",
      "Loss: 2.303 | Acc: 9.757% (1380/14144)\n",
      "Loss: 2.303 | Acc: 9.762% (1387/14208)\n",
      "Loss: 2.303 | Acc: 9.753% (1392/14272)\n",
      "Loss: 2.303 | Acc: 9.759% (1399/14336)\n",
      "Loss: 2.303 | Acc: 9.785% (1409/14400)\n",
      "Loss: 2.303 | Acc: 9.776% (1414/14464)\n",
      "Loss: 2.303 | Acc: 9.781% (1421/14528)\n",
      "Loss: 2.303 | Acc: 9.800% (1430/14592)\n",
      "Loss: 2.303 | Acc: 9.805% (1437/14656)\n",
      "Loss: 2.303 | Acc: 9.789% (1441/14720)\n",
      "Loss: 2.303 | Acc: 9.788% (1447/14784)\n",
      "Loss: 2.303 | Acc: 9.806% (1456/14848)\n",
      "Loss: 2.303 | Acc: 9.804% (1462/14912)\n",
      "Loss: 2.303 | Acc: 9.796% (1467/14976)\n",
      "Loss: 2.303 | Acc: 9.801% (1474/15040)\n",
      "Loss: 2.303 | Acc: 9.819% (1483/15104)\n",
      "Loss: 2.303 | Acc: 9.804% (1487/15168)\n",
      "Loss: 2.303 | Acc: 9.828% (1497/15232)\n",
      "Loss: 2.303 | Acc: 9.833% (1504/15296)\n",
      "Loss: 2.303 | Acc: 9.818% (1508/15360)\n",
      "Loss: 2.303 | Acc: 9.842% (1518/15424)\n",
      "Loss: 2.303 | Acc: 9.846% (1525/15488)\n",
      "Loss: 2.303 | Acc: 9.857% (1533/15552)\n",
      "Loss: 2.303 | Acc: 9.855% (1539/15616)\n",
      "Loss: 2.303 | Acc: 9.847% (1544/15680)\n",
      "Loss: 2.303 | Acc: 9.845% (1550/15744)\n",
      "Loss: 2.303 | Acc: 9.837% (1555/15808)\n",
      "Loss: 2.303 | Acc: 9.829% (1560/15872)\n",
      "Loss: 2.303 | Acc: 9.852% (1570/15936)\n",
      "Loss: 2.303 | Acc: 9.900% (1584/16000)\n",
      "Loss: 2.303 | Acc: 9.904% (1591/16064)\n",
      "Loss: 2.303 | Acc: 9.933% (1602/16128)\n",
      "Loss: 2.303 | Acc: 9.931% (1608/16192)\n",
      "Loss: 2.303 | Acc: 9.922% (1613/16256)\n",
      "Loss: 2.303 | Acc: 9.939% (1622/16320)\n",
      "Loss: 2.303 | Acc: 9.930% (1627/16384)\n",
      "Loss: 2.303 | Acc: 9.928% (1633/16448)\n",
      "Loss: 2.303 | Acc: 9.908% (1636/16512)\n",
      "Loss: 2.303 | Acc: 9.912% (1643/16576)\n",
      "Loss: 2.303 | Acc: 9.910% (1649/16640)\n",
      "Loss: 2.303 | Acc: 9.920% (1657/16704)\n",
      "Loss: 2.303 | Acc: 9.900% (1660/16768)\n",
      "Loss: 2.303 | Acc: 9.886% (1664/16832)\n",
      "Loss: 2.303 | Acc: 9.908% (1674/16896)\n",
      "Loss: 2.303 | Acc: 9.912% (1681/16960)\n",
      "Loss: 2.303 | Acc: 9.904% (1686/17024)\n",
      "Loss: 2.303 | Acc: 9.908% (1693/17088)\n",
      "Loss: 2.303 | Acc: 9.917% (1701/17152)\n",
      "Loss: 2.303 | Acc: 9.898% (1704/17216)\n",
      "Loss: 2.303 | Acc: 9.907% (1712/17280)\n",
      "Loss: 2.303 | Acc: 9.917% (1720/17344)\n",
      "Loss: 2.303 | Acc: 9.898% (1723/17408)\n",
      "Loss: 2.303 | Acc: 9.896% (1729/17472)\n",
      "Loss: 2.303 | Acc: 9.877% (1732/17536)\n",
      "Loss: 2.303 | Acc: 9.892% (1741/17600)\n",
      "Loss: 2.303 | Acc: 9.885% (1746/17664)\n",
      "Loss: 2.303 | Acc: 9.877% (1751/17728)\n",
      "Loss: 2.303 | Acc: 9.864% (1755/17792)\n",
      "Loss: 2.303 | Acc: 9.840% (1757/17856)\n",
      "Loss: 2.303 | Acc: 9.833% (1762/17920)\n",
      "Loss: 2.303 | Acc: 9.837% (1769/17984)\n",
      "Loss: 2.303 | Acc: 9.818% (1772/18048)\n",
      "Loss: 2.303 | Acc: 9.822% (1779/18112)\n",
      "Loss: 2.303 | Acc: 9.832% (1787/18176)\n",
      "Loss: 2.303 | Acc: 9.830% (1793/18240)\n",
      "Loss: 2.303 | Acc: 9.807% (1795/18304)\n",
      "Loss: 2.303 | Acc: 9.800% (1800/18368)\n",
      "Loss: 2.303 | Acc: 9.787% (1804/18432)\n",
      "Loss: 2.303 | Acc: 9.797% (1812/18496)\n",
      "Loss: 2.303 | Acc: 9.790% (1817/18560)\n",
      "Loss: 2.303 | Acc: 9.788% (1823/18624)\n",
      "Loss: 2.303 | Acc: 9.792% (1830/18688)\n",
      "Loss: 2.303 | Acc: 9.812% (1840/18752)\n",
      "Loss: 2.303 | Acc: 9.843% (1852/18816)\n",
      "Loss: 2.303 | Acc: 9.852% (1860/18880)\n",
      "Loss: 2.303 | Acc: 9.840% (1864/18944)\n",
      "Loss: 2.303 | Acc: 9.838% (1870/19008)\n",
      "Loss: 2.303 | Acc: 9.826% (1874/19072)\n",
      "Loss: 2.303 | Acc: 9.830% (1881/19136)\n",
      "Loss: 2.303 | Acc: 9.818% (1885/19200)\n",
      "Loss: 2.303 | Acc: 9.842% (1896/19264)\n",
      "Loss: 2.303 | Acc: 9.841% (1902/19328)\n",
      "Loss: 2.303 | Acc: 9.839% (1908/19392)\n",
      "Loss: 2.303 | Acc: 9.832% (1913/19456)\n",
      "Loss: 2.303 | Acc: 9.846% (1922/19520)\n",
      "Loss: 2.303 | Acc: 9.870% (1933/19584)\n",
      "Loss: 2.303 | Acc: 9.853% (1936/19648)\n",
      "Loss: 2.303 | Acc: 9.862% (1944/19712)\n",
      "Loss: 2.303 | Acc: 9.876% (1953/19776)\n",
      "Loss: 2.303 | Acc: 9.879% (1960/19840)\n",
      "Loss: 2.303 | Acc: 9.887% (1968/19904)\n",
      "Loss: 2.303 | Acc: 9.891% (1975/19968)\n",
      "Loss: 2.303 | Acc: 9.894% (1982/20032)\n",
      "Loss: 2.303 | Acc: 9.893% (1988/20096)\n",
      "Loss: 2.303 | Acc: 9.886% (1993/20160)\n",
      "Loss: 2.303 | Acc: 9.874% (1997/20224)\n",
      "Loss: 2.303 | Acc: 9.858% (2000/20288)\n",
      "Loss: 2.303 | Acc: 9.847% (2004/20352)\n",
      "Loss: 2.303 | Acc: 9.870% (2015/20416)\n",
      "Loss: 2.303 | Acc: 9.863% (2020/20480)\n",
      "Loss: 2.303 | Acc: 9.862% (2026/20544)\n",
      "Loss: 2.303 | Acc: 9.851% (2030/20608)\n",
      "Loss: 2.303 | Acc: 9.839% (2034/20672)\n",
      "Loss: 2.303 | Acc: 9.843% (2041/20736)\n",
      "Loss: 2.303 | Acc: 9.846% (2048/20800)\n",
      "Loss: 2.303 | Acc: 9.845% (2054/20864)\n",
      "Loss: 2.303 | Acc: 9.848% (2061/20928)\n",
      "Loss: 2.303 | Acc: 9.847% (2067/20992)\n",
      "Loss: 2.303 | Acc: 9.855% (2075/21056)\n",
      "Loss: 2.303 | Acc: 9.867% (2084/21120)\n",
      "Loss: 2.303 | Acc: 9.871% (2091/21184)\n",
      "Loss: 2.303 | Acc: 9.888% (2101/21248)\n",
      "Loss: 2.303 | Acc: 9.891% (2108/21312)\n",
      "Loss: 2.303 | Acc: 9.885% (2113/21376)\n",
      "Loss: 2.303 | Acc: 9.907% (2124/21440)\n",
      "Loss: 2.303 | Acc: 9.919% (2133/21504)\n",
      "Loss: 2.303 | Acc: 9.927% (2141/21568)\n",
      "Loss: 2.303 | Acc: 9.944% (2151/21632)\n",
      "Loss: 2.303 | Acc: 9.933% (2155/21696)\n",
      "Loss: 2.303 | Acc: 9.917% (2158/21760)\n",
      "Loss: 2.303 | Acc: 9.902% (2161/21824)\n",
      "Loss: 2.303 | Acc: 9.900% (2167/21888)\n",
      "Loss: 2.303 | Acc: 9.908% (2175/21952)\n",
      "Loss: 2.303 | Acc: 9.920% (2184/22016)\n",
      "Loss: 2.303 | Acc: 9.918% (2190/22080)\n",
      "Loss: 2.303 | Acc: 9.921% (2197/22144)\n",
      "Loss: 2.303 | Acc: 9.915% (2202/22208)\n",
      "Loss: 2.303 | Acc: 9.909% (2207/22272)\n",
      "Loss: 2.303 | Acc: 9.903% (2212/22336)\n",
      "Loss: 2.303 | Acc: 9.915% (2221/22400)\n",
      "Loss: 2.303 | Acc: 9.909% (2226/22464)\n",
      "Loss: 2.303 | Acc: 9.925% (2236/22528)\n",
      "Loss: 2.303 | Acc: 9.928% (2243/22592)\n",
      "Loss: 2.303 | Acc: 9.922% (2248/22656)\n",
      "Loss: 2.303 | Acc: 9.921% (2254/22720)\n",
      "Loss: 2.303 | Acc: 9.919% (2260/22784)\n",
      "Loss: 2.303 | Acc: 9.918% (2266/22848)\n",
      "Loss: 2.303 | Acc: 9.934% (2276/22912)\n",
      "Loss: 2.303 | Acc: 9.945% (2285/22976)\n",
      "Loss: 2.303 | Acc: 9.939% (2290/23040)\n",
      "Loss: 2.303 | Acc: 9.955% (2300/23104)\n",
      "Loss: 2.303 | Acc: 9.940% (2303/23168)\n",
      "Loss: 2.303 | Acc: 9.969% (2316/23232)\n",
      "Loss: 2.303 | Acc: 9.963% (2321/23296)\n",
      "Loss: 2.303 | Acc: 9.974% (2330/23360)\n",
      "Loss: 2.303 | Acc: 9.968% (2335/23424)\n",
      "Loss: 2.303 | Acc: 9.954% (2338/23488)\n",
      "Loss: 2.303 | Acc: 9.948% (2343/23552)\n",
      "Loss: 2.303 | Acc: 9.942% (2348/23616)\n",
      "Loss: 2.303 | Acc: 9.954% (2357/23680)\n",
      "Loss: 2.303 | Acc: 9.969% (2367/23744)\n",
      "Loss: 2.303 | Acc: 9.980% (2376/23808)\n",
      "Loss: 2.303 | Acc: 9.957% (2377/23872)\n",
      "Loss: 2.303 | Acc: 9.935% (2378/23936)\n",
      "Loss: 2.303 | Acc: 9.925% (2382/24000)\n",
      "Loss: 2.303 | Acc: 9.932% (2390/24064)\n",
      "Loss: 2.303 | Acc: 9.918% (2393/24128)\n",
      "Loss: 2.303 | Acc: 9.908% (2397/24192)\n",
      "Loss: 2.303 | Acc: 9.907% (2403/24256)\n",
      "Loss: 2.303 | Acc: 9.922% (2413/24320)\n",
      "Loss: 2.303 | Acc: 9.933% (2422/24384)\n",
      "Loss: 2.303 | Acc: 9.931% (2428/24448)\n",
      "Loss: 2.303 | Acc: 9.934% (2435/24512)\n",
      "Loss: 2.303 | Acc: 9.924% (2439/24576)\n",
      "Loss: 2.303 | Acc: 9.919% (2444/24640)\n",
      "Loss: 2.303 | Acc: 9.921% (2451/24704)\n",
      "Loss: 2.303 | Acc: 9.932% (2460/24768)\n",
      "Loss: 2.303 | Acc: 9.935% (2467/24832)\n",
      "Loss: 2.303 | Acc: 9.933% (2473/24896)\n",
      "Loss: 2.303 | Acc: 9.920% (2476/24960)\n",
      "Loss: 2.303 | Acc: 9.930% (2485/25024)\n",
      "Loss: 2.303 | Acc: 9.921% (2489/25088)\n",
      "Loss: 2.303 | Acc: 9.924% (2496/25152)\n",
      "Loss: 2.303 | Acc: 9.938% (2506/25216)\n",
      "Loss: 2.303 | Acc: 9.945% (2514/25280)\n",
      "Loss: 2.303 | Acc: 9.955% (2523/25344)\n",
      "Loss: 2.303 | Acc: 9.965% (2532/25408)\n",
      "Loss: 2.303 | Acc: 9.960% (2537/25472)\n",
      "Loss: 2.303 | Acc: 9.962% (2544/25536)\n",
      "Loss: 2.303 | Acc: 9.973% (2553/25600)\n",
      "Loss: 2.303 | Acc: 9.991% (2564/25664)\n",
      "Loss: 2.303 | Acc: 9.997% (2572/25728)\n",
      "Loss: 2.303 | Acc: 9.991% (2577/25792)\n",
      "Loss: 2.303 | Acc: 9.994% (2584/25856)\n",
      "Loss: 2.303 | Acc: 9.985% (2588/25920)\n",
      "Loss: 2.303 | Acc: 9.968% (2590/25984)\n",
      "Loss: 2.303 | Acc: 9.982% (2600/26048)\n",
      "Loss: 2.303 | Acc: 9.976% (2605/26112)\n",
      "Loss: 2.303 | Acc: 9.990% (2615/26176)\n",
      "Loss: 2.303 | Acc: 9.989% (2621/26240)\n",
      "Loss: 2.303 | Acc: 9.991% (2628/26304)\n",
      "Loss: 2.303 | Acc: 9.986% (2633/26368)\n",
      "Loss: 2.303 | Acc: 9.969% (2635/26432)\n",
      "Loss: 2.303 | Acc: 9.964% (2640/26496)\n",
      "Loss: 2.303 | Acc: 9.959% (2645/26560)\n",
      "Loss: 2.303 | Acc: 9.965% (2653/26624)\n",
      "Loss: 2.303 | Acc: 9.982% (2664/26688)\n",
      "Loss: 2.303 | Acc: 9.981% (2670/26752)\n",
      "Loss: 2.303 | Acc: 9.998% (2681/26816)\n",
      "Loss: 2.303 | Acc: 10.011% (2691/26880)\n",
      "Loss: 2.303 | Acc: 9.995% (2693/26944)\n",
      "Loss: 2.303 | Acc: 9.990% (2698/27008)\n",
      "Loss: 2.303 | Acc: 9.988% (2704/27072)\n",
      "Loss: 2.303 | Acc: 9.990% (2711/27136)\n",
      "Loss: 2.303 | Acc: 9.996% (2719/27200)\n",
      "Loss: 2.303 | Acc: 9.991% (2724/27264)\n",
      "Loss: 2.303 | Acc: 9.979% (2727/27328)\n",
      "Loss: 2.303 | Acc: 9.977% (2733/27392)\n",
      "Loss: 2.303 | Acc: 9.991% (2743/27456)\n",
      "Loss: 2.303 | Acc: 10.000% (2752/27520)\n",
      "Loss: 2.303 | Acc: 9.991% (2756/27584)\n",
      "Loss: 2.303 | Acc: 10.001% (2765/27648)\n",
      "Loss: 2.303 | Acc: 10.003% (2772/27712)\n",
      "Loss: 2.303 | Acc: 10.009% (2780/27776)\n",
      "Loss: 2.303 | Acc: 10.011% (2787/27840)\n",
      "Loss: 2.303 | Acc: 10.002% (2791/27904)\n",
      "Loss: 2.303 | Acc: 9.994% (2795/27968)\n",
      "Loss: 2.303 | Acc: 9.999% (2803/28032)\n",
      "Loss: 2.303 | Acc: 9.998% (2809/28096)\n",
      "Loss: 2.303 | Acc: 9.989% (2813/28160)\n",
      "Loss: 2.303 | Acc: 9.991% (2820/28224)\n",
      "Loss: 2.303 | Acc: 9.990% (2826/28288)\n",
      "Loss: 2.303 | Acc: 9.975% (2828/28352)\n",
      "Loss: 2.303 | Acc: 9.970% (2833/28416)\n",
      "Loss: 2.303 | Acc: 9.968% (2839/28480)\n",
      "Loss: 2.303 | Acc: 9.978% (2848/28544)\n",
      "Loss: 2.303 | Acc: 9.987% (2857/28608)\n",
      "Loss: 2.303 | Acc: 9.989% (2864/28672)\n",
      "Loss: 2.303 | Acc: 9.987% (2870/28736)\n",
      "Loss: 2.303 | Acc: 9.983% (2875/28800)\n",
      "Loss: 2.303 | Acc: 9.978% (2880/28864)\n",
      "Loss: 2.303 | Acc: 9.980% (2887/28928)\n",
      "Loss: 2.303 | Acc: 9.992% (2897/28992)\n",
      "Loss: 2.303 | Acc: 9.998% (2905/29056)\n",
      "Loss: 2.303 | Acc: 10.007% (2914/29120)\n",
      "Loss: 2.303 | Acc: 9.992% (2916/29184)\n",
      "Loss: 2.303 | Acc: 9.997% (2924/29248)\n",
      "Loss: 2.303 | Acc: 10.003% (2932/29312)\n",
      "Loss: 2.303 | Acc: 9.998% (2937/29376)\n",
      "Loss: 2.303 | Acc: 9.986% (2940/29440)\n",
      "Loss: 2.303 | Acc: 9.975% (2943/29504)\n",
      "Loss: 2.303 | Acc: 9.974% (2949/29568)\n",
      "Loss: 2.303 | Acc: 9.986% (2959/29632)\n",
      "Loss: 2.303 | Acc: 9.988% (2966/29696)\n",
      "Loss: 2.303 | Acc: 9.980% (2970/29760)\n",
      "Loss: 2.303 | Acc: 9.985% (2978/29824)\n",
      "Loss: 2.303 | Acc: 9.967% (2979/29888)\n",
      "Loss: 2.303 | Acc: 9.969% (2986/29952)\n",
      "Loss: 2.303 | Acc: 9.965% (2991/30016)\n",
      "Loss: 2.303 | Acc: 9.960% (2996/30080)\n",
      "Loss: 2.303 | Acc: 9.965% (3004/30144)\n",
      "Loss: 2.303 | Acc: 9.971% (3012/30208)\n",
      "Loss: 2.303 | Acc: 9.976% (3020/30272)\n",
      "Loss: 2.303 | Acc: 9.965% (3023/30336)\n",
      "Loss: 2.303 | Acc: 9.980% (3034/30400)\n",
      "Loss: 2.303 | Acc: 9.986% (3042/30464)\n",
      "Loss: 2.303 | Acc: 9.984% (3048/30528)\n",
      "Loss: 2.303 | Acc: 10.006% (3061/30592)\n",
      "Loss: 2.303 | Acc: 10.011% (3069/30656)\n",
      "Loss: 2.303 | Acc: 10.003% (3073/30720)\n",
      "Loss: 2.303 | Acc: 9.992% (3076/30784)\n",
      "Loss: 2.303 | Acc: 9.994% (3083/30848)\n",
      "Loss: 2.303 | Acc: 9.996% (3090/30912)\n",
      "Loss: 2.303 | Acc: 10.005% (3099/30976)\n",
      "Loss: 2.303 | Acc: 10.000% (3104/31040)\n",
      "Loss: 2.303 | Acc: 9.995% (3109/31104)\n",
      "Loss: 2.303 | Acc: 10.007% (3119/31168)\n",
      "Loss: 2.303 | Acc: 10.015% (3128/31232)\n",
      "Loss: 2.303 | Acc: 10.004% (3131/31296)\n",
      "Loss: 2.303 | Acc: 10.006% (3138/31360)\n",
      "Loss: 2.303 | Acc: 10.005% (3144/31424)\n",
      "Loss: 2.303 | Acc: 9.994% (3147/31488)\n",
      "Loss: 2.303 | Acc: 10.003% (3156/31552)\n",
      "Loss: 2.303 | Acc: 10.001% (3162/31616)\n",
      "Loss: 2.303 | Acc: 10.003% (3169/31680)\n",
      "Loss: 2.303 | Acc: 10.005% (3176/31744)\n",
      "Loss: 2.303 | Acc: 10.004% (3182/31808)\n",
      "Loss: 2.303 | Acc: 10.015% (3192/31872)\n",
      "Loss: 2.303 | Acc: 10.017% (3199/31936)\n",
      "Loss: 2.303 | Acc: 10.016% (3205/32000)\n",
      "Loss: 2.303 | Acc: 10.021% (3213/32064)\n",
      "Loss: 2.303 | Acc: 10.013% (3217/32128)\n",
      "Loss: 2.303 | Acc: 10.006% (3221/32192)\n",
      "Loss: 2.303 | Acc: 10.001% (3226/32256)\n",
      "Loss: 2.303 | Acc: 9.988% (3228/32320)\n",
      "Loss: 2.303 | Acc: 9.990% (3235/32384)\n",
      "Loss: 2.303 | Acc: 9.991% (3242/32448)\n",
      "Loss: 2.303 | Acc: 9.990% (3248/32512)\n",
      "Loss: 2.303 | Acc: 9.983% (3252/32576)\n",
      "Loss: 2.303 | Acc: 9.985% (3259/32640)\n",
      "Loss: 2.303 | Acc: 9.983% (3265/32704)\n",
      "Loss: 2.303 | Acc: 9.973% (3268/32768)\n",
      "Loss: 2.303 | Acc: 9.975% (3275/32832)\n",
      "Loss: 2.303 | Acc: 9.974% (3281/32896)\n",
      "Loss: 2.303 | Acc: 9.982% (3290/32960)\n",
      "Loss: 2.303 | Acc: 9.972% (3293/33024)\n",
      "Loss: 2.303 | Acc: 9.973% (3300/33088)\n",
      "Loss: 2.303 | Acc: 9.972% (3306/33152)\n",
      "Loss: 2.303 | Acc: 9.965% (3310/33216)\n",
      "Loss: 2.303 | Acc: 9.973% (3319/33280)\n",
      "Loss: 2.303 | Acc: 9.972% (3325/33344)\n",
      "Loss: 2.303 | Acc: 9.983% (3335/33408)\n",
      "Loss: 2.303 | Acc: 9.984% (3342/33472)\n",
      "Loss: 2.303 | Acc: 9.986% (3349/33536)\n",
      "Loss: 2.303 | Acc: 9.988% (3356/33600)\n",
      "Loss: 2.303 | Acc: 9.993% (3364/33664)\n",
      "Loss: 2.303 | Acc: 9.992% (3370/33728)\n",
      "Loss: 2.303 | Acc: 9.991% (3376/33792)\n",
      "Loss: 2.303 | Acc: 9.986% (3381/33856)\n",
      "Loss: 2.303 | Acc: 9.994% (3390/33920)\n",
      "Loss: 2.303 | Acc: 9.999% (3398/33984)\n",
      "Loss: 2.303 | Acc: 10.001% (3405/34048)\n",
      "Loss: 2.303 | Acc: 10.008% (3414/34112)\n",
      "Loss: 2.303 | Acc: 10.007% (3420/34176)\n",
      "Loss: 2.303 | Acc: 10.000% (3424/34240)\n",
      "Loss: 2.303 | Acc: 9.996% (3429/34304)\n",
      "Loss: 2.303 | Acc: 10.006% (3439/34368)\n",
      "Loss: 2.303 | Acc: 10.031% (3454/34432)\n",
      "Loss: 2.303 | Acc: 10.030% (3460/34496)\n",
      "Loss: 2.303 | Acc: 10.029% (3466/34560)\n",
      "Loss: 2.303 | Acc: 10.028% (3472/34624)\n",
      "Loss: 2.303 | Acc: 10.018% (3475/34688)\n",
      "Loss: 2.303 | Acc: 10.017% (3481/34752)\n",
      "Loss: 2.303 | Acc: 10.016% (3487/34816)\n",
      "Loss: 2.303 | Acc: 10.017% (3494/34880)\n",
      "Loss: 2.303 | Acc: 10.010% (3498/34944)\n",
      "Loss: 2.303 | Acc: 10.021% (3508/35008)\n",
      "Loss: 2.303 | Acc: 10.019% (3514/35072)\n",
      "Loss: 2.303 | Acc: 10.015% (3519/35136)\n",
      "Loss: 2.303 | Acc: 10.009% (3523/35200)\n",
      "Loss: 2.303 | Acc: 10.010% (3530/35264)\n",
      "Loss: 2.303 | Acc: 10.006% (3535/35328)\n",
      "Loss: 2.303 | Acc: 10.005% (3541/35392)\n",
      "Loss: 2.303 | Acc: 10.018% (3552/35456)\n",
      "Loss: 2.303 | Acc: 10.020% (3559/35520)\n",
      "Loss: 2.303 | Acc: 10.016% (3564/35584)\n",
      "Loss: 2.303 | Acc: 10.026% (3574/35648)\n",
      "Loss: 2.303 | Acc: 10.033% (3583/35712)\n",
      "Loss: 2.303 | Acc: 10.026% (3587/35776)\n",
      "Loss: 2.303 | Acc: 10.031% (3595/35840)\n",
      "Loss: 2.303 | Acc: 10.032% (3602/35904)\n",
      "Loss: 2.303 | Acc: 10.031% (3608/35968)\n",
      "Loss: 2.303 | Acc: 10.038% (3617/36032)\n",
      "Loss: 2.303 | Acc: 10.029% (3620/36096)\n",
      "Loss: 2.303 | Acc: 10.025% (3625/36160)\n",
      "Loss: 2.303 | Acc: 10.024% (3631/36224)\n",
      "Loss: 2.303 | Acc: 10.014% (3634/36288)\n",
      "Loss: 2.303 | Acc: 10.010% (3639/36352)\n",
      "Loss: 2.303 | Acc: 10.012% (3646/36416)\n",
      "Loss: 2.303 | Acc: 10.008% (3651/36480)\n",
      "Loss: 2.303 | Acc: 10.002% (3655/36544)\n",
      "Loss: 2.303 | Acc: 10.003% (3662/36608)\n",
      "Loss: 2.303 | Acc: 9.997% (3666/36672)\n",
      "Loss: 2.303 | Acc: 9.998% (3673/36736)\n",
      "Loss: 2.303 | Acc: 9.992% (3677/36800)\n",
      "Loss: 2.303 | Acc: 9.993% (3684/36864)\n",
      "Loss: 2.303 | Acc: 9.984% (3687/36928)\n",
      "Loss: 2.303 | Acc: 9.981% (3692/36992)\n",
      "Loss: 2.303 | Acc: 9.988% (3701/37056)\n",
      "Loss: 2.303 | Acc: 9.981% (3705/37120)\n",
      "Loss: 2.303 | Acc: 9.980% (3711/37184)\n",
      "Loss: 2.303 | Acc: 9.979% (3717/37248)\n",
      "Loss: 2.303 | Acc: 9.978% (3723/37312)\n",
      "Loss: 2.303 | Acc: 9.977% (3729/37376)\n",
      "Loss: 2.303 | Acc: 9.965% (3731/37440)\n",
      "Loss: 2.303 | Acc: 9.967% (3738/37504)\n",
      "Loss: 2.303 | Acc: 9.977% (3748/37568)\n",
      "Loss: 2.303 | Acc: 9.978% (3755/37632)\n",
      "Loss: 2.303 | Acc: 9.977% (3761/37696)\n",
      "Loss: 2.303 | Acc: 9.987% (3771/37760)\n",
      "Loss: 2.303 | Acc: 9.991% (3779/37824)\n",
      "Loss: 2.303 | Acc: 9.995% (3787/37888)\n",
      "Loss: 2.303 | Acc: 9.986% (3790/37952)\n",
      "Loss: 2.303 | Acc: 9.985% (3796/38016)\n",
      "Loss: 2.303 | Acc: 9.995% (3806/38080)\n",
      "Loss: 2.303 | Acc: 10.002% (3815/38144)\n",
      "Loss: 2.303 | Acc: 9.995% (3819/38208)\n",
      "Loss: 2.303 | Acc: 9.994% (3825/38272)\n",
      "Loss: 2.303 | Acc: 9.988% (3829/38336)\n",
      "Loss: 2.303 | Acc: 9.984% (3834/38400)\n",
      "Loss: 2.303 | Acc: 9.989% (3842/38464)\n",
      "Loss: 2.303 | Acc: 9.985% (3847/38528)\n",
      "Loss: 2.303 | Acc: 9.981% (3852/38592)\n",
      "Loss: 2.303 | Acc: 9.993% (3863/38656)\n",
      "Loss: 2.303 | Acc: 9.990% (3868/38720)\n",
      "Loss: 2.303 | Acc: 9.994% (3876/38784)\n",
      "Loss: 2.303 | Acc: 9.988% (3880/38848)\n",
      "Loss: 2.303 | Acc: 9.989% (3887/38912)\n",
      "Loss: 2.303 | Acc: 9.983% (3891/38976)\n",
      "Loss: 2.303 | Acc: 9.995% (3902/39040)\n",
      "Loss: 2.303 | Acc: 9.989% (3906/39104)\n",
      "Loss: 2.303 | Acc: 9.990% (3913/39168)\n",
      "Loss: 2.303 | Acc: 9.999% (3923/39232)\n",
      "Loss: 2.303 | Acc: 10.014% (3935/39296)\n",
      "Loss: 2.303 | Acc: 10.013% (3941/39360)\n",
      "Loss: 2.303 | Acc: 10.009% (3946/39424)\n",
      "Loss: 2.303 | Acc: 10.006% (3951/39488)\n",
      "Loss: 2.303 | Acc: 10.012% (3960/39552)\n",
      "Loss: 2.303 | Acc: 10.014% (3967/39616)\n",
      "Loss: 2.303 | Acc: 10.013% (3973/39680)\n",
      "Loss: 2.303 | Acc: 10.007% (3977/39744)\n",
      "Loss: 2.303 | Acc: 10.003% (3982/39808)\n",
      "Loss: 2.303 | Acc: 10.005% (3989/39872)\n",
      "Loss: 2.303 | Acc: 10.014% (3999/39936)\n",
      "Loss: 2.303 | Acc: 10.010% (4004/40000)\n",
      "Loss: 2.303 | Acc: 10.021% (4015/40064)\n",
      "Loss: 2.303 | Acc: 10.020% (4021/40128)\n",
      "Loss: 2.303 | Acc: 10.014% (4025/40192)\n",
      "Loss: 2.303 | Acc: 10.018% (4033/40256)\n",
      "Loss: 2.303 | Acc: 10.025% (4042/40320)\n",
      "Loss: 2.303 | Acc: 10.024% (4048/40384)\n",
      "Loss: 2.303 | Acc: 10.020% (4053/40448)\n",
      "Loss: 2.303 | Acc: 10.022% (4060/40512)\n",
      "Loss: 2.303 | Acc: 10.023% (4067/40576)\n",
      "Loss: 2.303 | Acc: 10.039% (4080/40640)\n",
      "Loss: 2.303 | Acc: 10.033% (4084/40704)\n",
      "Loss: 2.303 | Acc: 10.027% (4088/40768)\n",
      "Loss: 2.303 | Acc: 10.031% (4096/40832)\n",
      "Loss: 2.303 | Acc: 10.023% (4099/40896)\n",
      "Loss: 2.303 | Acc: 10.015% (4102/40960)\n",
      "Loss: 2.303 | Acc: 10.011% (4107/41024)\n",
      "Loss: 2.303 | Acc: 10.003% (4110/41088)\n",
      "Loss: 2.303 | Acc: 10.000% (4115/41152)\n",
      "Loss: 2.303 | Acc: 10.001% (4122/41216)\n",
      "Loss: 2.303 | Acc: 9.998% (4127/41280)\n",
      "Loss: 2.303 | Acc: 9.999% (4134/41344)\n",
      "Loss: 2.303 | Acc: 9.998% (4140/41408)\n",
      "Loss: 2.303 | Acc: 10.004% (4149/41472)\n",
      "Loss: 2.303 | Acc: 10.001% (4154/41536)\n",
      "Loss: 2.303 | Acc: 10.010% (4164/41600)\n",
      "Loss: 2.303 | Acc: 9.999% (4166/41664)\n",
      "Loss: 2.303 | Acc: 9.993% (4170/41728)\n",
      "Loss: 2.303 | Acc: 9.990% (4175/41792)\n",
      "Loss: 2.303 | Acc: 9.984% (4179/41856)\n",
      "Loss: 2.303 | Acc: 9.986% (4186/41920)\n",
      "Loss: 2.303 | Acc: 9.985% (4192/41984)\n",
      "Loss: 2.303 | Acc: 9.977% (4195/42048)\n",
      "Loss: 2.303 | Acc: 9.969% (4198/42112)\n",
      "Loss: 2.303 | Acc: 9.961% (4201/42176)\n",
      "Loss: 2.303 | Acc: 9.967% (4210/42240)\n",
      "Loss: 2.303 | Acc: 9.966% (4216/42304)\n",
      "Loss: 2.303 | Acc: 9.970% (4224/42368)\n",
      "Loss: 2.303 | Acc: 9.978% (4234/42432)\n",
      "Loss: 2.303 | Acc: 9.980% (4241/42496)\n",
      "Loss: 2.303 | Acc: 9.972% (4244/42560)\n",
      "Loss: 2.303 | Acc: 9.973% (4251/42624)\n",
      "Loss: 2.303 | Acc: 9.970% (4256/42688)\n",
      "Loss: 2.303 | Acc: 9.967% (4261/42752)\n",
      "Loss: 2.303 | Acc: 9.966% (4267/42816)\n",
      "Loss: 2.303 | Acc: 9.974% (4277/42880)\n",
      "Loss: 2.303 | Acc: 9.990% (4290/42944)\n",
      "Loss: 2.303 | Acc: 9.989% (4296/43008)\n",
      "Loss: 2.303 | Acc: 9.983% (4300/43072)\n",
      "Loss: 2.303 | Acc: 9.980% (4305/43136)\n",
      "Loss: 2.303 | Acc: 9.984% (4313/43200)\n",
      "Loss: 2.303 | Acc: 9.974% (4315/43264)\n",
      "Loss: 2.303 | Acc: 9.970% (4320/43328)\n",
      "Loss: 2.303 | Acc: 9.974% (4328/43392)\n",
      "Loss: 2.303 | Acc: 9.969% (4332/43456)\n",
      "Loss: 2.303 | Acc: 9.959% (4334/43520)\n",
      "Loss: 2.303 | Acc: 9.951% (4337/43584)\n",
      "Loss: 2.303 | Acc: 9.945% (4341/43648)\n",
      "Loss: 2.303 | Acc: 9.954% (4351/43712)\n",
      "Loss: 2.303 | Acc: 9.951% (4356/43776)\n",
      "Loss: 2.303 | Acc: 9.957% (4365/43840)\n",
      "Loss: 2.303 | Acc: 9.956% (4371/43904)\n",
      "Loss: 2.303 | Acc: 9.962% (4380/43968)\n",
      "Loss: 2.303 | Acc: 9.968% (4389/44032)\n",
      "Loss: 2.303 | Acc: 9.965% (4394/44096)\n",
      "Loss: 2.303 | Acc: 9.959% (4398/44160)\n",
      "Loss: 2.303 | Acc: 9.972% (4410/44224)\n",
      "Loss: 2.303 | Acc: 9.971% (4416/44288)\n",
      "Loss: 2.303 | Acc: 9.970% (4422/44352)\n",
      "Loss: 2.303 | Acc: 9.969% (4428/44416)\n",
      "Loss: 2.303 | Acc: 9.964% (4432/44480)\n",
      "Loss: 2.303 | Acc: 9.965% (4439/44544)\n",
      "Loss: 2.303 | Acc: 9.962% (4444/44608)\n",
      "Loss: 2.303 | Acc: 9.964% (4451/44672)\n",
      "Loss: 2.303 | Acc: 9.965% (4458/44736)\n",
      "Loss: 2.303 | Acc: 9.962% (4463/44800)\n",
      "Loss: 2.303 | Acc: 9.955% (4466/44864)\n",
      "Loss: 2.303 | Acc: 9.947% (4469/44928)\n",
      "Loss: 2.303 | Acc: 9.944% (4474/44992)\n",
      "Loss: 2.303 | Acc: 9.948% (4482/45056)\n",
      "Loss: 2.303 | Acc: 9.951% (4490/45120)\n",
      "Loss: 2.303 | Acc: 9.957% (4499/45184)\n",
      "Loss: 2.303 | Acc: 9.961% (4507/45248)\n",
      "Loss: 2.303 | Acc: 9.971% (4518/45312)\n",
      "Loss: 2.303 | Acc: 9.974% (4526/45376)\n",
      "Loss: 2.303 | Acc: 9.971% (4531/45440)\n",
      "Loss: 2.303 | Acc: 9.968% (4536/45504)\n",
      "Loss: 2.303 | Acc: 9.972% (4544/45568)\n",
      "Loss: 2.303 | Acc: 9.969% (4549/45632)\n",
      "Loss: 2.303 | Acc: 9.972% (4557/45696)\n",
      "Loss: 2.303 | Acc: 9.974% (4564/45760)\n",
      "Loss: 2.303 | Acc: 9.977% (4572/45824)\n",
      "Loss: 2.303 | Acc: 9.981% (4580/45888)\n",
      "Loss: 2.303 | Acc: 9.971% (4582/45952)\n",
      "Loss: 2.303 | Acc: 9.968% (4587/46016)\n",
      "Loss: 2.303 | Acc: 9.967% (4593/46080)\n",
      "Loss: 2.303 | Acc: 9.969% (4600/46144)\n",
      "Loss: 2.303 | Acc: 9.964% (4604/46208)\n",
      "Loss: 2.303 | Acc: 9.967% (4612/46272)\n",
      "Loss: 2.303 | Acc: 9.971% (4620/46336)\n",
      "Loss: 2.303 | Acc: 9.985% (4633/46400)\n",
      "Loss: 2.303 | Acc: 9.984% (4639/46464)\n",
      "Loss: 2.303 | Acc: 9.992% (4649/46528)\n",
      "Loss: 2.303 | Acc: 9.995% (4657/46592)\n",
      "Loss: 2.303 | Acc: 9.997% (4664/46656)\n",
      "Loss: 2.303 | Acc: 10.002% (4673/46720)\n",
      "Loss: 2.303 | Acc: 9.999% (4678/46784)\n",
      "Loss: 2.303 | Acc: 9.998% (4684/46848)\n",
      "Loss: 2.303 | Acc: 9.995% (4689/46912)\n",
      "Loss: 2.303 | Acc: 9.990% (4693/46976)\n",
      "Loss: 2.303 | Acc: 9.991% (4700/47040)\n",
      "Loss: 2.303 | Acc: 9.989% (4705/47104)\n",
      "Loss: 2.303 | Acc: 9.998% (4716/47168)\n",
      "Loss: 2.303 | Acc: 9.991% (4719/47232)\n",
      "Loss: 2.303 | Acc: 10.003% (4731/47296)\n",
      "Loss: 2.303 | Acc: 9.998% (4735/47360)\n",
      "Loss: 2.303 | Acc: 9.999% (4742/47424)\n",
      "Loss: 2.303 | Acc: 9.990% (4744/47488)\n",
      "Loss: 2.303 | Acc: 9.989% (4750/47552)\n",
      "Loss: 2.303 | Acc: 9.980% (4752/47616)\n",
      "Loss: 2.303 | Acc: 9.981% (4759/47680)\n",
      "Loss: 2.303 | Acc: 9.982% (4766/47744)\n",
      "Loss: 2.303 | Acc: 9.984% (4773/47808)\n",
      "Loss: 2.303 | Acc: 9.983% (4779/47872)\n",
      "Loss: 2.303 | Acc: 9.988% (4788/47936)\n",
      "Loss: 2.303 | Acc: 9.985% (4793/48000)\n",
      "Loss: 2.303 | Acc: 9.983% (4798/48064)\n",
      "Loss: 2.303 | Acc: 9.978% (4802/48128)\n",
      "Loss: 2.303 | Acc: 9.981% (4810/48192)\n",
      "Loss: 2.303 | Acc: 9.976% (4814/48256)\n",
      "Loss: 2.303 | Acc: 9.981% (4823/48320)\n",
      "Loss: 2.303 | Acc: 9.989% (4833/48384)\n",
      "Loss: 2.303 | Acc: 9.992% (4841/48448)\n",
      "Loss: 2.303 | Acc: 9.993% (4848/48512)\n",
      "Loss: 2.303 | Acc: 9.993% (4854/48576)\n",
      "Loss: 2.303 | Acc: 9.988% (4858/48640)\n",
      "Loss: 2.303 | Acc: 9.987% (4864/48704)\n",
      "Loss: 2.303 | Acc: 9.986% (4870/48768)\n",
      "Loss: 2.303 | Acc: 9.993% (4880/48832)\n",
      "Loss: 2.303 | Acc: 10.001% (4890/48896)\n",
      "Loss: 2.303 | Acc: 9.996% (4894/48960)\n",
      "Loss: 2.303 | Acc: 9.994% (4897/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 9.993877551020407\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.303 | Acc: 9.375% (6/64)\n",
      "Loss: 2.302 | Acc: 11.719% (15/128)\n",
      "Loss: 2.302 | Acc: 9.896% (19/192)\n",
      "Loss: 2.303 | Acc: 8.984% (23/256)\n",
      "Loss: 2.302 | Acc: 9.375% (30/320)\n",
      "Loss: 2.302 | Acc: 9.375% (36/384)\n",
      "Loss: 2.302 | Acc: 9.821% (44/448)\n",
      "Loss: 2.303 | Acc: 9.570% (49/512)\n",
      "Loss: 2.303 | Acc: 9.201% (53/576)\n",
      "Loss: 2.303 | Acc: 9.219% (59/640)\n",
      "Loss: 2.303 | Acc: 9.943% (70/704)\n",
      "Loss: 2.303 | Acc: 9.766% (75/768)\n",
      "Loss: 2.303 | Acc: 9.976% (83/832)\n",
      "Loss: 2.303 | Acc: 10.156% (91/896)\n",
      "Loss: 2.303 | Acc: 10.417% (100/960)\n",
      "Loss: 2.303 | Acc: 10.156% (104/1024)\n",
      "Loss: 2.302 | Acc: 10.478% (114/1088)\n",
      "Loss: 2.302 | Acc: 10.417% (120/1152)\n",
      "Loss: 2.302 | Acc: 10.526% (128/1216)\n",
      "Loss: 2.302 | Acc: 10.547% (135/1280)\n",
      "Loss: 2.302 | Acc: 10.193% (137/1344)\n",
      "Loss: 2.302 | Acc: 10.227% (144/1408)\n",
      "Loss: 2.303 | Acc: 10.190% (150/1472)\n",
      "Loss: 2.303 | Acc: 10.091% (155/1536)\n",
      "Loss: 2.303 | Acc: 10.062% (161/1600)\n",
      "Loss: 2.303 | Acc: 10.216% (170/1664)\n",
      "Loss: 2.303 | Acc: 9.954% (172/1728)\n",
      "Loss: 2.303 | Acc: 9.933% (178/1792)\n",
      "Loss: 2.303 | Acc: 9.806% (182/1856)\n",
      "Loss: 2.303 | Acc: 9.896% (190/1920)\n",
      "Loss: 2.303 | Acc: 9.929% (197/1984)\n",
      "Loss: 2.303 | Acc: 9.961% (204/2048)\n",
      "Loss: 2.303 | Acc: 9.943% (210/2112)\n",
      "Loss: 2.303 | Acc: 9.881% (215/2176)\n",
      "Loss: 2.303 | Acc: 9.866% (221/2240)\n",
      "Loss: 2.303 | Acc: 9.983% (230/2304)\n",
      "Loss: 2.303 | Acc: 9.882% (234/2368)\n",
      "Loss: 2.303 | Acc: 9.786% (238/2432)\n",
      "Loss: 2.303 | Acc: 9.655% (241/2496)\n",
      "Loss: 2.303 | Acc: 9.648% (247/2560)\n",
      "Loss: 2.303 | Acc: 9.642% (253/2624)\n",
      "Loss: 2.303 | Acc: 9.673% (260/2688)\n",
      "Loss: 2.303 | Acc: 9.629% (265/2752)\n",
      "Loss: 2.303 | Acc: 9.730% (274/2816)\n",
      "Loss: 2.303 | Acc: 9.861% (284/2880)\n",
      "Loss: 2.303 | Acc: 9.851% (290/2944)\n",
      "Loss: 2.303 | Acc: 9.807% (295/3008)\n",
      "Loss: 2.303 | Acc: 9.896% (304/3072)\n",
      "Loss: 2.303 | Acc: 9.885% (310/3136)\n",
      "Loss: 2.303 | Acc: 9.844% (315/3200)\n",
      "Loss: 2.303 | Acc: 9.926% (324/3264)\n",
      "Loss: 2.303 | Acc: 9.886% (329/3328)\n",
      "Loss: 2.303 | Acc: 9.906% (336/3392)\n",
      "Loss: 2.303 | Acc: 9.780% (338/3456)\n",
      "Loss: 2.303 | Acc: 9.830% (346/3520)\n",
      "Loss: 2.303 | Acc: 9.766% (350/3584)\n",
      "Loss: 2.303 | Acc: 9.896% (361/3648)\n",
      "Loss: 2.303 | Acc: 9.860% (366/3712)\n",
      "Loss: 2.303 | Acc: 9.825% (371/3776)\n",
      "Loss: 2.303 | Acc: 9.818% (377/3840)\n",
      "Loss: 2.303 | Acc: 9.810% (383/3904)\n",
      "Loss: 2.303 | Acc: 9.778% (388/3968)\n",
      "Loss: 2.303 | Acc: 9.673% (390/4032)\n",
      "Loss: 2.303 | Acc: 9.692% (397/4096)\n",
      "Loss: 2.303 | Acc: 9.712% (404/4160)\n",
      "Loss: 2.303 | Acc: 9.706% (410/4224)\n",
      "Loss: 2.303 | Acc: 9.655% (414/4288)\n",
      "Loss: 2.303 | Acc: 9.628% (419/4352)\n",
      "Loss: 2.303 | Acc: 9.647% (426/4416)\n",
      "Loss: 2.303 | Acc: 9.665% (433/4480)\n",
      "Loss: 2.303 | Acc: 9.749% (443/4544)\n",
      "Loss: 2.303 | Acc: 9.809% (452/4608)\n",
      "Loss: 2.303 | Acc: 9.824% (459/4672)\n",
      "Loss: 2.303 | Acc: 9.840% (466/4736)\n",
      "Loss: 2.303 | Acc: 9.812% (471/4800)\n",
      "Loss: 2.303 | Acc: 9.868% (480/4864)\n",
      "Loss: 2.303 | Acc: 9.862% (486/4928)\n",
      "Loss: 2.303 | Acc: 9.916% (495/4992)\n",
      "Loss: 2.303 | Acc: 9.949% (503/5056)\n",
      "Loss: 2.303 | Acc: 9.883% (506/5120)\n",
      "Loss: 2.303 | Acc: 9.877% (512/5184)\n",
      "Loss: 2.303 | Acc: 9.947% (522/5248)\n",
      "Loss: 2.303 | Acc: 9.940% (528/5312)\n",
      "Loss: 2.303 | Acc: 9.952% (535/5376)\n",
      "Loss: 2.303 | Acc: 9.945% (541/5440)\n",
      "Loss: 2.303 | Acc: 9.938% (547/5504)\n",
      "Loss: 2.303 | Acc: 10.022% (558/5568)\n",
      "Loss: 2.303 | Acc: 10.050% (566/5632)\n",
      "Loss: 2.303 | Acc: 10.060% (573/5696)\n",
      "Loss: 2.303 | Acc: 10.052% (579/5760)\n",
      "Loss: 2.303 | Acc: 10.045% (585/5824)\n",
      "Loss: 2.303 | Acc: 10.054% (592/5888)\n",
      "Loss: 2.303 | Acc: 10.047% (598/5952)\n",
      "Loss: 2.303 | Acc: 10.023% (603/6016)\n",
      "Loss: 2.303 | Acc: 10.049% (611/6080)\n",
      "Loss: 2.303 | Acc: 10.059% (618/6144)\n",
      "Loss: 2.303 | Acc: 10.100% (627/6208)\n",
      "Loss: 2.303 | Acc: 10.045% (630/6272)\n",
      "Loss: 2.303 | Acc: 10.038% (636/6336)\n",
      "Loss: 2.303 | Acc: 10.062% (644/6400)\n",
      "Loss: 2.303 | Acc: 10.040% (649/6464)\n",
      "Loss: 2.303 | Acc: 10.003% (653/6528)\n",
      "Loss: 2.303 | Acc: 9.982% (658/6592)\n",
      "Loss: 2.303 | Acc: 9.916% (660/6656)\n",
      "Loss: 2.303 | Acc: 9.940% (668/6720)\n",
      "Loss: 2.303 | Acc: 9.965% (676/6784)\n",
      "Loss: 2.303 | Acc: 9.988% (684/6848)\n",
      "Loss: 2.303 | Acc: 9.983% (690/6912)\n",
      "Loss: 2.303 | Acc: 9.963% (695/6976)\n",
      "Loss: 2.303 | Acc: 10.014% (705/7040)\n",
      "Loss: 2.303 | Acc: 9.980% (709/7104)\n",
      "Loss: 2.303 | Acc: 9.947% (713/7168)\n",
      "Loss: 2.303 | Acc: 9.997% (723/7232)\n",
      "Loss: 2.303 | Acc: 9.978% (728/7296)\n",
      "Loss: 2.303 | Acc: 9.986% (735/7360)\n",
      "Loss: 2.303 | Acc: 10.062% (747/7424)\n",
      "Loss: 2.303 | Acc: 10.016% (750/7488)\n",
      "Loss: 2.303 | Acc: 10.024% (757/7552)\n",
      "Loss: 2.303 | Acc: 10.005% (762/7616)\n",
      "Loss: 2.303 | Acc: 9.961% (765/7680)\n",
      "Loss: 2.303 | Acc: 9.969% (772/7744)\n",
      "Loss: 2.303 | Acc: 9.990% (780/7808)\n",
      "Loss: 2.303 | Acc: 9.997% (787/7872)\n",
      "Loss: 2.303 | Acc: 10.018% (795/7936)\n",
      "Loss: 2.303 | Acc: 10.012% (801/8000)\n",
      "Loss: 2.303 | Acc: 9.995% (806/8064)\n",
      "Loss: 2.303 | Acc: 10.002% (813/8128)\n",
      "Loss: 2.303 | Acc: 10.022% (821/8192)\n",
      "Loss: 2.303 | Acc: 10.102% (834/8256)\n",
      "Loss: 2.303 | Acc: 10.156% (845/8320)\n",
      "Loss: 2.303 | Acc: 10.186% (854/8384)\n",
      "Loss: 2.303 | Acc: 10.192% (861/8448)\n",
      "Loss: 2.303 | Acc: 10.174% (866/8512)\n",
      "Loss: 2.303 | Acc: 10.168% (872/8576)\n",
      "Loss: 2.303 | Acc: 10.220% (883/8640)\n",
      "Loss: 2.303 | Acc: 10.237% (891/8704)\n",
      "Loss: 2.303 | Acc: 10.219% (896/8768)\n",
      "Loss: 2.303 | Acc: 10.236% (904/8832)\n",
      "Loss: 2.303 | Acc: 10.196% (907/8896)\n",
      "Loss: 2.303 | Acc: 10.145% (909/8960)\n",
      "Loss: 2.303 | Acc: 10.140% (915/9024)\n",
      "Loss: 2.303 | Acc: 10.101% (918/9088)\n",
      "Loss: 2.303 | Acc: 10.129% (927/9152)\n",
      "Loss: 2.303 | Acc: 10.113% (932/9216)\n",
      "Loss: 2.303 | Acc: 10.129% (940/9280)\n",
      "Loss: 2.303 | Acc: 10.167% (950/9344)\n",
      "Loss: 2.303 | Acc: 10.130% (953/9408)\n",
      "Loss: 2.303 | Acc: 10.146% (961/9472)\n",
      "Loss: 2.303 | Acc: 10.120% (965/9536)\n",
      "Loss: 2.303 | Acc: 10.094% (969/9600)\n",
      "Loss: 2.303 | Acc: 10.048% (971/9664)\n",
      "Loss: 2.303 | Acc: 10.033% (976/9728)\n",
      "Loss: 2.303 | Acc: 10.039% (983/9792)\n",
      "Loss: 2.303 | Acc: 10.014% (987/9856)\n",
      "Loss: 2.303 | Acc: 9.970% (989/9920)\n",
      "Loss: 2.303 | Acc: 9.986% (997/9984)\n",
      "Loss: 2.303 | Acc: 10.000% (1000/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 10.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.303 | Acc: 17.188% (11/64)\n",
      "Loss: 2.303 | Acc: 14.844% (19/128)\n",
      "Loss: 2.303 | Acc: 13.542% (26/192)\n",
      "Loss: 2.303 | Acc: 12.891% (33/256)\n",
      "Loss: 2.302 | Acc: 12.188% (39/320)\n",
      "Loss: 2.302 | Acc: 12.500% (48/384)\n",
      "Loss: 2.302 | Acc: 11.830% (53/448)\n",
      "Loss: 2.302 | Acc: 11.719% (60/512)\n",
      "Loss: 2.302 | Acc: 11.979% (69/576)\n",
      "Loss: 2.303 | Acc: 11.875% (76/640)\n",
      "Loss: 2.303 | Acc: 11.790% (83/704)\n",
      "Loss: 2.302 | Acc: 11.979% (92/768)\n",
      "Loss: 2.303 | Acc: 11.418% (95/832)\n",
      "Loss: 2.302 | Acc: 11.607% (104/896)\n",
      "Loss: 2.302 | Acc: 11.562% (111/960)\n",
      "Loss: 2.302 | Acc: 11.621% (119/1024)\n",
      "Loss: 2.302 | Acc: 11.673% (127/1088)\n",
      "Loss: 2.302 | Acc: 11.632% (134/1152)\n",
      "Loss: 2.302 | Acc: 11.760% (143/1216)\n",
      "Loss: 2.302 | Acc: 11.719% (150/1280)\n",
      "Loss: 2.302 | Acc: 11.607% (156/1344)\n",
      "Loss: 2.302 | Acc: 11.435% (161/1408)\n",
      "Loss: 2.302 | Acc: 11.481% (169/1472)\n",
      "Loss: 2.302 | Acc: 11.263% (173/1536)\n",
      "Loss: 2.302 | Acc: 11.438% (183/1600)\n",
      "Loss: 2.302 | Acc: 11.298% (188/1664)\n",
      "Loss: 2.302 | Acc: 11.227% (194/1728)\n",
      "Loss: 2.302 | Acc: 11.105% (199/1792)\n",
      "Loss: 2.302 | Acc: 11.315% (210/1856)\n",
      "Loss: 2.302 | Acc: 11.510% (221/1920)\n",
      "Loss: 2.302 | Acc: 11.442% (227/1984)\n",
      "Loss: 2.302 | Acc: 11.426% (234/2048)\n",
      "Loss: 2.302 | Acc: 11.411% (241/2112)\n",
      "Loss: 2.302 | Acc: 11.121% (242/2176)\n",
      "Loss: 2.302 | Acc: 11.027% (247/2240)\n",
      "Loss: 2.302 | Acc: 11.024% (254/2304)\n",
      "Loss: 2.302 | Acc: 10.811% (256/2368)\n",
      "Loss: 2.302 | Acc: 10.773% (262/2432)\n",
      "Loss: 2.302 | Acc: 10.737% (268/2496)\n",
      "Loss: 2.302 | Acc: 10.664% (273/2560)\n",
      "Loss: 2.302 | Acc: 10.633% (279/2624)\n",
      "Loss: 2.302 | Acc: 10.640% (286/2688)\n",
      "Loss: 2.302 | Acc: 10.538% (290/2752)\n",
      "Loss: 2.302 | Acc: 10.653% (300/2816)\n",
      "Loss: 2.302 | Acc: 10.764% (310/2880)\n",
      "Loss: 2.302 | Acc: 10.768% (317/2944)\n",
      "Loss: 2.302 | Acc: 10.738% (323/3008)\n",
      "Loss: 2.302 | Acc: 10.807% (332/3072)\n",
      "Loss: 2.302 | Acc: 10.842% (340/3136)\n",
      "Loss: 2.302 | Acc: 10.906% (349/3200)\n",
      "Loss: 2.302 | Acc: 10.876% (355/3264)\n",
      "Loss: 2.302 | Acc: 10.847% (361/3328)\n",
      "Loss: 2.302 | Acc: 10.849% (368/3392)\n",
      "Loss: 2.302 | Acc: 10.793% (373/3456)\n",
      "Loss: 2.302 | Acc: 10.767% (379/3520)\n",
      "Loss: 2.303 | Acc: 10.714% (384/3584)\n",
      "Loss: 2.302 | Acc: 10.828% (395/3648)\n",
      "Loss: 2.302 | Acc: 10.776% (400/3712)\n",
      "Loss: 2.303 | Acc: 10.673% (403/3776)\n",
      "Loss: 2.302 | Acc: 10.729% (412/3840)\n",
      "Loss: 2.302 | Acc: 10.707% (418/3904)\n",
      "Loss: 2.302 | Acc: 10.761% (427/3968)\n",
      "Loss: 2.302 | Acc: 10.714% (432/4032)\n",
      "Loss: 2.303 | Acc: 10.693% (438/4096)\n",
      "Loss: 2.302 | Acc: 10.649% (443/4160)\n",
      "Loss: 2.302 | Acc: 10.701% (452/4224)\n",
      "Loss: 2.302 | Acc: 10.704% (459/4288)\n",
      "Loss: 2.302 | Acc: 10.662% (464/4352)\n",
      "Loss: 2.302 | Acc: 10.620% (469/4416)\n",
      "Loss: 2.303 | Acc: 10.558% (473/4480)\n",
      "Loss: 2.303 | Acc: 10.541% (479/4544)\n",
      "Loss: 2.302 | Acc: 10.590% (488/4608)\n",
      "Loss: 2.302 | Acc: 10.638% (497/4672)\n",
      "Loss: 2.303 | Acc: 10.579% (501/4736)\n",
      "Loss: 2.303 | Acc: 10.562% (507/4800)\n",
      "Loss: 2.303 | Acc: 10.526% (512/4864)\n",
      "Loss: 2.303 | Acc: 10.572% (521/4928)\n",
      "Loss: 2.303 | Acc: 10.557% (527/4992)\n",
      "Loss: 2.303 | Acc: 10.542% (533/5056)\n",
      "Loss: 2.303 | Acc: 10.469% (536/5120)\n",
      "Loss: 2.303 | Acc: 10.590% (549/5184)\n",
      "Loss: 2.303 | Acc: 10.556% (554/5248)\n",
      "Loss: 2.303 | Acc: 10.617% (564/5312)\n",
      "Loss: 2.303 | Acc: 10.584% (569/5376)\n",
      "Loss: 2.303 | Acc: 10.625% (578/5440)\n",
      "Loss: 2.303 | Acc: 10.647% (586/5504)\n",
      "Loss: 2.303 | Acc: 10.578% (589/5568)\n",
      "Loss: 2.303 | Acc: 10.671% (601/5632)\n",
      "Loss: 2.303 | Acc: 10.692% (609/5696)\n",
      "Loss: 2.303 | Acc: 10.747% (619/5760)\n",
      "Loss: 2.303 | Acc: 10.663% (621/5824)\n",
      "Loss: 2.303 | Acc: 10.632% (626/5888)\n",
      "Loss: 2.303 | Acc: 10.551% (628/5952)\n",
      "Loss: 2.303 | Acc: 10.588% (637/6016)\n",
      "Loss: 2.303 | Acc: 10.576% (643/6080)\n",
      "Loss: 2.303 | Acc: 10.596% (651/6144)\n",
      "Loss: 2.303 | Acc: 10.631% (660/6208)\n",
      "Loss: 2.303 | Acc: 10.603% (665/6272)\n",
      "Loss: 2.303 | Acc: 10.559% (669/6336)\n",
      "Loss: 2.303 | Acc: 10.531% (674/6400)\n",
      "Loss: 2.303 | Acc: 10.504% (679/6464)\n",
      "Loss: 2.303 | Acc: 10.493% (685/6528)\n",
      "Loss: 2.303 | Acc: 10.437% (688/6592)\n",
      "Loss: 2.303 | Acc: 10.382% (691/6656)\n",
      "Loss: 2.303 | Acc: 10.357% (696/6720)\n",
      "Loss: 2.303 | Acc: 10.304% (699/6784)\n",
      "Loss: 2.303 | Acc: 10.324% (707/6848)\n",
      "Loss: 2.303 | Acc: 10.344% (715/6912)\n",
      "Loss: 2.303 | Acc: 10.364% (723/6976)\n",
      "Loss: 2.303 | Acc: 10.327% (727/7040)\n",
      "Loss: 2.303 | Acc: 10.318% (733/7104)\n",
      "Loss: 2.303 | Acc: 10.324% (740/7168)\n",
      "Loss: 2.303 | Acc: 10.315% (746/7232)\n",
      "Loss: 2.303 | Acc: 10.293% (751/7296)\n",
      "Loss: 2.303 | Acc: 10.299% (758/7360)\n",
      "Loss: 2.303 | Acc: 10.318% (766/7424)\n",
      "Loss: 2.303 | Acc: 10.270% (769/7488)\n",
      "Loss: 2.303 | Acc: 10.236% (773/7552)\n",
      "Loss: 2.303 | Acc: 10.294% (784/7616)\n",
      "Loss: 2.303 | Acc: 10.286% (790/7680)\n",
      "Loss: 2.303 | Acc: 10.266% (795/7744)\n",
      "Loss: 2.303 | Acc: 10.246% (800/7808)\n",
      "Loss: 2.303 | Acc: 10.315% (812/7872)\n",
      "Loss: 2.303 | Acc: 10.282% (816/7936)\n",
      "Loss: 2.303 | Acc: 10.275% (822/8000)\n",
      "Loss: 2.303 | Acc: 10.293% (830/8064)\n",
      "Loss: 2.303 | Acc: 10.347% (841/8128)\n",
      "Loss: 2.303 | Acc: 10.339% (847/8192)\n",
      "Loss: 2.303 | Acc: 10.320% (852/8256)\n",
      "Loss: 2.303 | Acc: 10.361% (862/8320)\n",
      "Loss: 2.303 | Acc: 10.329% (866/8384)\n",
      "Loss: 2.303 | Acc: 10.298% (870/8448)\n",
      "Loss: 2.303 | Acc: 10.303% (877/8512)\n",
      "Loss: 2.303 | Acc: 10.285% (882/8576)\n",
      "Loss: 2.303 | Acc: 10.336% (893/8640)\n",
      "Loss: 2.303 | Acc: 10.340% (900/8704)\n",
      "Loss: 2.303 | Acc: 10.344% (907/8768)\n",
      "Loss: 2.303 | Acc: 10.349% (914/8832)\n",
      "Loss: 2.303 | Acc: 10.342% (920/8896)\n",
      "Loss: 2.303 | Acc: 10.346% (927/8960)\n",
      "Loss: 2.303 | Acc: 10.361% (935/9024)\n",
      "Loss: 2.303 | Acc: 10.332% (939/9088)\n",
      "Loss: 2.303 | Acc: 10.315% (944/9152)\n",
      "Loss: 2.303 | Acc: 10.352% (954/9216)\n",
      "Loss: 2.303 | Acc: 10.334% (959/9280)\n",
      "Loss: 2.303 | Acc: 10.349% (967/9344)\n",
      "Loss: 2.303 | Acc: 10.342% (973/9408)\n",
      "Loss: 2.303 | Acc: 10.325% (978/9472)\n",
      "Loss: 2.303 | Acc: 10.319% (984/9536)\n",
      "Loss: 2.303 | Acc: 10.292% (988/9600)\n",
      "Loss: 2.303 | Acc: 10.296% (995/9664)\n",
      "Loss: 2.303 | Acc: 10.341% (1006/9728)\n",
      "Loss: 2.303 | Acc: 10.355% (1014/9792)\n",
      "Loss: 2.303 | Acc: 10.329% (1018/9856)\n",
      "Loss: 2.303 | Acc: 10.282% (1020/9920)\n",
      "Loss: 2.303 | Acc: 10.276% (1026/9984)\n",
      "Loss: 2.303 | Acc: 10.251% (1030/10048)\n",
      "Loss: 2.303 | Acc: 10.305% (1042/10112)\n",
      "Loss: 2.303 | Acc: 10.279% (1046/10176)\n",
      "Loss: 2.303 | Acc: 10.303% (1055/10240)\n",
      "Loss: 2.303 | Acc: 10.287% (1060/10304)\n",
      "Loss: 2.303 | Acc: 10.272% (1065/10368)\n",
      "Loss: 2.303 | Acc: 10.286% (1073/10432)\n",
      "Loss: 2.303 | Acc: 10.290% (1080/10496)\n",
      "Loss: 2.303 | Acc: 10.294% (1087/10560)\n",
      "Loss: 2.303 | Acc: 10.279% (1092/10624)\n",
      "Loss: 2.303 | Acc: 10.292% (1100/10688)\n",
      "Loss: 2.303 | Acc: 10.286% (1106/10752)\n",
      "Loss: 2.303 | Acc: 10.281% (1112/10816)\n",
      "Loss: 2.303 | Acc: 10.312% (1122/10880)\n",
      "Loss: 2.303 | Acc: 10.334% (1131/10944)\n",
      "Loss: 2.303 | Acc: 10.329% (1137/11008)\n",
      "Loss: 2.303 | Acc: 10.368% (1148/11072)\n",
      "Loss: 2.303 | Acc: 10.363% (1154/11136)\n",
      "Loss: 2.303 | Acc: 10.366% (1161/11200)\n",
      "Loss: 2.303 | Acc: 10.352% (1166/11264)\n",
      "Loss: 2.303 | Acc: 10.355% (1173/11328)\n",
      "Loss: 2.303 | Acc: 10.332% (1177/11392)\n",
      "Loss: 2.303 | Acc: 10.300% (1180/11456)\n",
      "Loss: 2.303 | Acc: 10.286% (1185/11520)\n",
      "Loss: 2.303 | Acc: 10.273% (1190/11584)\n",
      "Loss: 2.303 | Acc: 10.319% (1202/11648)\n",
      "Loss: 2.303 | Acc: 10.314% (1208/11712)\n",
      "Loss: 2.303 | Acc: 10.301% (1213/11776)\n",
      "Loss: 2.303 | Acc: 10.304% (1220/11840)\n",
      "Loss: 2.303 | Acc: 10.333% (1230/11904)\n",
      "Loss: 2.303 | Acc: 10.344% (1238/11968)\n",
      "Loss: 2.303 | Acc: 10.314% (1241/12032)\n",
      "Loss: 2.303 | Acc: 10.284% (1244/12096)\n",
      "Loss: 2.303 | Acc: 10.304% (1253/12160)\n",
      "Loss: 2.303 | Acc: 10.299% (1259/12224)\n",
      "Loss: 2.303 | Acc: 10.295% (1265/12288)\n",
      "Loss: 2.303 | Acc: 10.257% (1267/12352)\n",
      "Loss: 2.303 | Acc: 10.229% (1270/12416)\n",
      "Loss: 2.303 | Acc: 10.272% (1282/12480)\n",
      "Loss: 2.303 | Acc: 10.268% (1288/12544)\n",
      "Loss: 2.303 | Acc: 10.303% (1299/12608)\n",
      "Loss: 2.303 | Acc: 10.314% (1307/12672)\n",
      "Loss: 2.303 | Acc: 10.309% (1313/12736)\n",
      "Loss: 2.303 | Acc: 10.336% (1323/12800)\n",
      "Loss: 2.303 | Acc: 10.316% (1327/12864)\n",
      "Loss: 2.303 | Acc: 10.288% (1330/12928)\n",
      "Loss: 2.303 | Acc: 10.299% (1338/12992)\n",
      "Loss: 2.303 | Acc: 10.256% (1339/13056)\n",
      "Loss: 2.303 | Acc: 10.221% (1341/13120)\n",
      "Loss: 2.303 | Acc: 10.209% (1346/13184)\n",
      "Loss: 2.303 | Acc: 10.205% (1352/13248)\n",
      "Loss: 2.303 | Acc: 10.171% (1354/13312)\n",
      "Loss: 2.303 | Acc: 10.167% (1360/13376)\n",
      "Loss: 2.303 | Acc: 10.156% (1365/13440)\n",
      "Loss: 2.303 | Acc: 10.153% (1371/13504)\n",
      "Loss: 2.303 | Acc: 10.186% (1382/13568)\n",
      "Loss: 2.303 | Acc: 10.197% (1390/13632)\n",
      "Loss: 2.303 | Acc: 10.215% (1399/13696)\n",
      "Loss: 2.303 | Acc: 10.233% (1408/13760)\n",
      "Loss: 2.303 | Acc: 10.221% (1413/13824)\n",
      "Loss: 2.303 | Acc: 10.210% (1418/13888)\n",
      "Loss: 2.303 | Acc: 10.206% (1424/13952)\n",
      "Loss: 2.303 | Acc: 10.224% (1433/14016)\n",
      "Loss: 2.303 | Acc: 10.220% (1439/14080)\n",
      "Loss: 2.303 | Acc: 10.202% (1443/14144)\n",
      "Loss: 2.303 | Acc: 10.206% (1450/14208)\n",
      "Loss: 2.303 | Acc: 10.202% (1456/14272)\n",
      "Loss: 2.303 | Acc: 10.191% (1461/14336)\n",
      "Loss: 2.303 | Acc: 10.188% (1467/14400)\n",
      "Loss: 2.303 | Acc: 10.177% (1472/14464)\n",
      "Loss: 2.303 | Acc: 10.167% (1477/14528)\n",
      "Loss: 2.303 | Acc: 10.184% (1486/14592)\n",
      "Loss: 2.303 | Acc: 10.187% (1493/14656)\n",
      "Loss: 2.303 | Acc: 10.204% (1502/14720)\n",
      "Loss: 2.303 | Acc: 10.207% (1509/14784)\n",
      "Loss: 2.303 | Acc: 10.203% (1515/14848)\n",
      "Loss: 2.303 | Acc: 10.220% (1524/14912)\n",
      "Loss: 2.303 | Acc: 10.210% (1529/14976)\n",
      "Loss: 2.303 | Acc: 10.246% (1541/15040)\n",
      "Loss: 2.303 | Acc: 10.249% (1548/15104)\n",
      "Loss: 2.303 | Acc: 10.252% (1555/15168)\n",
      "Loss: 2.303 | Acc: 10.235% (1559/15232)\n",
      "Loss: 2.303 | Acc: 10.231% (1565/15296)\n",
      "Loss: 2.303 | Acc: 10.234% (1572/15360)\n",
      "Loss: 2.303 | Acc: 10.244% (1580/15424)\n",
      "Loss: 2.303 | Acc: 10.234% (1585/15488)\n",
      "Loss: 2.303 | Acc: 10.230% (1591/15552)\n",
      "Loss: 2.303 | Acc: 10.272% (1604/15616)\n",
      "Loss: 2.303 | Acc: 10.261% (1609/15680)\n",
      "Loss: 2.303 | Acc: 10.245% (1613/15744)\n",
      "Loss: 2.303 | Acc: 10.261% (1622/15808)\n",
      "Loss: 2.303 | Acc: 10.251% (1627/15872)\n",
      "Loss: 2.303 | Acc: 10.254% (1634/15936)\n",
      "Loss: 2.303 | Acc: 10.250% (1640/16000)\n",
      "Loss: 2.303 | Acc: 10.240% (1645/16064)\n",
      "Loss: 2.303 | Acc: 10.224% (1649/16128)\n",
      "Loss: 2.303 | Acc: 10.209% (1653/16192)\n",
      "Loss: 2.303 | Acc: 10.230% (1663/16256)\n",
      "Loss: 2.303 | Acc: 10.239% (1671/16320)\n",
      "Loss: 2.303 | Acc: 10.217% (1674/16384)\n",
      "Loss: 2.303 | Acc: 10.196% (1677/16448)\n",
      "Loss: 2.303 | Acc: 10.211% (1686/16512)\n",
      "Loss: 2.303 | Acc: 10.195% (1690/16576)\n",
      "Loss: 2.303 | Acc: 10.180% (1694/16640)\n",
      "Loss: 2.303 | Acc: 10.159% (1697/16704)\n",
      "Loss: 2.303 | Acc: 10.150% (1702/16768)\n",
      "Loss: 2.303 | Acc: 10.124% (1704/16832)\n",
      "Loss: 2.303 | Acc: 10.138% (1713/16896)\n",
      "Loss: 2.303 | Acc: 10.153% (1722/16960)\n",
      "Loss: 2.303 | Acc: 10.162% (1730/17024)\n",
      "Loss: 2.303 | Acc: 10.159% (1736/17088)\n",
      "Loss: 2.303 | Acc: 10.139% (1739/17152)\n",
      "Loss: 2.303 | Acc: 10.107% (1740/17216)\n",
      "Loss: 2.303 | Acc: 10.116% (1748/17280)\n",
      "Loss: 2.303 | Acc: 10.107% (1753/17344)\n",
      "Loss: 2.303 | Acc: 10.099% (1758/17408)\n",
      "Loss: 2.303 | Acc: 10.090% (1763/17472)\n",
      "Loss: 2.303 | Acc: 10.088% (1769/17536)\n",
      "Loss: 2.303 | Acc: 10.085% (1775/17600)\n",
      "Loss: 2.303 | Acc: 10.094% (1783/17664)\n",
      "Loss: 2.303 | Acc: 10.097% (1790/17728)\n",
      "Loss: 2.303 | Acc: 10.106% (1798/17792)\n",
      "Loss: 2.303 | Acc: 10.092% (1802/17856)\n",
      "Loss: 2.303 | Acc: 10.089% (1808/17920)\n",
      "Loss: 2.303 | Acc: 10.087% (1814/17984)\n",
      "Loss: 2.303 | Acc: 10.090% (1821/18048)\n",
      "Loss: 2.303 | Acc: 10.082% (1826/18112)\n",
      "Loss: 2.303 | Acc: 10.085% (1833/18176)\n",
      "Loss: 2.303 | Acc: 10.077% (1838/18240)\n",
      "Loss: 2.303 | Acc: 10.058% (1841/18304)\n",
      "Loss: 2.303 | Acc: 10.045% (1845/18368)\n",
      "Loss: 2.303 | Acc: 10.048% (1852/18432)\n",
      "Loss: 2.303 | Acc: 10.045% (1858/18496)\n",
      "Loss: 2.303 | Acc: 10.038% (1863/18560)\n",
      "Loss: 2.303 | Acc: 10.019% (1866/18624)\n",
      "Loss: 2.303 | Acc: 10.033% (1875/18688)\n",
      "Loss: 2.303 | Acc: 10.047% (1884/18752)\n",
      "Loss: 2.303 | Acc: 10.018% (1885/18816)\n",
      "Loss: 2.303 | Acc: 10.016% (1891/18880)\n",
      "Loss: 2.303 | Acc: 9.998% (1894/18944)\n",
      "Loss: 2.303 | Acc: 9.985% (1898/19008)\n",
      "Loss: 2.303 | Acc: 9.967% (1901/19072)\n",
      "Loss: 2.303 | Acc: 9.986% (1911/19136)\n",
      "Loss: 2.303 | Acc: 9.974% (1915/19200)\n",
      "Loss: 2.303 | Acc: 9.967% (1920/19264)\n",
      "Loss: 2.303 | Acc: 9.960% (1925/19328)\n",
      "Loss: 2.303 | Acc: 9.963% (1932/19392)\n",
      "Loss: 2.303 | Acc: 9.951% (1936/19456)\n",
      "Loss: 2.303 | Acc: 9.939% (1940/19520)\n",
      "Loss: 2.303 | Acc: 9.957% (1950/19584)\n",
      "Loss: 2.303 | Acc: 9.965% (1958/19648)\n",
      "Loss: 2.303 | Acc: 9.953% (1962/19712)\n",
      "Loss: 2.303 | Acc: 9.941% (1966/19776)\n",
      "Loss: 2.303 | Acc: 9.940% (1972/19840)\n",
      "Loss: 2.303 | Acc: 9.948% (1980/19904)\n",
      "Loss: 2.303 | Acc: 9.956% (1988/19968)\n",
      "Loss: 2.303 | Acc: 9.929% (1989/20032)\n",
      "Loss: 2.303 | Acc: 9.942% (1998/20096)\n",
      "Loss: 2.303 | Acc: 9.931% (2002/20160)\n",
      "Loss: 2.303 | Acc: 9.929% (2008/20224)\n",
      "Loss: 2.303 | Acc: 9.927% (2014/20288)\n",
      "Loss: 2.303 | Acc: 9.915% (2018/20352)\n",
      "Loss: 2.303 | Acc: 9.909% (2023/20416)\n",
      "Loss: 2.303 | Acc: 9.883% (2024/20480)\n",
      "Loss: 2.303 | Acc: 9.862% (2026/20544)\n",
      "Loss: 2.303 | Acc: 9.851% (2030/20608)\n",
      "Loss: 2.303 | Acc: 9.849% (2036/20672)\n",
      "Loss: 2.303 | Acc: 9.857% (2044/20736)\n",
      "Loss: 2.303 | Acc: 9.851% (2049/20800)\n",
      "Loss: 2.303 | Acc: 9.850% (2055/20864)\n",
      "Loss: 2.303 | Acc: 9.853% (2062/20928)\n",
      "Loss: 2.303 | Acc: 9.861% (2070/20992)\n",
      "Loss: 2.303 | Acc: 9.859% (2076/21056)\n",
      "Loss: 2.303 | Acc: 9.877% (2086/21120)\n",
      "Loss: 2.303 | Acc: 9.880% (2093/21184)\n",
      "Loss: 2.303 | Acc: 9.869% (2097/21248)\n",
      "Loss: 2.303 | Acc: 9.863% (2102/21312)\n",
      "Loss: 2.303 | Acc: 9.847% (2105/21376)\n",
      "Loss: 2.303 | Acc: 9.851% (2112/21440)\n",
      "Loss: 2.303 | Acc: 9.863% (2121/21504)\n",
      "Loss: 2.303 | Acc: 9.866% (2128/21568)\n",
      "Loss: 2.303 | Acc: 9.870% (2135/21632)\n",
      "Loss: 2.303 | Acc: 9.891% (2146/21696)\n",
      "Loss: 2.303 | Acc: 9.885% (2151/21760)\n",
      "Loss: 2.303 | Acc: 9.893% (2159/21824)\n",
      "Loss: 2.303 | Acc: 9.896% (2166/21888)\n",
      "Loss: 2.303 | Acc: 9.894% (2172/21952)\n",
      "Loss: 2.303 | Acc: 9.897% (2179/22016)\n",
      "Loss: 2.303 | Acc: 9.891% (2184/22080)\n",
      "Loss: 2.303 | Acc: 9.899% (2192/22144)\n",
      "Loss: 2.303 | Acc: 9.888% (2196/22208)\n",
      "Loss: 2.303 | Acc: 9.878% (2200/22272)\n",
      "Loss: 2.303 | Acc: 9.885% (2208/22336)\n",
      "Loss: 2.303 | Acc: 9.879% (2213/22400)\n",
      "Loss: 2.303 | Acc: 9.874% (2218/22464)\n",
      "Loss: 2.303 | Acc: 9.872% (2224/22528)\n",
      "Loss: 2.303 | Acc: 9.866% (2229/22592)\n",
      "Loss: 2.303 | Acc: 9.869% (2236/22656)\n",
      "Loss: 2.303 | Acc: 9.877% (2244/22720)\n",
      "Loss: 2.303 | Acc: 9.897% (2255/22784)\n",
      "Loss: 2.303 | Acc: 9.918% (2266/22848)\n",
      "Loss: 2.303 | Acc: 9.947% (2279/22912)\n",
      "Loss: 2.303 | Acc: 9.950% (2286/22976)\n",
      "Loss: 2.303 | Acc: 9.957% (2294/23040)\n",
      "Loss: 2.303 | Acc: 9.959% (2301/23104)\n",
      "Loss: 2.303 | Acc: 9.975% (2311/23168)\n",
      "Loss: 2.303 | Acc: 9.969% (2316/23232)\n",
      "Loss: 2.303 | Acc: 9.963% (2321/23296)\n",
      "Loss: 2.303 | Acc: 9.953% (2325/23360)\n",
      "Loss: 2.303 | Acc: 9.964% (2334/23424)\n",
      "Loss: 2.303 | Acc: 9.954% (2338/23488)\n",
      "Loss: 2.303 | Acc: 9.957% (2345/23552)\n",
      "Loss: 2.303 | Acc: 9.972% (2355/23616)\n",
      "Loss: 2.303 | Acc: 9.962% (2359/23680)\n",
      "Loss: 2.303 | Acc: 9.956% (2364/23744)\n",
      "Loss: 2.303 | Acc: 9.950% (2369/23808)\n",
      "Loss: 2.303 | Acc: 9.941% (2373/23872)\n",
      "Loss: 2.303 | Acc: 9.935% (2378/23936)\n",
      "Loss: 2.303 | Acc: 9.933% (2384/24000)\n",
      "Loss: 2.303 | Acc: 9.944% (2393/24064)\n",
      "Loss: 2.303 | Acc: 9.959% (2403/24128)\n",
      "Loss: 2.303 | Acc: 9.970% (2412/24192)\n",
      "Loss: 2.303 | Acc: 9.960% (2416/24256)\n",
      "Loss: 2.303 | Acc: 9.971% (2425/24320)\n",
      "Loss: 2.303 | Acc: 9.970% (2431/24384)\n",
      "Loss: 2.303 | Acc: 9.964% (2436/24448)\n",
      "Loss: 2.303 | Acc: 9.954% (2440/24512)\n",
      "Loss: 2.303 | Acc: 9.953% (2446/24576)\n",
      "Loss: 2.303 | Acc: 9.947% (2451/24640)\n",
      "Loss: 2.303 | Acc: 9.946% (2457/24704)\n",
      "Loss: 2.303 | Acc: 9.948% (2464/24768)\n",
      "Loss: 2.303 | Acc: 9.931% (2466/24832)\n",
      "Loss: 2.303 | Acc: 9.921% (2470/24896)\n",
      "Loss: 2.303 | Acc: 9.908% (2473/24960)\n",
      "Loss: 2.303 | Acc: 9.914% (2481/25024)\n",
      "Loss: 2.303 | Acc: 9.913% (2487/25088)\n",
      "Loss: 2.303 | Acc: 9.904% (2491/25152)\n",
      "Loss: 2.303 | Acc: 9.906% (2498/25216)\n",
      "Loss: 2.303 | Acc: 9.897% (2502/25280)\n",
      "Loss: 2.303 | Acc: 9.896% (2508/25344)\n",
      "Loss: 2.303 | Acc: 9.898% (2515/25408)\n",
      "Loss: 2.303 | Acc: 9.893% (2520/25472)\n",
      "Loss: 2.303 | Acc: 9.900% (2528/25536)\n",
      "Loss: 2.303 | Acc: 9.902% (2535/25600)\n",
      "Loss: 2.303 | Acc: 9.905% (2542/25664)\n",
      "Loss: 2.303 | Acc: 9.915% (2551/25728)\n",
      "Loss: 2.303 | Acc: 9.910% (2556/25792)\n",
      "Loss: 2.303 | Acc: 9.901% (2560/25856)\n",
      "Loss: 2.303 | Acc: 9.911% (2569/25920)\n",
      "Loss: 2.303 | Acc: 9.898% (2572/25984)\n",
      "Loss: 2.303 | Acc: 9.897% (2578/26048)\n",
      "Loss: 2.303 | Acc: 9.892% (2583/26112)\n",
      "Loss: 2.303 | Acc: 9.891% (2589/26176)\n",
      "Loss: 2.303 | Acc: 9.882% (2593/26240)\n",
      "Loss: 2.303 | Acc: 9.877% (2598/26304)\n",
      "Loss: 2.303 | Acc: 9.887% (2607/26368)\n",
      "Loss: 2.303 | Acc: 9.878% (2611/26432)\n",
      "Loss: 2.303 | Acc: 9.877% (2617/26496)\n",
      "Loss: 2.303 | Acc: 9.883% (2625/26560)\n",
      "Loss: 2.303 | Acc: 9.897% (2635/26624)\n",
      "Loss: 2.303 | Acc: 9.892% (2640/26688)\n",
      "Loss: 2.303 | Acc: 9.891% (2646/26752)\n",
      "Loss: 2.303 | Acc: 9.897% (2654/26816)\n",
      "Loss: 2.303 | Acc: 9.888% (2658/26880)\n",
      "Loss: 2.303 | Acc: 9.872% (2660/26944)\n",
      "Loss: 2.303 | Acc: 9.875% (2667/27008)\n",
      "Loss: 2.303 | Acc: 9.892% (2678/27072)\n",
      "Loss: 2.303 | Acc: 9.902% (2687/27136)\n",
      "Loss: 2.303 | Acc: 9.908% (2695/27200)\n",
      "Loss: 2.303 | Acc: 9.896% (2698/27264)\n",
      "Loss: 2.303 | Acc: 9.902% (2706/27328)\n",
      "Loss: 2.303 | Acc: 9.904% (2713/27392)\n",
      "Loss: 2.303 | Acc: 9.903% (2719/27456)\n",
      "Loss: 2.303 | Acc: 9.895% (2723/27520)\n",
      "Loss: 2.303 | Acc: 9.886% (2727/27584)\n",
      "Loss: 2.303 | Acc: 9.885% (2733/27648)\n",
      "Loss: 2.303 | Acc: 9.880% (2738/27712)\n",
      "Loss: 2.303 | Acc: 9.868% (2741/27776)\n",
      "Loss: 2.303 | Acc: 9.874% (2749/27840)\n",
      "Loss: 2.303 | Acc: 9.866% (2753/27904)\n",
      "Loss: 2.303 | Acc: 9.868% (2760/27968)\n",
      "Loss: 2.303 | Acc: 9.857% (2763/28032)\n",
      "Loss: 2.303 | Acc: 9.863% (2771/28096)\n",
      "Loss: 2.303 | Acc: 9.862% (2777/28160)\n",
      "Loss: 2.303 | Acc: 9.846% (2779/28224)\n",
      "Loss: 2.303 | Acc: 9.849% (2786/28288)\n",
      "Loss: 2.303 | Acc: 9.851% (2793/28352)\n",
      "Loss: 2.303 | Acc: 9.836% (2795/28416)\n",
      "Loss: 2.303 | Acc: 9.838% (2802/28480)\n",
      "Loss: 2.303 | Acc: 9.837% (2808/28544)\n",
      "Loss: 2.303 | Acc: 9.840% (2815/28608)\n",
      "Loss: 2.303 | Acc: 9.849% (2824/28672)\n",
      "Loss: 2.303 | Acc: 9.845% (2829/28736)\n",
      "Loss: 2.303 | Acc: 9.854% (2838/28800)\n",
      "Loss: 2.303 | Acc: 9.839% (2840/28864)\n",
      "Loss: 2.303 | Acc: 9.835% (2845/28928)\n",
      "Loss: 2.303 | Acc: 9.848% (2855/28992)\n",
      "Loss: 2.303 | Acc: 9.843% (2860/29056)\n",
      "Loss: 2.303 | Acc: 9.832% (2863/29120)\n",
      "Loss: 2.303 | Acc: 9.841% (2872/29184)\n",
      "Loss: 2.303 | Acc: 9.840% (2878/29248)\n",
      "Loss: 2.303 | Acc: 9.842% (2885/29312)\n",
      "Loss: 2.303 | Acc: 9.848% (2893/29376)\n",
      "Loss: 2.303 | Acc: 9.854% (2901/29440)\n",
      "Loss: 2.303 | Acc: 9.860% (2909/29504)\n",
      "Loss: 2.303 | Acc: 9.862% (2916/29568)\n",
      "Loss: 2.303 | Acc: 9.868% (2924/29632)\n",
      "Loss: 2.303 | Acc: 9.867% (2930/29696)\n",
      "Loss: 2.303 | Acc: 9.882% (2941/29760)\n",
      "Loss: 2.303 | Acc: 9.875% (2945/29824)\n",
      "Loss: 2.303 | Acc: 9.867% (2949/29888)\n",
      "Loss: 2.303 | Acc: 9.859% (2953/29952)\n",
      "Loss: 2.303 | Acc: 9.865% (2961/30016)\n",
      "Loss: 2.303 | Acc: 9.864% (2967/30080)\n",
      "Loss: 2.303 | Acc: 9.859% (2972/30144)\n",
      "Loss: 2.303 | Acc: 9.855% (2977/30208)\n",
      "Loss: 2.303 | Acc: 9.844% (2980/30272)\n",
      "Loss: 2.303 | Acc: 9.843% (2986/30336)\n",
      "Loss: 2.303 | Acc: 9.842% (2992/30400)\n",
      "Loss: 2.303 | Acc: 9.841% (2998/30464)\n",
      "Loss: 2.303 | Acc: 9.837% (3003/30528)\n",
      "Loss: 2.303 | Acc: 9.846% (3012/30592)\n",
      "Loss: 2.303 | Acc: 9.848% (3019/30656)\n",
      "Loss: 2.303 | Acc: 9.850% (3026/30720)\n",
      "Loss: 2.303 | Acc: 9.846% (3031/30784)\n",
      "Loss: 2.303 | Acc: 9.839% (3035/30848)\n",
      "Loss: 2.303 | Acc: 9.831% (3039/30912)\n",
      "Loss: 2.303 | Acc: 9.843% (3049/30976)\n",
      "Loss: 2.303 | Acc: 9.852% (3058/31040)\n",
      "Loss: 2.303 | Acc: 9.851% (3064/31104)\n",
      "Loss: 2.303 | Acc: 9.856% (3072/31168)\n",
      "Loss: 2.303 | Acc: 9.849% (3076/31232)\n",
      "Loss: 2.303 | Acc: 9.851% (3083/31296)\n",
      "Loss: 2.303 | Acc: 9.847% (3088/31360)\n",
      "Loss: 2.303 | Acc: 9.852% (3096/31424)\n",
      "Loss: 2.303 | Acc: 9.851% (3102/31488)\n",
      "Loss: 2.303 | Acc: 9.847% (3107/31552)\n",
      "Loss: 2.303 | Acc: 9.837% (3110/31616)\n",
      "Loss: 2.303 | Acc: 9.826% (3113/31680)\n",
      "Loss: 2.303 | Acc: 9.819% (3117/31744)\n",
      "Loss: 2.303 | Acc: 9.806% (3119/31808)\n",
      "Loss: 2.303 | Acc: 9.802% (3124/31872)\n",
      "Loss: 2.303 | Acc: 9.791% (3127/31936)\n",
      "Loss: 2.303 | Acc: 9.791% (3133/32000)\n",
      "Loss: 2.303 | Acc: 9.787% (3138/32064)\n",
      "Loss: 2.303 | Acc: 9.789% (3145/32128)\n",
      "Loss: 2.303 | Acc: 9.788% (3151/32192)\n",
      "Loss: 2.303 | Acc: 9.797% (3160/32256)\n",
      "Loss: 2.303 | Acc: 9.799% (3167/32320)\n",
      "Loss: 2.303 | Acc: 9.795% (3172/32384)\n",
      "Loss: 2.303 | Acc: 9.794% (3178/32448)\n",
      "Loss: 2.303 | Acc: 9.787% (3182/32512)\n",
      "Loss: 2.303 | Acc: 9.780% (3186/32576)\n",
      "Loss: 2.303 | Acc: 9.779% (3192/32640)\n",
      "Loss: 2.303 | Acc: 9.776% (3197/32704)\n",
      "Loss: 2.303 | Acc: 9.778% (3204/32768)\n",
      "Loss: 2.303 | Acc: 9.777% (3210/32832)\n",
      "Loss: 2.303 | Acc: 9.770% (3214/32896)\n",
      "Loss: 2.303 | Acc: 9.782% (3224/32960)\n",
      "Loss: 2.303 | Acc: 9.769% (3226/33024)\n",
      "Loss: 2.303 | Acc: 9.762% (3230/33088)\n",
      "Loss: 2.303 | Acc: 9.758% (3235/33152)\n",
      "Loss: 2.303 | Acc: 9.769% (3245/33216)\n",
      "Loss: 2.303 | Acc: 9.760% (3248/33280)\n",
      "Loss: 2.303 | Acc: 9.762% (3255/33344)\n",
      "Loss: 2.303 | Acc: 9.755% (3259/33408)\n",
      "Loss: 2.303 | Acc: 9.760% (3267/33472)\n",
      "Loss: 2.303 | Acc: 9.760% (3273/33536)\n",
      "Loss: 2.303 | Acc: 9.759% (3279/33600)\n",
      "Loss: 2.303 | Acc: 9.755% (3284/33664)\n",
      "Loss: 2.303 | Acc: 9.757% (3291/33728)\n",
      "Loss: 2.303 | Acc: 9.745% (3293/33792)\n",
      "Loss: 2.303 | Acc: 9.759% (3304/33856)\n",
      "Loss: 2.303 | Acc: 9.749% (3307/33920)\n",
      "Loss: 2.303 | Acc: 9.760% (3317/33984)\n",
      "Loss: 2.303 | Acc: 9.757% (3322/34048)\n",
      "Loss: 2.303 | Acc: 9.753% (3327/34112)\n",
      "Loss: 2.303 | Acc: 9.755% (3334/34176)\n",
      "Loss: 2.303 | Acc: 9.752% (3339/34240)\n",
      "Loss: 2.303 | Acc: 9.757% (3347/34304)\n",
      "Loss: 2.303 | Acc: 9.756% (3353/34368)\n",
      "Loss: 2.303 | Acc: 9.755% (3359/34432)\n",
      "Loss: 2.303 | Acc: 9.749% (3363/34496)\n",
      "Loss: 2.303 | Acc: 9.742% (3367/34560)\n",
      "Loss: 2.303 | Acc: 9.736% (3371/34624)\n",
      "Loss: 2.303 | Acc: 9.744% (3380/34688)\n",
      "Loss: 2.303 | Acc: 9.755% (3390/34752)\n",
      "Loss: 2.303 | Acc: 9.754% (3396/34816)\n",
      "Loss: 2.303 | Acc: 9.759% (3404/34880)\n",
      "Loss: 2.303 | Acc: 9.767% (3413/34944)\n",
      "Loss: 2.303 | Acc: 9.766% (3419/35008)\n",
      "Loss: 2.303 | Acc: 9.774% (3428/35072)\n",
      "Loss: 2.303 | Acc: 9.793% (3441/35136)\n",
      "Loss: 2.303 | Acc: 9.798% (3449/35200)\n",
      "Loss: 2.303 | Acc: 9.789% (3452/35264)\n",
      "Loss: 2.303 | Acc: 9.788% (3458/35328)\n",
      "Loss: 2.303 | Acc: 9.799% (3468/35392)\n",
      "Loss: 2.303 | Acc: 9.804% (3476/35456)\n",
      "Loss: 2.303 | Acc: 9.794% (3479/35520)\n",
      "Loss: 2.303 | Acc: 9.782% (3481/35584)\n",
      "Loss: 2.303 | Acc: 9.793% (3491/35648)\n",
      "Loss: 2.303 | Acc: 9.789% (3496/35712)\n",
      "Loss: 2.303 | Acc: 9.791% (3503/35776)\n",
      "Loss: 2.303 | Acc: 9.794% (3510/35840)\n",
      "Loss: 2.303 | Acc: 9.801% (3519/35904)\n",
      "Loss: 2.303 | Acc: 9.814% (3530/35968)\n",
      "Loss: 2.303 | Acc: 9.805% (3533/36032)\n",
      "Loss: 2.303 | Acc: 9.804% (3539/36096)\n",
      "Loss: 2.303 | Acc: 9.806% (3546/36160)\n",
      "Loss: 2.303 | Acc: 9.800% (3550/36224)\n",
      "Loss: 2.303 | Acc: 9.808% (3559/36288)\n",
      "Loss: 2.303 | Acc: 9.796% (3561/36352)\n",
      "Loss: 2.303 | Acc: 9.809% (3572/36416)\n",
      "Loss: 2.303 | Acc: 9.816% (3581/36480)\n",
      "Loss: 2.303 | Acc: 9.818% (3588/36544)\n",
      "Loss: 2.303 | Acc: 9.815% (3593/36608)\n",
      "Loss: 2.303 | Acc: 9.806% (3596/36672)\n",
      "Loss: 2.303 | Acc: 9.808% (3603/36736)\n",
      "Loss: 2.303 | Acc: 9.804% (3608/36800)\n",
      "Loss: 2.303 | Acc: 9.806% (3615/36864)\n",
      "Loss: 2.303 | Acc: 9.814% (3624/36928)\n",
      "Loss: 2.303 | Acc: 9.810% (3629/36992)\n",
      "Loss: 2.303 | Acc: 9.809% (3635/37056)\n",
      "Loss: 2.303 | Acc: 9.809% (3641/37120)\n",
      "Loss: 2.303 | Acc: 9.819% (3651/37184)\n",
      "Loss: 2.303 | Acc: 9.823% (3659/37248)\n",
      "Loss: 2.303 | Acc: 9.825% (3666/37312)\n",
      "Loss: 2.303 | Acc: 9.830% (3674/37376)\n",
      "Loss: 2.303 | Acc: 9.824% (3678/37440)\n",
      "Loss: 2.303 | Acc: 9.826% (3685/37504)\n",
      "Loss: 2.303 | Acc: 9.822% (3690/37568)\n",
      "Loss: 2.303 | Acc: 9.827% (3698/37632)\n",
      "Loss: 2.303 | Acc: 9.821% (3702/37696)\n",
      "Loss: 2.303 | Acc: 9.823% (3709/37760)\n",
      "Loss: 2.303 | Acc: 9.806% (3709/37824)\n",
      "Loss: 2.303 | Acc: 9.813% (3718/37888)\n",
      "Loss: 2.303 | Acc: 9.818% (3726/37952)\n",
      "Loss: 2.303 | Acc: 9.809% (3729/38016)\n",
      "Loss: 2.303 | Acc: 9.819% (3739/38080)\n",
      "Loss: 2.303 | Acc: 9.818% (3745/38144)\n",
      "Loss: 2.303 | Acc: 9.823% (3753/38208)\n",
      "Loss: 2.303 | Acc: 9.822% (3759/38272)\n",
      "Loss: 2.303 | Acc: 9.826% (3767/38336)\n",
      "Loss: 2.303 | Acc: 9.828% (3774/38400)\n",
      "Loss: 2.303 | Acc: 9.827% (3780/38464)\n",
      "Loss: 2.303 | Acc: 9.821% (3784/38528)\n",
      "Loss: 2.303 | Acc: 9.834% (3795/38592)\n",
      "Loss: 2.303 | Acc: 9.843% (3805/38656)\n",
      "Loss: 2.303 | Acc: 9.840% (3810/38720)\n",
      "Loss: 2.303 | Acc: 9.839% (3816/38784)\n",
      "Loss: 2.303 | Acc: 9.846% (3825/38848)\n",
      "Loss: 2.303 | Acc: 9.838% (3828/38912)\n",
      "Loss: 2.303 | Acc: 9.837% (3834/38976)\n",
      "Loss: 2.303 | Acc: 9.836% (3840/39040)\n",
      "Loss: 2.303 | Acc: 9.830% (3844/39104)\n",
      "Loss: 2.303 | Acc: 9.829% (3850/39168)\n",
      "Loss: 2.303 | Acc: 9.816% (3851/39232)\n",
      "Loss: 2.303 | Acc: 9.810% (3855/39296)\n",
      "Loss: 2.303 | Acc: 9.807% (3860/39360)\n",
      "Loss: 2.303 | Acc: 9.806% (3866/39424)\n",
      "Loss: 2.303 | Acc: 9.806% (3872/39488)\n",
      "Loss: 2.303 | Acc: 9.807% (3879/39552)\n",
      "Loss: 2.303 | Acc: 9.807% (3885/39616)\n",
      "Loss: 2.303 | Acc: 9.811% (3893/39680)\n",
      "Loss: 2.303 | Acc: 9.810% (3899/39744)\n",
      "Loss: 2.303 | Acc: 9.805% (3903/39808)\n",
      "Loss: 2.303 | Acc: 9.814% (3913/39872)\n",
      "Loss: 2.303 | Acc: 9.821% (3922/39936)\n",
      "Loss: 2.303 | Acc: 9.828% (3931/40000)\n",
      "Loss: 2.303 | Acc: 9.819% (3934/40064)\n",
      "Loss: 2.303 | Acc: 9.814% (3938/40128)\n",
      "Loss: 2.303 | Acc: 9.815% (3945/40192)\n",
      "Loss: 2.303 | Acc: 9.812% (3950/40256)\n",
      "Loss: 2.303 | Acc: 9.814% (3957/40320)\n",
      "Loss: 2.303 | Acc: 9.811% (3962/40384)\n",
      "Loss: 2.303 | Acc: 9.810% (3968/40448)\n",
      "Loss: 2.303 | Acc: 9.809% (3974/40512)\n",
      "Loss: 2.303 | Acc: 9.824% (3986/40576)\n",
      "Loss: 2.303 | Acc: 9.828% (3994/40640)\n",
      "Loss: 2.303 | Acc: 9.825% (3999/40704)\n",
      "Loss: 2.303 | Acc: 9.831% (4008/40768)\n",
      "Loss: 2.303 | Acc: 9.843% (4019/40832)\n",
      "Loss: 2.303 | Acc: 9.844% (4026/40896)\n",
      "Loss: 2.303 | Acc: 9.849% (4034/40960)\n",
      "Loss: 2.303 | Acc: 9.845% (4039/41024)\n",
      "Loss: 2.303 | Acc: 9.850% (4047/41088)\n",
      "Loss: 2.303 | Acc: 9.854% (4055/41152)\n",
      "Loss: 2.303 | Acc: 9.848% (4059/41216)\n",
      "Loss: 2.303 | Acc: 9.845% (4064/41280)\n",
      "Loss: 2.303 | Acc: 9.844% (4070/41344)\n",
      "Loss: 2.303 | Acc: 9.834% (4072/41408)\n",
      "Loss: 2.303 | Acc: 9.833% (4078/41472)\n",
      "Loss: 2.303 | Acc: 9.830% (4083/41536)\n",
      "Loss: 2.303 | Acc: 9.822% (4086/41600)\n",
      "Loss: 2.303 | Acc: 9.817% (4090/41664)\n",
      "Loss: 2.303 | Acc: 9.811% (4094/41728)\n",
      "Loss: 2.303 | Acc: 9.801% (4096/41792)\n",
      "Loss: 2.303 | Acc: 9.800% (4102/41856)\n",
      "Loss: 2.303 | Acc: 9.792% (4105/41920)\n",
      "Loss: 2.303 | Acc: 9.789% (4110/41984)\n",
      "Loss: 2.303 | Acc: 9.786% (4115/42048)\n",
      "Loss: 2.303 | Acc: 9.795% (4125/42112)\n",
      "Loss: 2.303 | Acc: 9.785% (4127/42176)\n",
      "Loss: 2.303 | Acc: 9.787% (4134/42240)\n",
      "Loss: 2.303 | Acc: 9.796% (4144/42304)\n",
      "Loss: 2.303 | Acc: 9.790% (4148/42368)\n",
      "Loss: 2.303 | Acc: 9.794% (4156/42432)\n",
      "Loss: 2.303 | Acc: 9.799% (4164/42496)\n",
      "Loss: 2.303 | Acc: 9.789% (4166/42560)\n",
      "Loss: 2.303 | Acc: 9.776% (4167/42624)\n",
      "Loss: 2.303 | Acc: 9.776% (4173/42688)\n",
      "Loss: 2.303 | Acc: 9.777% (4180/42752)\n",
      "Loss: 2.303 | Acc: 9.770% (4183/42816)\n",
      "Loss: 2.303 | Acc: 9.776% (4192/42880)\n",
      "Loss: 2.303 | Acc: 9.787% (4203/42944)\n",
      "Loss: 2.303 | Acc: 9.787% (4209/43008)\n",
      "Loss: 2.303 | Acc: 9.784% (4214/43072)\n",
      "Loss: 2.303 | Acc: 9.785% (4221/43136)\n",
      "Loss: 2.303 | Acc: 9.796% (4232/43200)\n",
      "Loss: 2.303 | Acc: 9.789% (4235/43264)\n",
      "Loss: 2.303 | Acc: 9.797% (4245/43328)\n",
      "Loss: 2.303 | Acc: 9.790% (4248/43392)\n",
      "Loss: 2.303 | Acc: 9.787% (4253/43456)\n",
      "Loss: 2.303 | Acc: 9.786% (4259/43520)\n",
      "Loss: 2.303 | Acc: 9.790% (4267/43584)\n",
      "Loss: 2.303 | Acc: 9.797% (4276/43648)\n",
      "Loss: 2.303 | Acc: 9.805% (4286/43712)\n",
      "Loss: 2.303 | Acc: 9.793% (4287/43776)\n",
      "Loss: 2.303 | Acc: 9.792% (4293/43840)\n",
      "Loss: 2.303 | Acc: 9.792% (4299/43904)\n",
      "Loss: 2.303 | Acc: 9.798% (4308/43968)\n",
      "Loss: 2.303 | Acc: 9.793% (4312/44032)\n",
      "Loss: 2.303 | Acc: 9.801% (4322/44096)\n",
      "Loss: 2.303 | Acc: 9.796% (4326/44160)\n",
      "Loss: 2.303 | Acc: 9.802% (4335/44224)\n",
      "Loss: 2.303 | Acc: 9.793% (4337/44288)\n",
      "Loss: 2.303 | Acc: 9.797% (4345/44352)\n",
      "Loss: 2.303 | Acc: 9.798% (4352/44416)\n",
      "Loss: 2.303 | Acc: 9.798% (4358/44480)\n",
      "Loss: 2.303 | Acc: 9.802% (4366/44544)\n",
      "Loss: 2.303 | Acc: 9.803% (4373/44608)\n",
      "Loss: 2.303 | Acc: 9.803% (4379/44672)\n",
      "Loss: 2.303 | Acc: 9.802% (4385/44736)\n",
      "Loss: 2.303 | Acc: 9.806% (4393/44800)\n",
      "Loss: 2.303 | Acc: 9.807% (4400/44864)\n",
      "Loss: 2.303 | Acc: 9.807% (4406/44928)\n",
      "Loss: 2.303 | Acc: 9.808% (4413/44992)\n",
      "Loss: 2.303 | Acc: 9.803% (4417/45056)\n",
      "Loss: 2.303 | Acc: 9.803% (4423/45120)\n",
      "Loss: 2.303 | Acc: 9.811% (4433/45184)\n",
      "Loss: 2.303 | Acc: 9.808% (4438/45248)\n",
      "Loss: 2.303 | Acc: 9.801% (4441/45312)\n",
      "Loss: 2.303 | Acc: 9.798% (4446/45376)\n",
      "Loss: 2.303 | Acc: 9.793% (4450/45440)\n",
      "Loss: 2.303 | Acc: 9.799% (4459/45504)\n",
      "Loss: 2.303 | Acc: 9.799% (4465/45568)\n",
      "Loss: 2.303 | Acc: 9.791% (4468/45632)\n",
      "Loss: 2.303 | Acc: 9.793% (4475/45696)\n",
      "Loss: 2.303 | Acc: 9.792% (4481/45760)\n",
      "Loss: 2.303 | Acc: 9.794% (4488/45824)\n",
      "Loss: 2.303 | Acc: 9.789% (4492/45888)\n",
      "Loss: 2.303 | Acc: 9.782% (4495/45952)\n",
      "Loss: 2.303 | Acc: 9.788% (4504/46016)\n",
      "Loss: 2.303 | Acc: 9.787% (4510/46080)\n",
      "Loss: 2.303 | Acc: 9.795% (4520/46144)\n",
      "Loss: 2.303 | Acc: 9.797% (4527/46208)\n",
      "Loss: 2.303 | Acc: 9.790% (4530/46272)\n",
      "Loss: 2.303 | Acc: 9.787% (4535/46336)\n",
      "Loss: 2.303 | Acc: 9.787% (4541/46400)\n",
      "Loss: 2.303 | Acc: 9.799% (4553/46464)\n",
      "Loss: 2.303 | Acc: 9.798% (4559/46528)\n",
      "Loss: 2.303 | Acc: 9.794% (4563/46592)\n",
      "Loss: 2.303 | Acc: 9.789% (4567/46656)\n",
      "Loss: 2.303 | Acc: 9.788% (4573/46720)\n",
      "Loss: 2.303 | Acc: 9.800% (4585/46784)\n",
      "Loss: 2.303 | Acc: 9.802% (4592/46848)\n",
      "Loss: 2.303 | Acc: 9.806% (4600/46912)\n",
      "Loss: 2.303 | Acc: 9.803% (4605/46976)\n",
      "Loss: 2.303 | Acc: 9.809% (4614/47040)\n",
      "Loss: 2.303 | Acc: 9.817% (4624/47104)\n",
      "Loss: 2.303 | Acc: 9.818% (4631/47168)\n",
      "Loss: 2.303 | Acc: 9.815% (4636/47232)\n",
      "Loss: 2.303 | Acc: 9.813% (4641/47296)\n",
      "Loss: 2.303 | Acc: 9.814% (4648/47360)\n",
      "Loss: 2.303 | Acc: 9.818% (4656/47424)\n",
      "Loss: 2.303 | Acc: 9.821% (4664/47488)\n",
      "Loss: 2.303 | Acc: 9.823% (4671/47552)\n",
      "Loss: 2.303 | Acc: 9.824% (4678/47616)\n",
      "Loss: 2.303 | Acc: 9.818% (4681/47680)\n",
      "Loss: 2.303 | Acc: 9.821% (4689/47744)\n",
      "Loss: 2.303 | Acc: 9.823% (4696/47808)\n",
      "Loss: 2.303 | Acc: 9.820% (4701/47872)\n",
      "Loss: 2.303 | Acc: 9.813% (4704/47936)\n",
      "Loss: 2.303 | Acc: 9.812% (4710/48000)\n",
      "Loss: 2.303 | Acc: 9.812% (4716/48064)\n",
      "Loss: 2.303 | Acc: 9.811% (4722/48128)\n",
      "Loss: 2.303 | Acc: 9.809% (4727/48192)\n",
      "Loss: 2.303 | Acc: 9.812% (4735/48256)\n",
      "Loss: 2.303 | Acc: 9.810% (4740/48320)\n",
      "Loss: 2.303 | Acc: 9.805% (4744/48384)\n",
      "Loss: 2.303 | Acc: 9.808% (4752/48448)\n",
      "Loss: 2.303 | Acc: 9.810% (4759/48512)\n",
      "Loss: 2.303 | Acc: 9.813% (4767/48576)\n",
      "Loss: 2.303 | Acc: 9.815% (4774/48640)\n",
      "Loss: 2.303 | Acc: 9.818% (4782/48704)\n",
      "Loss: 2.303 | Acc: 9.828% (4793/48768)\n",
      "Loss: 2.303 | Acc: 9.834% (4802/48832)\n",
      "Loss: 2.303 | Acc: 9.831% (4807/48896)\n",
      "Loss: 2.303 | Acc: 9.839% (4817/48960)\n",
      "Loss: 2.303 | Acc: 9.839% (4821/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 9.838775510204082\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.302 | Acc: 14.062% (9/64)\n",
      "Loss: 2.303 | Acc: 8.594% (11/128)\n",
      "Loss: 2.302 | Acc: 10.938% (21/192)\n",
      "Loss: 2.302 | Acc: 11.719% (30/256)\n",
      "Loss: 2.302 | Acc: 10.938% (35/320)\n",
      "Loss: 2.302 | Acc: 11.458% (44/384)\n",
      "Loss: 2.302 | Acc: 11.384% (51/448)\n",
      "Loss: 2.302 | Acc: 11.523% (59/512)\n",
      "Loss: 2.302 | Acc: 11.632% (67/576)\n",
      "Loss: 2.302 | Acc: 11.719% (75/640)\n",
      "Loss: 2.302 | Acc: 11.790% (83/704)\n",
      "Loss: 2.302 | Acc: 11.719% (90/768)\n",
      "Loss: 2.303 | Acc: 11.178% (93/832)\n",
      "Loss: 2.303 | Acc: 10.826% (97/896)\n",
      "Loss: 2.303 | Acc: 10.729% (103/960)\n",
      "Loss: 2.303 | Acc: 10.742% (110/1024)\n",
      "Loss: 2.303 | Acc: 10.570% (115/1088)\n",
      "Loss: 2.303 | Acc: 10.330% (119/1152)\n",
      "Loss: 2.303 | Acc: 10.197% (124/1216)\n",
      "Loss: 2.303 | Acc: 10.234% (131/1280)\n",
      "Loss: 2.303 | Acc: 10.342% (139/1344)\n",
      "Loss: 2.303 | Acc: 10.511% (148/1408)\n",
      "Loss: 2.303 | Acc: 10.462% (154/1472)\n",
      "Loss: 2.303 | Acc: 10.221% (157/1536)\n",
      "Loss: 2.303 | Acc: 9.938% (159/1600)\n",
      "Loss: 2.303 | Acc: 10.096% (168/1664)\n",
      "Loss: 2.303 | Acc: 10.185% (176/1728)\n",
      "Loss: 2.303 | Acc: 9.933% (178/1792)\n",
      "Loss: 2.303 | Acc: 10.183% (189/1856)\n",
      "Loss: 2.303 | Acc: 10.312% (198/1920)\n",
      "Loss: 2.303 | Acc: 10.131% (201/1984)\n",
      "Loss: 2.303 | Acc: 10.107% (207/2048)\n",
      "Loss: 2.303 | Acc: 9.943% (210/2112)\n",
      "Loss: 2.303 | Acc: 10.110% (220/2176)\n",
      "Loss: 2.303 | Acc: 10.134% (227/2240)\n",
      "Loss: 2.303 | Acc: 10.243% (236/2304)\n",
      "Loss: 2.303 | Acc: 10.177% (241/2368)\n",
      "Loss: 2.303 | Acc: 10.074% (245/2432)\n",
      "Loss: 2.303 | Acc: 10.256% (256/2496)\n",
      "Loss: 2.303 | Acc: 10.312% (264/2560)\n",
      "Loss: 2.303 | Acc: 10.175% (267/2624)\n",
      "Loss: 2.303 | Acc: 10.007% (269/2688)\n",
      "Loss: 2.303 | Acc: 10.029% (276/2752)\n",
      "Loss: 2.303 | Acc: 10.050% (283/2816)\n",
      "Loss: 2.303 | Acc: 10.069% (290/2880)\n",
      "Loss: 2.303 | Acc: 10.088% (297/2944)\n",
      "Loss: 2.303 | Acc: 10.073% (303/3008)\n",
      "Loss: 2.303 | Acc: 9.961% (306/3072)\n",
      "Loss: 2.303 | Acc: 9.981% (313/3136)\n",
      "Loss: 2.303 | Acc: 10.219% (327/3200)\n",
      "Loss: 2.303 | Acc: 10.263% (335/3264)\n",
      "Loss: 2.303 | Acc: 10.276% (342/3328)\n",
      "Loss: 2.303 | Acc: 10.259% (348/3392)\n",
      "Loss: 2.303 | Acc: 10.214% (353/3456)\n",
      "Loss: 2.303 | Acc: 10.227% (360/3520)\n",
      "Loss: 2.303 | Acc: 10.212% (366/3584)\n",
      "Loss: 2.303 | Acc: 10.143% (370/3648)\n",
      "Loss: 2.303 | Acc: 10.156% (377/3712)\n",
      "Loss: 2.303 | Acc: 10.222% (386/3776)\n",
      "Loss: 2.303 | Acc: 10.260% (394/3840)\n",
      "Loss: 2.303 | Acc: 10.297% (402/3904)\n",
      "Loss: 2.303 | Acc: 10.307% (409/3968)\n",
      "Loss: 2.303 | Acc: 10.367% (418/4032)\n",
      "Loss: 2.303 | Acc: 10.254% (420/4096)\n",
      "Loss: 2.303 | Acc: 10.144% (422/4160)\n",
      "Loss: 2.303 | Acc: 10.085% (426/4224)\n",
      "Loss: 2.303 | Acc: 10.121% (434/4288)\n",
      "Loss: 2.303 | Acc: 10.156% (442/4352)\n",
      "Loss: 2.303 | Acc: 10.145% (448/4416)\n",
      "Loss: 2.303 | Acc: 10.156% (455/4480)\n",
      "Loss: 2.303 | Acc: 10.189% (463/4544)\n",
      "Loss: 2.303 | Acc: 10.200% (470/4608)\n",
      "Loss: 2.303 | Acc: 10.103% (472/4672)\n",
      "Loss: 2.303 | Acc: 10.198% (483/4736)\n",
      "Loss: 2.303 | Acc: 10.250% (492/4800)\n",
      "Loss: 2.303 | Acc: 10.259% (499/4864)\n",
      "Loss: 2.303 | Acc: 10.227% (504/4928)\n",
      "Loss: 2.303 | Acc: 10.276% (513/4992)\n",
      "Loss: 2.303 | Acc: 10.206% (516/5056)\n",
      "Loss: 2.303 | Acc: 10.254% (525/5120)\n",
      "Loss: 2.303 | Acc: 10.301% (534/5184)\n",
      "Loss: 2.303 | Acc: 10.213% (536/5248)\n",
      "Loss: 2.303 | Acc: 10.222% (543/5312)\n",
      "Loss: 2.303 | Acc: 10.193% (548/5376)\n",
      "Loss: 2.303 | Acc: 10.184% (554/5440)\n",
      "Loss: 2.303 | Acc: 10.193% (561/5504)\n",
      "Loss: 2.303 | Acc: 10.147% (565/5568)\n",
      "Loss: 2.303 | Acc: 10.138% (571/5632)\n",
      "Loss: 2.303 | Acc: 10.060% (573/5696)\n",
      "Loss: 2.303 | Acc: 10.017% (577/5760)\n",
      "Loss: 2.303 | Acc: 9.993% (582/5824)\n",
      "Loss: 2.303 | Acc: 9.952% (586/5888)\n",
      "Loss: 2.303 | Acc: 9.980% (594/5952)\n",
      "Loss: 2.303 | Acc: 10.057% (605/6016)\n",
      "Loss: 2.303 | Acc: 10.000% (608/6080)\n",
      "Loss: 2.303 | Acc: 10.042% (617/6144)\n",
      "Loss: 2.303 | Acc: 10.084% (626/6208)\n",
      "Loss: 2.303 | Acc: 10.029% (629/6272)\n",
      "Loss: 2.303 | Acc: 10.038% (636/6336)\n",
      "Loss: 2.303 | Acc: 10.031% (642/6400)\n",
      "Loss: 2.303 | Acc: 9.978% (645/6464)\n",
      "Loss: 2.303 | Acc: 9.957% (650/6528)\n",
      "Loss: 2.303 | Acc: 9.967% (657/6592)\n",
      "Loss: 2.303 | Acc: 9.931% (661/6656)\n",
      "Loss: 2.303 | Acc: 9.881% (664/6720)\n",
      "Loss: 2.303 | Acc: 9.876% (670/6784)\n",
      "Loss: 2.303 | Acc: 9.886% (677/6848)\n",
      "Loss: 2.303 | Acc: 9.910% (685/6912)\n",
      "Loss: 2.303 | Acc: 9.920% (692/6976)\n",
      "Loss: 2.303 | Acc: 9.886% (696/7040)\n",
      "Loss: 2.303 | Acc: 9.896% (703/7104)\n",
      "Loss: 2.303 | Acc: 9.863% (707/7168)\n",
      "Loss: 2.303 | Acc: 9.845% (712/7232)\n",
      "Loss: 2.303 | Acc: 9.814% (716/7296)\n",
      "Loss: 2.303 | Acc: 9.755% (718/7360)\n",
      "Loss: 2.303 | Acc: 9.739% (723/7424)\n",
      "Loss: 2.303 | Acc: 9.749% (730/7488)\n",
      "Loss: 2.303 | Acc: 9.785% (739/7552)\n",
      "Loss: 2.303 | Acc: 9.821% (748/7616)\n",
      "Loss: 2.303 | Acc: 9.844% (756/7680)\n",
      "Loss: 2.303 | Acc: 9.853% (763/7744)\n",
      "Loss: 2.303 | Acc: 9.887% (772/7808)\n",
      "Loss: 2.303 | Acc: 9.858% (776/7872)\n",
      "Loss: 2.303 | Acc: 9.879% (784/7936)\n",
      "Loss: 2.303 | Acc: 9.887% (791/8000)\n",
      "Loss: 2.303 | Acc: 9.896% (798/8064)\n",
      "Loss: 2.303 | Acc: 9.904% (805/8128)\n",
      "Loss: 2.303 | Acc: 9.912% (812/8192)\n",
      "Loss: 2.303 | Acc: 9.884% (816/8256)\n",
      "Loss: 2.303 | Acc: 9.868% (821/8320)\n",
      "Loss: 2.303 | Acc: 9.864% (827/8384)\n",
      "Loss: 2.303 | Acc: 9.860% (833/8448)\n",
      "Loss: 2.303 | Acc: 9.880% (841/8512)\n",
      "Loss: 2.303 | Acc: 9.865% (846/8576)\n",
      "Loss: 2.303 | Acc: 9.826% (849/8640)\n",
      "Loss: 2.303 | Acc: 9.812% (854/8704)\n",
      "Loss: 2.303 | Acc: 9.820% (861/8768)\n",
      "Loss: 2.303 | Acc: 9.805% (866/8832)\n",
      "Loss: 2.303 | Acc: 9.802% (872/8896)\n",
      "Loss: 2.303 | Acc: 9.844% (882/8960)\n",
      "Loss: 2.303 | Acc: 9.840% (888/9024)\n",
      "Loss: 2.303 | Acc: 9.870% (897/9088)\n",
      "Loss: 2.303 | Acc: 9.889% (905/9152)\n",
      "Loss: 2.303 | Acc: 9.907% (913/9216)\n",
      "Loss: 2.303 | Acc: 9.925% (921/9280)\n",
      "Loss: 2.303 | Acc: 9.953% (930/9344)\n",
      "Loss: 2.303 | Acc: 9.991% (940/9408)\n",
      "Loss: 2.303 | Acc: 9.977% (945/9472)\n",
      "Loss: 2.303 | Acc: 10.004% (954/9536)\n",
      "Loss: 2.303 | Acc: 10.021% (962/9600)\n",
      "Loss: 2.303 | Acc: 9.996% (966/9664)\n",
      "Loss: 2.303 | Acc: 10.033% (976/9728)\n",
      "Loss: 2.303 | Acc: 10.059% (985/9792)\n",
      "Loss: 2.303 | Acc: 10.045% (990/9856)\n",
      "Loss: 2.303 | Acc: 10.040% (996/9920)\n",
      "Loss: 2.303 | Acc: 10.016% (1000/9984)\n",
      "Loss: 2.303 | Acc: 10.000% (1000/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 10.0\n",
      "\n",
      "Final train set accuracy is 9.838775510204082\n",
      "Final test set accuracy is 10.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "network = ViT(hidden_dims=hidden_dims, \n",
    "            input_dims=input_dims, \n",
    "            output_dims=output_dims, \n",
    "            num_trans_layers = num_trans_layers, \n",
    "            num_heads=num_heads, \n",
    "            image_k=image_k, \n",
    "            patch_k=patch_k, bias=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-dutch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
